{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ed83995-c35d-4d64-96ac-57e9fe276bd8"
      },
      "source": [
        "<p style=\"text-align:center\">\n",
        "    <a href=\"https://skills.network\" target=\"_blank\">\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
        "    </a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ff1b27-0005-4d7e-9c8e-207004e9693a"
      },
      "source": [
        "# **Create and Configure a Vector Database to Store Document Embeddings**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ede879-7b30-4e46-96d3-ee9ca687b6ef"
      },
      "source": [
        "Estimated time needed: **30** minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd417867-b877-4359-b9b0-d887fabfce5b"
      },
      "source": [
        "## Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f486bf4-4a23-4aac-be1f-5a8152c3b2b1"
      },
      "source": [
        "Imagine you are working in a customer support center that receives a high volume of inquiries and tickets every day. Your task is to create a system that can quickly provide support agents with the most relevant information to resolve customer issues. Traditional methods of searching through FAQs or support documents can be slow and inefficient, leading to delayed responses and dissatisfied customers.\n",
        "\n",
        "To address this challenge, you will use embedding models to convert support documents and past inquiry responses into numerical vectors that capture their semantic content. These vectors will be stored in a vector database, enabling fast and accurate similarity searches. For example, when a support agent receives a new inquiry about a product issue, the system can instantly retrieve similar past inquiries and their resolutions, helping the agent to provide a quicker and more accurate response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "295f533d-f9ae-4fca-adf3-d65793ad0c35"
      },
      "source": [
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/veZYoygp9GqZrIw5f6SD0g/vector%20db.png\" width=\"50%\" alt=\"vector db\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "904b253e-0831-4a4b-91ca-7ec9275ed2ac"
      },
      "source": [
        "In this lab, you will learn how to use vector databases to store embeddings generated from textual data using LangChain. The focus will be on two popular vector databases: Chroma DB and FAISS (Facebook AI Similarity Search). You will also learn how to perform similarity searches in these databases based on a query, enabling efficient retrieval of relevant information. By the end of this lab, you will be able to effectively use vector databases to store and query embeddings, enhancing your data analysis and retrieval capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ef49b8-5f5b-415a-9059-11350abd13e9"
      },
      "source": [
        "## __Table of Contents__\n",
        "\n",
        "<ol>\n",
        "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
        "    <li>\n",
        "        <a href=\"#Setup\">Setup</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
        "            <li><a href=\"#Load-text\">Load text</a></li>\n",
        "            <li><a href=\"#Split-data\">Split data</a></li>\n",
        "            <li><a href=\"#Embedding model\">Embedding model</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "    <li>\n",
        "        <a href=\"#Vector-store\">Vector store</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Chroma-DB\">Chroma DB</a></li>\n",
        "            <li><a href=\"#FIASS-DB\">FIASS DB</a></li>\n",
        "            <li><a href=\"#Managing-vector-store:-adding,-updating,-and-deleting-entries\">Managing vector store: adding, updating, and deleting entries</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "</ol>\n",
        "\n",
        "<a href=\"#Exercises\">Exercises</a>\n",
        "<ol>\n",
        "    <li><a href=\"#Exercise-1---Use-another-query-to-conduct-similarity-search.\">Exercise 1. Use another query to conduct similarity search.</a></li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53429db7-7210-45d4-86d3-ef68fcc6da1b"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "After completing this lab you will be able to:\n",
        "\n",
        "- Prepare and preprocess documents for embeddings.\n",
        "- Generate embeddings using watsonx.ai's embedding model.\n",
        "- Store these embeddings in Chroma DB and FAISS.\n",
        "- Perform similarity searches to retrieve relevant documents based on new inquiries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea734cd6-41aa-4e2f-b3ed-43b7bb6122ac"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69fc2ce7-d234-4f4b-9082-8ec2aec82b41"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec84633a-96a9-45dd-8215-7cea8a11b0ba"
      },
      "source": [
        "For this lab, you will use the following libraries:\n",
        "\n",
        "* [`ibm-watson-ai`](https://ibm.github.io/watsonx-ai-python-sdk/) for using LLMs from IBM's watsonx.ai.\n",
        "* [`langchain`, `langchain-ibm`, `langchain-community`](https://www.langchain.com/) for using relevant features from Langchain.\n",
        "* [`chromadb`](https://www.trychroma.com/) is a open-source vector database used to store embeddings.\n",
        "* [`faiss-cpu`](https://pypi.org/project/faiss-cpu/) is used to support the using of FAISS vector database.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33a274ff-4520-4a12-b447-9f4fbb438979"
      },
      "source": [
        "### Installing required libraries\n",
        "\n",
        "The following required libraries are __not__ preinstalled in the Skills Network Labs environment. __You must run the following cell__ to install them:\n",
        "\n",
        "**Note:** The version is being pinned here to specify the version. It's recommended that you do this as well. Even if the library is updated in the future, the installed library could still support this lab work.\n",
        "\n",
        "This might take approximately 1-2 minutes.\n",
        "\n",
        "As `%%capture` is used to capture the installation, you won't see the output process. After the installation is completed, you will see a number beside the cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "30wyZ3DoQgt_",
        "outputId": "3e7d3923-c051-47b5-96cf-090c206fadda"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohappyeyeballs==2.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.6.1)\n",
            "Collecting aiohttp==3.12.13 (from -r requirements.txt (line 9))\n",
            "  Downloading aiohttp-3.12.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: aiosignal==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.3.2)\n",
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.7.0)\n",
            "Requirement already satisfied: anyio==4.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.9.0)\n",
            "Requirement already satisfied: asgiref==3.8.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 22)) (3.8.1)\n",
            "Requirement already satisfied: attrs==25.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (25.3.0)\n",
            "Requirement already satisfied: backoff==2.2.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 26)) (2.2.1)\n",
            "Requirement already satisfied: bcrypt==4.3.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 28)) (4.3.0)\n",
            "Requirement already satisfied: build==1.2.2.post1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (1.2.2.post1)\n",
            "Requirement already satisfied: cachetools==5.5.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (5.5.2)\n",
            "Requirement already satisfied: certifi==2025.6.15 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer==3.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (3.4.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 44)) (0.7.3)\n",
            "Requirement already satisfied: chromadb==0.4.24 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 46)) (0.4.24)\n",
            "Requirement already satisfied: click==8.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 48)) (8.2.1)\n",
            "Requirement already satisfied: coloredlogs==15.0.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 52)) (15.0.1)\n",
            "Requirement already satisfied: dataclasses-json==0.6.7 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 54)) (0.6.7)\n",
            "Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 56)) (1.9.0)\n",
            "Requirement already satisfied: durationpy==0.10 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 58)) (0.10)\n",
            "Requirement already satisfied: faiss-cpu==1.8.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 60)) (1.8.0)\n",
            "Collecting fastapi==0.115.13 (from -r requirements.txt (line 62))\n",
            "  Downloading fastapi-0.115.13-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 64)) (3.18.0)\n",
            "Requirement already satisfied: flatbuffers==25.2.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 66)) (25.2.10)\n",
            "Requirement already satisfied: frozenlist==1.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 68)) (1.7.0)\n",
            "Collecting fsspec==2025.5.1 (from -r requirements.txt (line 72))\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting google-auth==2.40.3 (from -r requirements.txt (line 74))\n",
            "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: googleapis-common-protos==1.70.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 76)) (1.70.0)\n",
            "Requirement already satisfied: greenlet==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 78)) (3.2.3)\n",
            "Requirement already satisfied: grpcio==1.73.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 80)) (1.73.0)\n",
            "Requirement already satisfied: h11==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 84)) (0.16.0)\n",
            "Collecting hf-xet==1.1.5 (from -r requirements.txt (line 88))\n",
            "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: httpcore==1.0.9 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 90)) (1.0.9)\n",
            "Requirement already satisfied: httptools==0.6.4 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 92)) (0.6.4)\n",
            "Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 94)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub==0.33.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 96)) (0.33.0)\n",
            "Requirement already satisfied: humanfriendly==10.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 98)) (10.0)\n",
            "Requirement already satisfied: ibm-cos-sdk==2.13.6 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 100)) (2.13.6)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.13.6 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 102)) (2.13.6)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.6 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 106)) (2.13.6)\n",
            "Requirement already satisfied: ibm-watsonx-ai==1.0.4 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 108)) (1.0.4)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 112)) (3.10)\n",
            "Requirement already satisfied: importlib-metadata==8.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 118)) (8.7.0)\n",
            "Requirement already satisfied: importlib-resources==6.5.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 122)) (6.5.2)\n",
            "Requirement already satisfied: jmespath==1.0.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 124)) (1.0.1)\n",
            "Requirement already satisfied: jsonpatch==1.33 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 128)) (1.33)\n",
            "Requirement already satisfied: jsonpointer==3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 130)) (3.0.0)\n",
            "Requirement already satisfied: kubernetes==33.1.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 132)) (33.1.0)\n",
            "Requirement already satisfied: langchain==0.2.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 134)) (0.2.1)\n",
            "Requirement already satisfied: langchain-community==0.2.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 138)) (0.2.1)\n",
            "Requirement already satisfied: langchain-core==0.2.43 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 140)) (0.2.43)\n",
            "Requirement already satisfied: langchain-ibm==0.1.7 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 146)) (0.1.7)\n",
            "Requirement already satisfied: langchain-text-splitters==0.2.4 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 148)) (0.2.4)\n",
            "Requirement already satisfied: langsmith==0.1.147 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 150)) (0.1.147)\n",
            "Requirement already satisfied: lomond==0.3.3 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 155)) (0.3.3)\n",
            "Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 157)) (3.0.0)\n",
            "Requirement already satisfied: marshmallow==3.26.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 159)) (3.26.1)\n",
            "Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 161)) (0.1.2)\n",
            "Requirement already satisfied: mmh3==5.1.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 163)) (5.1.0)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 165)) (1.3.0)\n",
            "Collecting multidict==6.5.0 (from -r requirements.txt (line 167))\n",
            "  Downloading multidict-6.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: mypy-extensions==1.1.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 171)) (1.1.0)\n",
            "Requirement already satisfied: numpy==1.26.4 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 173)) (1.26.4)\n",
            "Collecting oauthlib==3.3.1 (from -r requirements.txt (line 182))\n",
            "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: onnxruntime==1.22.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 186)) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api==1.34.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 188)) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 197)) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.34.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 199)) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.55b1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 201)) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 205)) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi==0.55b1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 207)) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 209)) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk==1.34.1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 213)) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 217)) (0.55b1)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.55b1 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 223)) (0.55b1)\n",
            "Requirement already satisfied: orjson==3.10.18 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 227)) (3.10.18)\n",
            "Requirement already satisfied: overrides==7.7.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 231)) (7.7.0)\n",
            "Requirement already satisfied: packaging==24.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 233)) (24.2)\n",
            "Requirement already satisfied: pandas==2.1.4 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 242)) (2.1.4)\n",
            "Requirement already satisfied: posthog==5.4.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 244)) (5.4.0)\n",
            "Requirement already satisfied: propcache==0.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 246)) (0.3.2)\n",
            "Requirement already satisfied: protobuf==5.29.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 250)) (5.29.5)\n",
            "Requirement already satisfied: pulsar-client==3.7.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 255)) (3.7.0)\n",
            "Requirement already satisfied: pyasn1==0.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 257)) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules==0.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 261)) (0.4.2)\n",
            "Requirement already satisfied: pydantic==2.11.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 263)) (2.11.7)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 270)) (2.33.2)\n",
            "Requirement already satisfied: pygments==2.19.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 272)) (2.19.1)\n",
            "Requirement already satisfied: pypika==0.48.9 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 274)) (0.48.9)\n",
            "Requirement already satisfied: pyproject-hooks==1.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 276)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 278)) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv==1.1.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 284)) (1.1.0)\n",
            "Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 286)) (2025.2)\n",
            "Requirement already satisfied: pyyaml==6.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 288)) (6.0.2)\n",
            "Requirement already satisfied: requests==2.32.2 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 297)) (2.32.2)\n",
            "Requirement already satisfied: requests-oauthlib==2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 310)) (2.0.0)\n",
            "Requirement already satisfied: requests-toolbelt==1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 312)) (1.0.0)\n",
            "Collecting rich==14.0.0 (from -r requirements.txt (line 314))\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: rsa==4.9.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 316)) (4.9.1)\n",
            "Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 318)) (1.5.4)\n",
            "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 320)) (1.17.0)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 326)) (1.3.1)\n",
            "Requirement already satisfied: sqlalchemy==2.0.41 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 328)) (2.0.41)\n",
            "Requirement already satisfied: starlette==0.46.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 332)) (0.46.2)\n",
            "Collecting sympy==1.14.0 (from -r requirements.txt (line 334))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 336)) (0.9.0)\n",
            "Requirement already satisfied: tenacity==8.5.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 338)) (8.5.0)\n",
            "Requirement already satisfied: tokenizers==0.21.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 344)) (0.21.1)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 346)) (4.67.1)\n",
            "Requirement already satisfied: typer==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 350)) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions==4.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 352)) (4.14.0)\n",
            "Requirement already satisfied: typing-inspect==0.9.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 369)) (0.9.0)\n",
            "Requirement already satisfied: typing-inspection==0.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 371)) (0.4.1)\n",
            "Requirement already satisfied: tzdata==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 373)) (2025.2)\n",
            "Collecting urllib3==2.5.0 (from -r requirements.txt (line 375))\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: uvicorn==0.34.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]==0.34.3->-r requirements.txt (line 381)) (0.34.3)\n",
            "Requirement already satisfied: uvloop==0.21.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 383)) (0.21.0)\n",
            "Requirement already satisfied: watchfiles==1.1.0 in /root/.local/lib/python3.11/site-packages (from -r requirements.txt (line 385)) (1.1.0)\n",
            "Requirement already satisfied: websocket-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 387)) (1.8.0)\n",
            "Requirement already satisfied: websockets==15.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 389)) (15.0.1)\n",
            "Requirement already satisfied: wrapt==1.17.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 391)) (1.17.2)\n",
            "Requirement already satisfied: yarl==1.20.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 393)) (1.20.1)\n",
            "Requirement already satisfied: zipp==3.23.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 395)) (3.23.0)\n",
            "Downloading aiohttp-3.12.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.13-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.1/216.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.5/231.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.1/160.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, sympy, oauthlib, multidict, hf-xet, fsspec, rich, google-auth, fastapi, aiohttp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: oauthlib\n",
            "    Found existing installation: oauthlib 3.2.2\n",
            "    Uninstalling oauthlib-3.2.2:\n",
            "      Successfully uninstalled oauthlib-3.2.2\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.4.4\n",
            "    Uninstalling multidict-6.4.4:\n",
            "      Successfully uninstalled multidict-6.4.4\n",
            "  Attempting uninstall: hf-xet\n",
            "    Found existing installation: hf-xet 1.1.3\n",
            "    Uninstalling hf-xet-1.1.3:\n",
            "      Successfully uninstalled hf-xet-1.1.3\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.38.0\n",
            "    Uninstalling google-auth-2.38.0:\n",
            "      Successfully uninstalled google-auth-2.38.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.115.12\n",
            "    Uninstalling fastapi-0.115.12:\n",
            "      Successfully uninstalled fastapi-0.115.12\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.15\n",
            "    Uninstalling aiohttp-3.11.15:\n",
            "      Successfully uninstalled aiohttp-3.11.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.2 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.14.0 which is incompatible.\n",
            "bigframes 2.6.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.12.13 fastapi-0.115.13 fsspec-2025.5.1 google-auth-2.40.3 hf-xet-1.1.5 multidict-6.5.0 oauthlib-3.3.1 rich-14.0.0 sympy-1.14.0 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "aiohttp",
                  "google",
                  "multidict",
                  "oauthlib",
                  "rich",
                  "urllib3"
                ]
              },
              "id": "ed8ce4f546534ecb9a5eeb9a3d2318fd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "459c5383-b497-4814-b18f-93e66674a9d8",
        "outputId": "f681f033-b355-4d17-8817-4004ba134f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ibm-watsonx-ai==1.0.4\n",
            "  Downloading ibm_watsonx_ai-1.0.4-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: requests in /root/.local/lib/python3.11/site-packages (from ibm-watsonx-ai==1.0.4) (2.32.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai==1.0.4) (2.4.0)\n",
            "Collecting pandas<2.2.0,>=0.24.2 (from ibm-watsonx-ai==1.0.4)\n",
            "  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai==1.0.4) (2025.6.15)\n",
            "Requirement already satisfied: lomond in /root/.local/lib/python3.11/site-packages (from ibm-watsonx-ai==1.0.4) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai==1.0.4) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai==1.0.4) (24.2)\n",
            "Collecting ibm-cos-sdk<2.14.0,>=2.12.0 (from ibm-watsonx-ai==1.0.4)\n",
            "  Downloading ibm-cos-sdk-2.13.6.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai==1.0.4) (8.7.0)\n",
            "Collecting ibm-cos-sdk-core==2.13.6 (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.0.4)\n",
            "  Downloading ibm-cos-sdk-core-2.13.6.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ibm-cos-sdk-s3transfer==2.13.6 (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.0.4)\n",
            "  Downloading ibm-cos-sdk-s3transfer-2.13.6.tar.gz (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /root/.local/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.0.4) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai==1.0.4) (2.9.0.post0)\n",
            "Collecting requests (from ibm-watsonx-ai==1.0.4)\n",
            "  Downloading requests-2.32.2-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.23.2 in /root/.local/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai==1.0.4) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai==1.0.4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai==1.0.4) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ibm-watsonx-ai==1.0.4) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ibm-watsonx-ai==1.0.4) (3.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->ibm-watsonx-ai==1.0.4) (3.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from lomond->ibm-watsonx-ai==1.0.4) (1.17.0)\n",
            "Downloading ibm_watsonx_ai-1.0.4-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.2-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.13.6-py3-none-any.whl size=77230 sha256=2ce502da7828ff4e7af6adb8728f1cfabfd4d1d8b1787df81720de702097f58b\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/8c/76/f9472a53a2a80da414dd53ca55f08dbfe48ed6de76e51e3d5f\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.13.6-py3-none-any.whl size=661459 sha256=53072693ccf590b37772f4cf1b73bb2f4a671ce02e356c09272295cc2b1ceaa4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/8e/58/e0fdc8135343f394de9c98a48ed20adf698600e01b16ea46bc\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.13.6-py3-none-any.whl size=90203 sha256=4988a52abf80c525fe7215c2ab772e4f5cbd7a7cd45f713ea431080659592c78\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/71/63/176f15a501fd18265938e1c0b2928f9f380b96e3fc15680c80\n",
            "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: requests, pandas, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk, ibm-watsonx-ai\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: ibm-cos-sdk-core\n",
            "    Found existing installation: ibm-cos-sdk-core 2.14.2\n",
            "    Uninstalling ibm-cos-sdk-core-2.14.2:\n",
            "      Successfully uninstalled ibm-cos-sdk-core-2.14.2\n",
            "  Attempting uninstall: ibm-cos-sdk-s3transfer\n",
            "    Found existing installation: ibm-cos-sdk-s3transfer 2.14.2\n",
            "    Uninstalling ibm-cos-sdk-s3transfer-2.14.2:\n",
            "      Successfully uninstalled ibm-cos-sdk-s3transfer-2.14.2\n",
            "  Attempting uninstall: ibm-cos-sdk\n",
            "    Found existing installation: ibm-cos-sdk 2.14.2\n",
            "    Uninstalling ibm-cos-sdk-2.14.2:\n",
            "      Successfully uninstalled ibm-cos-sdk-2.14.2\n",
            "  Attempting uninstall: ibm-watsonx-ai\n",
            "    Found existing installation: ibm_watsonx_ai 1.3.26\n",
            "    Uninstalling ibm_watsonx_ai-1.3.26:\n",
            "      Successfully uninstalled ibm_watsonx_ai-1.3.26\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.2 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ibm-cos-sdk-2.13.6 ibm-cos-sdk-core-2.13.6 ibm-cos-sdk-s3transfer-2.13.6 ibm-watsonx-ai-1.0.4 pandas-2.1.4 requests-2.32.2\n",
            "Collecting langchain==0.2.1\n",
            "  Using cached langchain-0.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.1) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.1) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.1) (3.11.15)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /root/.local/lib/python3.11/site-packages (from langchain==0.2.1) (0.2.43)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /root/.local/lib/python3.11/site-packages (from langchain==0.2.1) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /root/.local/lib/python3.11/site-packages (from langchain==0.2.1) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /root/.local/lib/python3.11/site-packages (from langchain==0.2.1) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.1) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /root/.local/lib/python3.11/site-packages (from langchain==0.2.1) (2.32.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /root/.local/lib/python3.11/site-packages (from langchain==0.2.1) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.1) (1.20.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.1) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.1) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.1) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.1) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.1) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.1) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain==0.2.1) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.1) (1.3.1)\n",
            "Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.17\n",
            "    Uninstalling langchain-0.2.17:\n",
            "      Successfully uninstalled langchain-0.2.17\n",
            "\u001b[33m  WARNING: The script langchain-server is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed langchain-0.2.1\n",
            "Requirement already satisfied: langchain-ibm==0.1.7 in /root/.local/lib/python3.11/site-packages (0.1.7)\n",
            "Requirement already satisfied: ibm-watsonx-ai<2.0.0,>=1.0.1 in /root/.local/lib/python3.11/site-packages (from langchain-ibm==0.1.7) (1.0.4)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.50 in /root/.local/lib/python3.11/site-packages (from langchain-ibm==0.1.7) (0.2.43)\n",
            "Requirement already satisfied: requests in /root/.local/lib/python3.11/site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2.32.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2.4.0)\n",
            "Requirement already satisfied: pandas<2.2.0,>=0.24.2 in /root/.local/lib/python3.11/site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2.1.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2025.6.15)\n",
            "Requirement already satisfied: lomond in /root/.local/lib/python3.11/site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (24.2)\n",
            "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /root/.local/lib/python3.11/site-packages (from ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2.13.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (8.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /root/.local/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (2.11.7)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /root/.local/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (4.14.0)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.13.6 in /root/.local/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2.13.6)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.6 in /root/.local/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2.13.6)\n",
            "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /root/.local/lib/python3.11/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk-core==2.13.6->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2.9.0.post0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (1.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.23.2 in /root/.local/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (3.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (3.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from lomond->ibm-watsonx-ai<2.0.0,>=1.0.1->langchain-ibm==0.1.7) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.1.50->langchain-ibm==0.1.7) (1.3.1)\n",
            "Requirement already satisfied: langchain-community==0.2.1 in /root/.local/lib/python3.11/site-packages (0.2.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.1) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.1) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.1) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /root/.local/lib/python3.11/site-packages (from langchain-community==0.2.1) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /root/.local/lib/python3.11/site-packages (from langchain-community==0.2.1) (0.2.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /root/.local/lib/python3.11/site-packages (from langchain-community==0.2.1) (0.2.43)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /root/.local/lib/python3.11/site-packages (from langchain-community==0.2.1) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /root/.local/lib/python3.11/site-packages (from langchain-community==0.2.1) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /root/.local/lib/python3.11/site-packages (from langchain-community==0.2.1) (2.32.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /root/.local/lib/python3.11/site-packages (from langchain-community==0.2.1) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.1) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /root/.local/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.1) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /root/.local/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.1) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /root/.local/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (0.2.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.1) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.1) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.1) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.1) (0.4.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/.local/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.1) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.1) (1.3.1)\n",
            "Collecting chromadb==0.4.24\n",
            "  Using cached chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (1.2.2.post1)\n",
            "Requirement already satisfied: requests>=2.28 in /root/.local/lib/python3.11/site-packages (from chromadb==0.4.24) (2.32.2)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (2.11.7)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.24)\n",
            "  Using cached chroma_hnswlib-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (0.115.12)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /root/.local/lib/python3.11/site-packages (from chromadb==0.4.24) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb==0.4.24)\n",
            "  Using cached posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (4.14.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.24)\n",
            "  Using cached pulsar_client-3.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb==0.4.24)\n",
            "  Using cached onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb==0.4.24)\n",
            "  Using cached opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb==0.4.24)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb==0.4.24)\n",
            "  Using cached opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb==0.4.24)\n",
            "  Using cached opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb==0.4.24)\n",
            "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.4.24)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (1.73.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb==0.4.24)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb==0.4.24)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /root/.local/lib/python3.11/site-packages (from chromadb==0.4.24) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb==0.4.24)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb==0.4.24) (3.10.18)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb==0.4.24) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb==0.4.24) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb==0.4.24) (0.46.2)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2025.6.15)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb==0.4.24) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb==0.4.24)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.24)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb==0.4.24) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.24) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.24) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.24)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.24)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting opentelemetry-instrumentation==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24)\n",
            "  Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-util-http==0.55b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24)\n",
            "  Downloading opentelemetry_util_http-0.55b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb==0.4.24)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.4.24)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb==0.4.24) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.24) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.24) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb==0.4.24) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.24) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.28->chromadb==0.4.24) (3.10)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb==0.4.24) (0.33.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.24) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.24) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb==0.4.24) (13.9.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.24) (0.16.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb==0.4.24)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.24)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb==0.4.24)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.4.24)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.24) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.24) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.24) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.24) (1.1.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.24) (3.23.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.24) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.24) (2.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.24) (4.9.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.24)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.24) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.95.2->chromadb==0.4.24) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.24) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.24) (0.6.1)\n",
            "Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.55b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl (31 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.55b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.55b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pulsar_client-3.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=46613a5129483ff4d7b42f7c66cbbc48f7169254a09544f10fe1859513ddbf2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "\u001b[33m  WARNING: The script dotenv is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script humanfriendly is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script watchfiles is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script coloredlogs is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script onnxruntime_test is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts opentelemetry-bootstrap and opentelemetry-instrument are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script chroma is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.1.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-instrumentation-0.55b1 opentelemetry-instrumentation-asgi-0.55b1 opentelemetry-instrumentation-fastapi-0.55b1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 opentelemetry-util-http-0.55b1 overrides-7.7.0 posthog-5.4.0 pulsar-client-3.7.0 pypika-0.48.9 python-dotenv-1.1.0 uvloop-0.21.0 watchfiles-1.1.0\n",
            "Collecting faiss-cpu==1.8.0\n",
            "  Downloading faiss_cpu-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy in /root/.local/lib/python3.11/site-packages (from faiss-cpu==1.8.0) (1.26.4)\n",
            "Downloading faiss_cpu-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n",
            "Installation completed.\n"
          ]
        }
      ],
      "source": [
        "xxx - skip\n",
        "#%%capture\n",
        "!pip install --user \"ibm-watsonx-ai==1.0.4\"\n",
        "!pip install  --user \"langchain==0.2.1\"\n",
        "!pip install  --user \"langchain-ibm==0.1.7\"\n",
        "!pip install  --user \"langchain-community==0.2.1\"\n",
        "!pip install --user \"chromadb==0.4.24\"\n",
        "!pip install  --user \"faiss-cpu==1.8.0\"\n",
        "print(\"Installation completed.\")"
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9c17bdf-b7c0-42bc-b3de-b0973bc09f59"
      },
      "source": [
        "After you install the libraries, restart your kernel. You can do that by clicking the **Restart the kernel** icon.\n",
        "\n",
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/build-a-hotdog-not-hotdog-classifier-guided-project/images/Restarting_the_Kernel.png\" width=\"50%\" alt=\"Restart kernel\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fd031ce-0ef1-472b-b5a7-320ca5566840"
      },
      "source": [
        "-----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c366912-c18a-4e42-add8-9e284d5b6999"
      },
      "source": [
        "The following steps are prerequisite tasks for conducting this project's topic - vector store. These steps include:\n",
        "\n",
        "- Loading the source document.\n",
        "- Splitting the document into chunks.\n",
        "- Building an embedding model.\n",
        "  \n",
        "The details of these steps have been introduced in previous lessons.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1ddeef2-6384-47ad-abef-7b6922169650"
      },
      "source": [
        "### Load text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64db959b-98aa-44bb-a128-cc84a3d26db7"
      },
      "source": [
        "A text file has been prepared as the source document for the downstream vector database task.\n",
        "\n",
        "Now, let's download and load it using LangChain's `TextLoader`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "803b824d-1f92-444e-b410-a986e25b0e15",
        "outputId": "39aab1cf-c7d0-4f60-c690-3d8f3f87a14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-21 11:35:07--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BYlUHaillwM8EUItaIytHQ/companypolicies.txt\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15660 (15K) [text/plain]\n",
            "Saving to: ‘companypolicies.txt’\n",
            "\n",
            "companypolicies.txt 100%[===================>]  15.29K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-06-21 11:35:08 (88.5 MB/s) - ‘companypolicies.txt’ saved [15660/15660]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/BYlUHaillwM8EUItaIytHQ/companypolicies.txt\""
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29f609f8-6058-4fad-9185-ac7dd8d58f38"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader"
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1c28ecd-9477-431d-be81-f73c68ce09b8"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"companypolicies.txt\")\n",
        "data = loader.load()"
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac9dda86-2c0f-486f-b902-38d58ac8911c"
      },
      "source": [
        "You can have a look at this document.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53d3438d-87f1-4c50-8ac8-00b52ca7aca9",
        "outputId": "5953d5da-e49d-45f9-c1f6-4267d85df59b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'companypolicies.txt'}, page_content=\"1.\\tCode of Conduct\\n\\nOur Code of Conduct outlines the fundamental principles and ethical standards that guide every member of our organization. We are committed to maintaining a workplace that is built on integrity, respect, and accountability.\\nIntegrity: We hold ourselves to the highest ethical standards. This means acting honestly and transparently in all our interactions, whether with colleagues, clients, or the broader community. We respect and protect sensitive information, and we avoid conflicts of interest.\\nRespect: We embrace diversity and value each individual's contributions. Discrimination, harassment, or any form of disrespectful behavior is unacceptable. We create an inclusive environment where differences are celebrated and everyone is treated with dignity and courtesy.\\nAccountability: We take responsibility for our actions and decisions. We follow all relevant laws and regulations, and we strive to continuously improve our practices. We report any potential violations of this code and support the investigation of such matters.\\nSafety: We prioritize the safety of our employees, clients, and the communities we serve. We maintain a culture of safety, including reporting any unsafe conditions or practices.\\nEnvironmental Responsibility: We are committed to minimizing our environmental footprint and promoting sustainable practices.\\nOur Code of Conduct is not just a set of rules; it is the foundation of our organization's culture. We expect all employees to uphold these principles and serve as role models for others, ensuring we maintain our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2.\\tRecruitment Policy\\n\\nOur Recruitment Policy reflects our commitment to attracting, selecting, and onboarding the most qualified and diverse candidates to join our organization. We believe that the success of our company relies on the talents, skills, and dedication of our employees.\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively promote diversity and inclusion.\\nTransparency: We maintain transparency in our recruitment processes. All job vacancies are advertised internally and externally when appropriate. Job descriptions and requirements are clear and accurately represent the role.\\nSelection Criteria: Our selection process is based on the qualifications, experience, and skills necessary for the position. Interviews and assessments are conducted objectively, and decisions are made without bias.\\nData Privacy: We are committed to protecting the privacy of candidates' personal information and adhere to all relevant data protection laws and regulations.\\nFeedback: Candidates will receive timely and constructive feedback on their application and interview performance.\\nOnboarding: New employees receive comprehensive onboarding to help them integrate into the organization effectively. This includes information on our culture, policies, and expectations.\\nEmployee Referrals: We encourage and appreciate employee referrals as they contribute to building a strong and engaged team.\\nOur Recruitment Policy is a foundation for creating a diverse, inclusive, and talented workforce. It ensures that we attract and hire the best candidates who align with our company values and contribute to our continued success. We continuously review and update this policy to reflect evolving best practices in recruitment.\\n\\n3.\\tInternet and Email Policy\\n\\nOur Internet and Email Policy is established to guide the responsible and secure use of these essential tools within our organization. We recognize their significance in daily business operations and the importance of adhering to principles that maintain security, productivity, and legal compliance.\\nAcceptable Use: Company-provided internet and email services are primarily meant for job-related tasks. Limited personal use is allowed during non-work hours, provided it doesn't interfere with work responsibilities.\\nSecurity: Safeguard your login credentials, avoiding the sharing of passwords. Exercise caution with email attachments and links from unknown sources. Promptly report any unusual online activity or potential security breaches.\\nConfidentiality: Reserve email for the transmission of confidential information, trade secrets, and sensitive customer data only when encryption is applied. Exercise discretion when discussing company matters on public forums or social media.\\nHarassment and Inappropriate Content: Internet and email usage must not involve harassment, discrimination, or the distribution of offensive or inappropriate content. Show respect and sensitivity to others in all online communications.\\nCompliance: Ensure compliance with all relevant laws and regulations regarding internet and email usage, including those related to copyright and data protection.\\nMonitoring: The company retains the right to monitor internet and email usage for security and compliance purposes.\\nConsequences: Policy violations may lead to disciplinary measures, including potential termination.\\nOur Internet and Email Policy aims to promote safe, responsible usage of digital communication tools that align with our values and legal obligations. Each employee is expected to understand and follow this policy. Regular reviews ensure its alignment with evolving technology and security standards.\\n\\n4.\\tMobile Phone Policy\\n\\nThe Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance.\\nAcceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations.\\nSecurity: Safeguard your mobile device and access credentials. Exercise caution when downloading apps or clicking links from unfamiliar sources. Promptly report security concerns or suspicious activities related to your mobile device.\\nConfidentiality: Avoid transmitting sensitive company information via unsecured messaging apps or emails. Be discreet when discussing company matters in public spaces.\\nCost Management: Keep personal phone usage separate from company accounts and reimburse the company for any personal charges on company-issued phones.\\nCompliance: Adhere to all pertinent laws and regulations concerning mobile phone usage, including those related to data protection and privacy.\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\nConsequences: Non-compliance with this policy may lead to disciplinary actions, including the potential loss of mobile phone privileges.\\nThe Mobile Phone Policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices.\\n\\n5.\\tSmoking Policy\\n\\nPolicy Purpose: The Smoking Policy has been established to provide clear guidance and expectations concerning smoking on company premises. This policy is in place to ensure a safe and healthy environment for all employees, visitors, and the general public.\\nDesignated Smoking Areas: Smoking is only permitted in designated smoking areas, as marked by appropriate signage. These areas have been chosen to minimize exposure to secondhand smoke and to maintain the overall cleanliness of the premises.\\nSmoking Restrictions: Smoking inside company buildings, offices, meeting rooms, and other enclosed spaces is strictly prohibited. This includes electronic cigarettes and vaping devices.\\nCompliance with Applicable Laws: All employees and visitors must adhere to relevant federal, state, and local smoking laws and regulations.\\nDisposal of Smoking Materials: Properly dispose of cigarette butts and related materials in designated receptacles. Littering on company premises is prohibited.\\nNo Smoking in Company Vehicles: Smoking is not permitted in company vehicles, whether they are owned or leased, to maintain the condition and cleanliness of these vehicles.\\nEnforcement and Consequences: All employees and visitors are expected to adhere to this policy. Non-compliance may lead to appropriate disciplinary action, which could include fines, or, in the case of employees, possible termination of employment.\\nReview of Policy: This policy will be reviewed periodically to ensure its alignment with evolving legal requirements and best practices for maintaining a healthy and safe workplace.\\nWe appreciate your cooperation in maintaining a smoke-free and safe environment for all.\\n\\n6.\\tDrug and Alcohol Policy\\n\\nPolicy Objective: The Drug and Alcohol Policy is established to establish clear expectations and guidelines for the responsible use of drugs and alcohol within the organization. This policy aims to maintain a safe, healthy, and productive workplace.\\nProhibited Substances: The use, possession, distribution, or sale of illegal drugs or unauthorized controlled substances is strictly prohibited on company premises or during work-related activities. This includes the misuse of prescription drugs.\\nAlcohol Consumption: The consumption of alcoholic beverages is not allowed during work hours, on company property, or while performing company-related duties. Exception may be made for company-sanctioned events.\\nImpairment: Employees are expected to perform their job duties without impairment from drugs or alcohol. The use of substances that could impair job performance or pose a safety risk is prohibited.\\nTesting and Searches: The organization reserves the right to conduct drug and alcohol testing as per applicable laws and regulations. Employees may be subject to testing in cases of reasonable suspicion, post-accident, or as part of routine workplace safety measures.\\nReporting: Employees should report any concerns related to drug or alcohol misuse by themselves or their colleagues, as well as safety concerns arising from such misuse.\\nTreatment and Assistance: Employees with substance abuse issues are encouraged to seek help. The organization is committed to providing support, resources, and information to assist those seeking treatment.\\nConsequences: Violation of this policy may result in disciplinary actions, up to and including termination of employment. Legal action may also be pursued when necessary.\\nPolicy Review: This policy will undergo periodic review to ensure its continued relevance and compliance with evolving legal requirements and best practices for a safe and productive work environment.\\nYour adherence to this policy is appreciated as it helps to maintain a safe and drug-free workplace for all.\\n\\n7.\\tHealth and Safety Policy\\n\\nOur commitment to health and safety is paramount. We prioritize the well-being of our employees, customers, and the public. We diligently comply with all relevant health and safety laws and regulations. Our objective is to maintain a workplace free from hazards, preventing accidents, injuries, and illnesses. Every individual within our organization is responsible for upholding these standards. We regularly assess and improve our safety measures, provide adequate training, and encourage open communication regarding safety concerns. Through collective dedication, we aim to ensure a safe, healthy, and secure environment for all. Your cooperation is essential in achieving this common goal.\\n\\n8.\\tAnti-discrimination and Harassment Policy\\n\\nThe Anti-Discrimination and Harassment Policy is a testament to the commitment of this organization in fostering a workplace that is free from discrimination, harassment, and any form of unlawful bias. This policy applies to every individual within the organization, including employees, contractors, visitors, and clients.\\nNon-Discrimination: This organization strictly prohibits discrimination based on race, color, religion, gender, national origin, age, disability, sexual orientation, or any other legally protected characteristic in all aspects of employment, including recruitment, hiring, compensation, benefits, promotions, and terminations.\\nHarassment: Harassment in any form, whether based on the aforementioned characteristics or any other protected status, is unacceptable. This encompasses unwelcome advances, offensive jokes, slurs, and other verbal or physical conduct that creates a hostile or intimidating work environment.\\nReporting: Individuals who experience or witness any form of discrimination or harassment are encouraged to promptly report the incident to their supervisor, manager, or the designated HR representative. The organization is committed to a timely and confidential investigation of such complaints.\\nConsequences: Violation of this policy may result in disciplinary action, including termination of employment. The organization is committed to taking appropriate action against any individual found to be in violation of this policy.\\nReview and Update: This policy is subject to regular review and update to remain aligned with evolving legal requirements and best practices in preventing discrimination and harassment. This organization considers it a collective responsibility to ensure a workplace free from discrimination and harassment, and it is essential that every individual within the organization plays their part in upholding these principles.\\n\\n9.\\tDiscipline and Termination Policy\\n\\nThe Discipline and Termination Policy underscores the organization's commitment to maintaining a productive, ethical, and respectful work environment. This policy applies to all personnel, including employees, contractors, and temporary staff.\\nPerformance and Conduct Expectations: Employees are expected to meet performance standards and adhere to conduct guidelines. The organization will provide clear expectations, feedback, and opportunities for improvement when performance or conduct issues arise.\\nDisciplinary Actions: When necessary, disciplinary actions will be taken, which may include verbal warnings, written warnings, suspension, or other appropriate measures. Disciplinary actions are designed to address issues constructively and maintain performance standards.\\nTermination: In situations where an employee's performance or conduct issues persist, the organization may resort to termination. Termination may also occur for reasons such as redundancy, violation of policies, or restructuring.\\nTermination Procedure: The organization will follow appropriate procedures, ensuring fairness and adherence to legal requirements during the termination process. Employees may be eligible for notice periods, severance pay, or other benefits as per employment agreements and applicable laws.\\nExit Process: The organization will conduct an exit process to ensure a smooth transition for departing employees, including the return of company property, final pay, and cancellation of access and benefits.\\nThis policy serves as a framework for handling discipline and termination. The organization recognizes the importance of fairness and consistency in these processes, and decisions will be made after careful consideration. Every employee is expected to understand and adhere to this policy, contributing to a respectful and productive workplace. Regular reviews will ensure its alignment with evolving legal requirements and best practices.\\n\")]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data"
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00a19a3f-c5dd-4c5d-80e6-6b4a53fc8302"
      },
      "source": [
        "### Split data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9076b523-191b-48b6-9f7a-511315b6c1ed"
      },
      "source": [
        "The next step is to split the document using LangChain's text splitter. Here, you will use the `RecursiveCharacterTextSplitter, which is well-suited for this generic text. The following parameters have been set:\n",
        "\n",
        "- `chunk_size = 100`\n",
        "- `chunk_overlap = 20`\n",
        "- `length_function = len`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93b0e970-c72d-4f77-942e-63443de822f6"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9e9dcce-6e7c-4789-a203-bbc7cf94ea87"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        ")"
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03249315-6ef6-4c7c-8c03-6958e5bcc645"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_documents(data)"
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "897d990f-e0ed-481b-81d1-9a20ea33fbb8"
      },
      "source": [
        "Let's take a look at how many chunks you get.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eea4673d-e12b-4d57-a3b1-0fab782e9a20",
        "outputId": "9f090146-c52a-44fa-b967-0ac27545b0b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(chunks)"
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47eb9f45-ffa6-4cbb-870d-586d6abb9a33"
      },
      "source": [
        "So, in total, you get 215 chunks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb238e65-e588-40fa-a13f-aab5506844b4"
      },
      "source": [
        "### Embedding model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d9bc25e-ef62-42e7-9a42-a4d6a9e553e0"
      },
      "source": [
        "The following code demonstrates how to build an embedding model using the `watsonx.ai` package.\n",
        "\n",
        "For this project, the `ibm/slate-125m-english-rtrvr` embedding model will be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92503654-367b-4c72-8dd6-78262c05a49f"
      },
      "outputs": [],
      "source": [
        "xxx - skip\n",
        "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
        "from langchain_ibm import WatsonxEmbeddings"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "152323f9-8a34-473a-aaba-5022f7bae613"
      },
      "outputs": [],
      "source": [
        "xxx - skip\n",
        "embed_params = {\n",
        "    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
        "    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
        "}\n",
        "\n",
        "watsonx_embedding = WatsonxEmbeddings(\n",
        "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
        "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "    project_id=\"skills-network\",\n",
        "    params=embed_params,\n",
        ")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
        "from langchain_ibm import WatsonxEmbeddings\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "#    watsonx_api_key = userdata.get('WATSONX_APIKEY')\n",
        " # In Google Colab, use the built-in secrets feature\n",
        "    watsonx_api_key = userdata.get('IBM_API_KEY')\n",
        "    ibm_project_id = userdata.get('IBM_PROJECT_ID')\n",
        "except:\n",
        "    watsonx_api_key = getpass(\"Enter your WATSONX_APIKEY: \")\n",
        "\n",
        "# Configure embedding parameters\n",
        "embed_params = {\n",
        "    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
        "    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
        "}\n",
        "\n",
        "print(watsonx_api_key)\n",
        "\n",
        "# Create WatsonxEmbeddings with API key\n",
        "watsonx_embedding = WatsonxEmbeddings(\n",
        "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
        "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "    project_id=ibm_project_id,\n",
        "    apikey=watsonx_api_key,  # Add the API key here\n",
        "    params=embed_params,\n",
        ")\n",
        "\n",
        "print(\"✓ WatsonxEmbeddings configured successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMneruQHR4Mu",
        "outputId": "4bdbd793-8cd7-4a4e-e0c1-657a04a04f8f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am7HHaQuCo5s3jC39twjFp5U2VihaRi_fKpPc2UbH_8-\n",
            "✓ WatsonxEmbeddings configured successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbcb8a14-f47a-408e-897f-690ac1aff0eb"
      },
      "source": [
        "The embedding model is formed into the `watsonx_embedding` object.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d070cf31-a874-437a-a24b-fb2b73f7df48"
      },
      "source": [
        "## Vector store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8a4514-0b2b-4176-a7e0-b85ee7ae0b08"
      },
      "source": [
        "In this section, you will be guided on how to use two commonly used vector databases: Chroma DB and FAISS DB. You will also see how to perform a similarity search based on an input query using these databases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "481d21fa-6649-4b31-964c-25c2feedff99"
      },
      "source": [
        "### Chroma DB\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49725411-24cb-4bea-bfc7-203496f823c6"
      },
      "source": [
        "#### Build the database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dca56cc-236f-4884-b387-9230768bc5e3"
      },
      "source": [
        "First, you need to import `Chroma` from Langchain vector stores.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "250829ac-e8a5-4a5f-91ea-f2046600da6c"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma"
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f994b353-4e3e-4858-ae73-ecf76b55bb0a"
      },
      "source": [
        "Next, you need to create an ID list that will be used to assign each chunk a unique identifier, allowing you to track them later in the vector database. The length of this list should match the length of the chunks.\n",
        "\n",
        "Note: The IDs should be in string format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39a9e5ff-0824-4eff-a967-da5760f99643",
        "outputId": "e797cc3e-d3af-4ac5-f728-de5fd99a303f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214']\n"
          ]
        }
      ],
      "source": [
        "ids = [str(i) for i in range(0, len(chunks))]\n",
        "print(ids)"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1d4b399-6848-4aa9-a61d-db7f5dc0c71f"
      },
      "source": [
        "The next step is to use the embedding model to create embeddings for each chunk and then store them in the Chroma database.\n",
        "\n",
        "The following code demonstrates how to do this.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "476eba86-ffb2-408d-bf5d-cfde1af56028",
        "outputId": "9a12faae-832a-44e6-d964-c13cf118eea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ibm_watsonx_ai.wml_client_error:Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2024-05-10)\n",
            "Status code: 429, body: {\"errors\":[{\"code\":\"rate_limit_reached_requests\",\"message\":\"Rate limit of 2 requests per 1s was reached for instance id 491921c2-e8d0-47b4-84f9-fc00517474e3 (user RedHat-7121551, plan lite)\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-embeddings\"}],\"trace\":\"301dec410afc512a9f6b44536e080152\",\"status_code\":429}\n",
            "WARNING:ibm_watsonx_ai.wml_client_error:Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2024-05-10)\n",
            "Status code: 429, body: {\"errors\":[{\"code\":\"rate_limit_reached_requests\",\"message\":\"Rate limit of 2 requests per 1s was reached for instance id 491921c2-e8d0-47b4-84f9-fc00517474e3 (user RedHat-7121551, plan lite)\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-embeddings\"}],\"trace\":\"3531475570c2f8efdf2a2254af58273d\",\"status_code\":429}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ApiRequestFailure",
          "evalue": "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2024-05-10)\nStatus code: 429, body: {\"errors\":[{\"code\":\"rate_limit_reached_requests\",\"message\":\"Rate limit of 2 requests per 1s was reached for instance id 491921c2-e8d0-47b4-84f9-fc00517474e3 (user RedHat-7121551, plan lite)\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-embeddings\"}],\"trace\":\"301dec410afc512a9f6b44536e080152\",\"status_code\":429}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiRequestFailure\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-1660588027.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectordb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatsonx_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m    791\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             ):\n\u001b[0;32m--> 748\u001b[0;31m                 chroma_collection.add_texts(\n\u001b[0m\u001b[1;32m    749\u001b[0m                     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                     \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;31m# fill metadatas with empty dicts if somebody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_ibm/embeddings.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;34m\"\"\"Embed search docs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatsonx_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/embeddings/embeddings.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts, params, concurrency_limit)\u001b[0m\n\u001b[1;32m    238\u001b[0m         return [\n\u001b[1;32m    239\u001b[0m             \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             for vector in self.generate(\n\u001b[0m\u001b[1;32m    241\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcurrency_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcurrency_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             ).get(\"results\", [{}])\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/embeddings/embeddings.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, params, concurrency_limit)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0minputs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_splited\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconcurrency_limit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                         response_batch = list(\n\u001b[0m\u001b[1;32m    181\u001b[0m                             executor.map(\n\u001b[1;32m    182\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    617\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/embeddings/embeddings.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, params, concurrency_limit)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mgenerate_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHrefDefinitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fm_embeddings_href\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     def embed_documents(\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/embeddings/embeddings.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, generate_url, inputs, params)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"generate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_scoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/ibm_watsonx_ai/wml_resource.py\u001b[0m in \u001b[0;36m_handle_response\u001b[0;34m(self, expected_status_code, operationName, response, json_response)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             raise ApiRequestFailure(\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\"Failure during {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperationName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             )\n",
            "\u001b[0;31mApiRequestFailure\u001b[0m: Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/embeddings?version=2024-05-10)\nStatus code: 429, body: {\"errors\":[{\"code\":\"rate_limit_reached_requests\",\"message\":\"Rate limit of 2 requests per 1s was reached for instance id 491921c2-e8d0-47b4-84f9-fc00517474e3 (user RedHat-7121551, plan lite)\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai#text-embeddings\"}],\"trace\":\"301dec410afc512a9f6b44536e080152\",\"status_code\":429}"
          ]
        }
      ],
      "source": [
        "# skip as we must rate limit\n",
        "#vectordb = Chroma.from_documents(chunks, watsonx_embedding, ids=ids)"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from typing import List\n",
        "\n",
        "class RateLimitedEmbeddings:\n",
        "    def __init__(self, embedding_model, requests_per_second=1.5, batch_size=10):\n",
        "        \"\"\"\n",
        "        Wrapper around your embedding model that respects rate limits\n",
        "\n",
        "        Args:\n",
        "            embedding_model: Your watsonx embedding model\n",
        "            requests_per_second: Maximum requests per second (set below limit)\n",
        "            batch_size: Number of texts to embed in each request\n",
        "        \"\"\"\n",
        "        self.embedding_model = embedding_model\n",
        "        self.delay = 1.0 / requests_per_second  # Delay between requests\n",
        "        self.batch_size = batch_size\n",
        "        self.last_request_time = 0\n",
        "\n",
        "    def _wait_if_needed(self):\n",
        "        \"\"\"Wait if we need to respect rate limits\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "\n",
        "        if time_since_last < self.delay:\n",
        "            sleep_time = self.delay - time_since_last\n",
        "            print(f\"⏳ Rate limiting: waiting {sleep_time:.2f}s...\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed documents with rate limiting and batching\"\"\"\n",
        "        if not texts:\n",
        "            return []\n",
        "\n",
        "        all_embeddings = []\n",
        "        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "        print(f\"📊 Processing {len(texts)} texts in {total_batches} batches (batch size: {self.batch_size})\")\n",
        "\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch = texts[i:i + self.batch_size]\n",
        "            batch_num = i // self.batch_size + 1\n",
        "\n",
        "            print(f\"🔄 Processing batch {batch_num}/{total_batches} ({len(batch)} texts)...\")\n",
        "\n",
        "            # Wait if needed to respect rate limits\n",
        "            self._wait_if_needed()\n",
        "\n",
        "            try:\n",
        "                # Make the actual embedding request\n",
        "                batch_embeddings = self.embedding_model.embed_documents(batch)\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "                print(f\"✅ Batch {batch_num} completed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                if \"rate_limit\" in str(e).lower() or \"429\" in str(e):\n",
        "                    print(f\"⚠️ Rate limit hit in batch {batch_num}, waiting longer...\")\n",
        "                    time.sleep(2.0)  # Wait 2 seconds on rate limit\n",
        "                    # Retry the batch\n",
        "                    batch_embeddings = self.embedding_model.embed_documents(batch)\n",
        "                    all_embeddings.extend(batch_embeddings)\n",
        "                    print(f\"✅ Batch {batch_num} completed after retry\")\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        print(f\"🎉 All {len(texts)} texts embedded successfully!\")\n",
        "        return all_embeddings\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Embed a single query with rate limiting\"\"\"\n",
        "        self._wait_if_needed()\n",
        "        return self.embedding_model.embed_query(text)\n",
        "\n",
        "# Wrap your existing embedding model\n",
        "rate_limited_embedding = RateLimitedEmbeddings(\n",
        "    embedding_model=watsonx_embedding,\n",
        "    requests_per_second=1.5,  # Stay below 2 req/s limit\n",
        "    batch_size=10  # Embed 10 texts per request\n",
        ")\n",
        "\n",
        "# Now use the rate-limited version\n",
        "print(\"Creating vector database with rate limiting...\")\n",
        "vectordb = Chroma.from_documents(\n",
        "    chunks,\n",
        "    rate_limited_embedding,  # Use the rate-limited wrapper\n",
        "    ids=ids\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em15I4pnTIVO",
        "outputId": "ec54ccca-f501-42ac-f7e3-e3cefbe090c0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating vector database with rate limiting...\n",
            "📊 Processing 215 texts in 22 batches (batch size: 10)\n",
            "🔄 Processing batch 1/22 (10 texts)...\n",
            "✅ Batch 1 completed\n",
            "🔄 Processing batch 2/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.03s...\n",
            "✅ Batch 2 completed\n",
            "🔄 Processing batch 3/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.20s...\n",
            "✅ Batch 3 completed\n",
            "🔄 Processing batch 4/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.18s...\n",
            "✅ Batch 4 completed\n",
            "🔄 Processing batch 5/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.23s...\n",
            "✅ Batch 5 completed\n",
            "🔄 Processing batch 6/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.26s...\n",
            "✅ Batch 6 completed\n",
            "🔄 Processing batch 7/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.22s...\n",
            "✅ Batch 7 completed\n",
            "🔄 Processing batch 8/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.25s...\n",
            "✅ Batch 8 completed\n",
            "🔄 Processing batch 9/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.27s...\n",
            "✅ Batch 9 completed\n",
            "🔄 Processing batch 10/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.18s...\n",
            "✅ Batch 10 completed\n",
            "🔄 Processing batch 11/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.26s...\n",
            "✅ Batch 11 completed\n",
            "🔄 Processing batch 12/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.26s...\n",
            "✅ Batch 12 completed\n",
            "🔄 Processing batch 13/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.20s...\n",
            "✅ Batch 13 completed\n",
            "🔄 Processing batch 14/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.23s...\n",
            "✅ Batch 14 completed\n",
            "🔄 Processing batch 15/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.18s...\n",
            "✅ Batch 15 completed\n",
            "🔄 Processing batch 16/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.01s...\n",
            "✅ Batch 16 completed\n",
            "🔄 Processing batch 17/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.18s...\n",
            "✅ Batch 17 completed\n",
            "🔄 Processing batch 18/22 (10 texts)...\n",
            "✅ Batch 18 completed\n",
            "🔄 Processing batch 19/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.15s...\n",
            "✅ Batch 19 completed\n",
            "🔄 Processing batch 20/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.23s...\n",
            "✅ Batch 20 completed\n",
            "🔄 Processing batch 21/22 (10 texts)...\n",
            "⏳ Rate limiting: waiting 0.09s...\n",
            "✅ Batch 21 completed\n",
            "🔄 Processing batch 22/22 (5 texts)...\n",
            "⏳ Rate limiting: waiting 0.21s...\n",
            "✅ Batch 22 completed\n",
            "🎉 All 215 texts embedded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcb45613-d976-4f88-9d97-b6d479adfe68"
      },
      "source": [
        "Now that you have built the vector store named `vectordb`, you can use the method `.collection.get()` to print some of the chunks indexed by their IDs.\n",
        "\n",
        "Note: Although the chunks are stored in the database in embedding format, when you retrieve and print them by their IDs, the database will return the chunk text information instead of the embedding vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "895e9365-5322-435e-9fd2-27c95eaed92d",
        "outputId": "85ab36d1-4bd7-4329-d949-0b9b01333091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ids': ['0'], 'embeddings': None, 'metadatas': [{'source': 'companypolicies.txt'}], 'documents': ['1.\\tCode of Conduct'], 'uris': None, 'data': None}\n",
            "{'ids': ['1'], 'embeddings': None, 'metadatas': [{'source': 'companypolicies.txt'}], 'documents': ['Our Code of Conduct outlines the fundamental principles and ethical standards that guide every'], 'uris': None, 'data': None}\n",
            "{'ids': ['2'], 'embeddings': None, 'metadatas': [{'source': 'companypolicies.txt'}], 'documents': ['that guide every member of our organization. We are committed to maintaining a workplace that is'], 'uris': None, 'data': None}\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    print(vectordb._collection.get(ids=str(i)))"
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ee3294b-0e70-46be-bd4b-dbe0c8890a56"
      },
      "source": [
        "You can also use the method `._collection.count()` to see the length of the vector database, which should be the same as the length of chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "475cc6ce-8515-4672-b777-a1c2437ac978",
        "outputId": "cc3945b2-21b0-4c3b-b652-5e4a1e6e5d7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "vectordb._collection.count()"
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "270bdee9-6387-4efb-895d-075098fd1e64"
      },
      "source": [
        "#### Similarity search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81368674-b3ce-4d1d-98ee-840ddba805ae"
      },
      "source": [
        "Similarity search in a vector database involves finding items that are most similar to a given query item based on their vector representations.\n",
        "\n",
        "In this process, data objects are converted into vectors (which you've already done), and the search algorithm identifies and retrieves those with the closest vector distances to the query, enabling efficient and accurate identification of similar items in large datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a234543-3658-49b9-a31d-60e14119c22a"
      },
      "source": [
        "LangChain supports similarity search in vector stores using the method `.similarity_search()`.\n",
        "\n",
        "The following is an example of how to perform a similarity search based on the query \"Email policy.\"\n",
        "\n",
        "By default, it will return the top four closest vectors to the query.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d9ef1c7-26ac-40d2-b9af-3491d4713fc3",
        "outputId": "6c0a88b2-4a37-4150-cb3e-57b0b43ca8da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'companypolicies.txt'}, page_content='internet and email usage, including those related to copyright and data protection.'),\n",
              " Document(metadata={'source': 'companypolicies.txt'}, page_content='to this policy. Non-compliance may lead to appropriate disciplinary action, which could include'),\n",
              " Document(metadata={'source': 'companypolicies.txt'}, page_content='This policy serves as a framework for handling discipline and termination. The organization'),\n",
              " Document(metadata={'source': 'companypolicies.txt'}, page_content='Policy Purpose: The Smoking Policy has been established to provide clear guidance and expectations')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "query = \"Email policy\"\n",
        "docs = vectordb.similarity_search(query)\n",
        "docs"
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2a037db-62d4-48ee-a593-d956decd0a84"
      },
      "source": [
        "You can specify `k = 1` to just retrieve the top one result.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "358e01e4-de66-4364-a332-40795d784767",
        "outputId": "7233405e-2541-4bef-8297-bff7ba60ccb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'companypolicies.txt'}, page_content='internet and email usage, including those related to copyright and data protection.')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "vectordb.similarity_search(query, k = 1)"
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d522649-0d06-45de-a564-d9eaef0b6fde"
      },
      "source": [
        "### FIASS DB\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5450c0a-9cf6-48dd-b4c9-6a3c807cacc0"
      },
      "source": [
        "FIASS is another vector database that is supported by LangChain.\n",
        "\n",
        "The process of building and using FAISS is similar to Chroma DB.\n",
        "\n",
        "However, there may be differences in the retrieval results between FAISS and Chroma DB.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d60ee43-a488-434c-9224-3b01995605a8"
      },
      "source": [
        "#### Build the database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5091e8-4067-4f81-bb20-d35a09243913"
      },
      "source": [
        "Build the database and store the embeddings to the database here.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6487fd0d-337b-44fc-ae90-3521183f4bb8"
      },
      "outputs": [],
      "source": [
        "#from langchain_community.vectorstores import FAISS"
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44a8f718-39d1-487e-a7b1-8eb4aaff8425"
      },
      "outputs": [],
      "source": [
        "# - rate limited\n",
        "#from langchain_community.vectorstores import FAISS\n",
        "#faissdb = FAISS.from_documents(chunks, watsonx_embedding, ids=ids)"
      ],
      "execution_count": 24
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from typing import List\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "class RateLimitedEmbeddings:\n",
        "    def __init__(self, embedding_model, requests_per_second=1.5, batch_size=10):\n",
        "        \"\"\"\n",
        "        Wrapper around your embedding model that respects rate limits\n",
        "\n",
        "        Args:\n",
        "            embedding_model: Your watsonx embedding model\n",
        "            requests_per_second: Maximum requests per second (set below limit)\n",
        "            batch_size: Number of texts to embed in each request\n",
        "        \"\"\"\n",
        "        self.embedding_model = embedding_model\n",
        "        self.delay = 1.0 / requests_per_second  # Delay between requests\n",
        "        self.batch_size = batch_size\n",
        "        self.last_request_time = 0\n",
        "\n",
        "    def _wait_if_needed(self):\n",
        "        \"\"\"Wait if we need to respect rate limits\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "\n",
        "        if time_since_last < self.delay:\n",
        "            sleep_time = self.delay - time_since_last\n",
        "            print(f\"⏳ Rate limiting: waiting {sleep_time:.2f}s...\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed documents with rate limiting and batching\"\"\"\n",
        "        if not texts:\n",
        "            return []\n",
        "\n",
        "        all_embeddings = []\n",
        "        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "        print(f\"📊 Processing {len(texts)} texts in {total_batches} batches (batch size: {self.batch_size})\")\n",
        "\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch = texts[i:i + self.batch_size]\n",
        "            batch_num = i // self.batch_size + 1\n",
        "\n",
        "            print(f\"🔄 Processing batch {batch_num}/{total_batches} ({len(batch)} texts)...\")\n",
        "\n",
        "            # Wait if needed to respect rate limits\n",
        "            self._wait_if_needed()\n",
        "\n",
        "            try:\n",
        "                # Make the actual embedding request\n",
        "                batch_embeddings = self.embedding_model.embed_documents(batch)\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "                print(f\"✅ Batch {batch_num} completed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                if \"rate_limit\" in str(e).lower() or \"429\" in str(e):\n",
        "                    print(f\"⚠️ Rate limit hit in batch {batch_num}, waiting longer...\")\n",
        "                    time.sleep(2.0)  # Wait 2 seconds on rate limit\n",
        "                    # Retry the batch\n",
        "                    batch_embeddings = self.embedding_model.embed_documents(batch)\n",
        "                    all_embeddings.extend(batch_embeddings)\n",
        "                    print(f\"✅ Batch {batch_num} completed after retry\")\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        print(f\"🎉 All {len(texts)} texts embedded successfully!\")\n",
        "        return all_embeddings\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Embed a single query with rate limiting\"\"\"\n",
        "        self._wait_if_needed()\n",
        "        return self.embedding_model.embed_query(text)\n",
        "\n",
        "# Create rate-limited embedding wrapper\n",
        "rate_limited_embedding = RateLimitedEmbeddings(\n",
        "    embedding_model=watsonx_embedding,\n",
        "    requests_per_second=1.5,  # Stay below 2 req/s limit\n",
        "    batch_size=8  # Embed 8 texts per request\n",
        ")\n",
        "\n",
        "# Create FAISS database with rate limiting\n",
        "print(\"Creating FAISS vector database with rate limiting...\")\n",
        "faissdb = FAISS.from_documents(\n",
        "    chunks,\n",
        "    rate_limited_embedding,\n",
        "    ids=ids\n",
        ")\n",
        "print(\"✅ FAISS database created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPK4PE1tdE0M",
        "outputId": "ac6ef12a-7d42-4708-97cc-7c6c6673e314"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating FAISS vector database with rate limiting...\n",
            "📊 Processing 215 texts in 27 batches (batch size: 8)\n",
            "🔄 Processing batch 1/27 (8 texts)...\n",
            "✅ Batch 1 completed\n",
            "🔄 Processing batch 2/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.16s...\n",
            "✅ Batch 2 completed\n",
            "🔄 Processing batch 3/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.21s...\n",
            "✅ Batch 3 completed\n",
            "🔄 Processing batch 4/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.21s...\n",
            "✅ Batch 4 completed\n",
            "🔄 Processing batch 5/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.29s...\n",
            "✅ Batch 5 completed\n",
            "🔄 Processing batch 6/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.28s...\n",
            "✅ Batch 6 completed\n",
            "🔄 Processing batch 7/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.30s...\n",
            "✅ Batch 7 completed\n",
            "🔄 Processing batch 8/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.28s...\n",
            "✅ Batch 8 completed\n",
            "🔄 Processing batch 9/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.26s...\n",
            "✅ Batch 9 completed\n",
            "🔄 Processing batch 10/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.27s...\n",
            "✅ Batch 10 completed\n",
            "🔄 Processing batch 11/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.28s...\n",
            "✅ Batch 11 completed\n",
            "🔄 Processing batch 12/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.26s...\n",
            "✅ Batch 12 completed\n",
            "🔄 Processing batch 13/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.32s...\n",
            "✅ Batch 13 completed\n",
            "🔄 Processing batch 14/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.24s...\n",
            "✅ Batch 14 completed\n",
            "🔄 Processing batch 15/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.16s...\n",
            "✅ Batch 15 completed\n",
            "🔄 Processing batch 16/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.28s...\n",
            "✅ Batch 16 completed\n",
            "🔄 Processing batch 17/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.11s...\n",
            "✅ Batch 17 completed\n",
            "🔄 Processing batch 18/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.27s...\n",
            "✅ Batch 18 completed\n",
            "🔄 Processing batch 19/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.16s...\n",
            "✅ Batch 19 completed\n",
            "🔄 Processing batch 20/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.29s...\n",
            "✅ Batch 20 completed\n",
            "🔄 Processing batch 21/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.28s...\n",
            "✅ Batch 21 completed\n",
            "🔄 Processing batch 22/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.27s...\n",
            "✅ Batch 22 completed\n",
            "🔄 Processing batch 23/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.13s...\n",
            "✅ Batch 23 completed\n",
            "🔄 Processing batch 24/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.31s...\n",
            "✅ Batch 24 completed\n",
            "🔄 Processing batch 25/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.29s...\n",
            "✅ Batch 25 completed\n",
            "🔄 Processing batch 26/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.18s...\n",
            "✅ Batch 26 completed\n",
            "🔄 Processing batch 27/27 (7 texts)...\n",
            "⏳ Rate limiting: waiting 0.29s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch 27 completed\n",
            "🎉 All 215 texts embedded successfully!\n",
            "✅ FAISS database created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc4281a1-d46a-477b-868b-b0fd0a42c7ec"
      },
      "source": [
        "Next, print the first three information pieces in the database based on IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "146f2ab5-6ff3-47db-8d83-d00929f9fed4",
        "outputId": "399ddbef-9d0e-4e4b-a8c1-1813b3b337de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='1.\tCode of Conduct' metadata={'source': 'companypolicies.txt'}\n",
            "page_content='Our Code of Conduct outlines the fundamental principles and ethical standards that guide every' metadata={'source': 'companypolicies.txt'}\n",
            "page_content='that guide every member of our organization. We are committed to maintaining a workplace that is' metadata={'source': 'companypolicies.txt'}\n"
          ]
        }
      ],
      "source": [
        "for i in range(3):\n",
        "    print(faissdb.docstore.search(str(i)))"
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69564d1b-6344-4b01-bad0-ccf349fde381"
      },
      "source": [
        "#### Similarity search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63bf764f-69bd-4e1e-a6e2-2cece4951cd4"
      },
      "source": [
        "Let's do a similarity search again using FIASS DB on the same query.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "405703c8-41ed-448d-b56d-676f9be03264"
      },
      "outputs": [],
      "source": [
        "#query = \"Email policy\"\n",
        "#docs = faissdb.similarity_search(query)\n",
        "#docs"
      ],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from typing import List\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "class RateLimitedEmbeddings:\n",
        "    def __init__(self, embedding_model, requests_per_second=1.5, batch_size=10):\n",
        "        \"\"\"\n",
        "        Wrapper around your embedding model that respects rate limits\n",
        "        \"\"\"\n",
        "        self.embedding_model = embedding_model\n",
        "        self.delay = 1.0 / requests_per_second\n",
        "        self.batch_size = batch_size\n",
        "        self.last_request_time = 0\n",
        "\n",
        "    def _wait_if_needed(self):\n",
        "        \"\"\"Wait if we need to respect rate limits\"\"\"\n",
        "        current_time = time.time()\n",
        "        time_since_last = current_time - self.last_request_time\n",
        "\n",
        "        if time_since_last < self.delay:\n",
        "            sleep_time = self.delay - time_since_last\n",
        "            print(f\"⏳ Rate limiting: waiting {sleep_time:.2f}s...\")\n",
        "            time.sleep(sleep_time)\n",
        "\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "    def __call__(self, text):\n",
        "        \"\"\"Make the class callable for FAISS compatibility\"\"\"\n",
        "        return self.embed_query(text)\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Embed documents with rate limiting and batching\"\"\"\n",
        "        if not texts:\n",
        "            return []\n",
        "\n",
        "        all_embeddings = []\n",
        "        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
        "\n",
        "        print(f\"📊 Processing {len(texts)} texts in {total_batches} batches (batch size: {self.batch_size})\")\n",
        "\n",
        "        for i in range(0, len(texts), self.batch_size):\n",
        "            batch = texts[i:i + self.batch_size]\n",
        "            batch_num = i // self.batch_size + 1\n",
        "\n",
        "            print(f\"🔄 Processing batch {batch_num}/{total_batches} ({len(batch)} texts)...\")\n",
        "\n",
        "            self._wait_if_needed()\n",
        "\n",
        "            try:\n",
        "                batch_embeddings = self.embedding_model.embed_documents(batch)\n",
        "                all_embeddings.extend(batch_embeddings)\n",
        "                print(f\"✅ Batch {batch_num} completed\")\n",
        "\n",
        "            except Exception as e:\n",
        "                if \"rate_limit\" in str(e).lower() or \"429\" in str(e):\n",
        "                    print(f\"⚠️ Rate limit hit in batch {batch_num}, waiting longer...\")\n",
        "                    time.sleep(2.0)\n",
        "                    batch_embeddings = self.embedding_model.embed_documents(batch)\n",
        "                    all_embeddings.extend(batch_embeddings)\n",
        "                    print(f\"✅ Batch {batch_num} completed after retry\")\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        print(f\"🎉 All {len(texts)} texts embedded successfully!\")\n",
        "        return all_embeddings\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        \"\"\"Embed a single query with rate limiting\"\"\"\n",
        "        print(f\"🔍 Embedding query: '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
        "        self._wait_if_needed()\n",
        "\n",
        "        try:\n",
        "            result = self.embedding_model.embed_query(text)\n",
        "            print(f\"✅ Query embedded successfully\")\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            if \"rate_limit\" in str(e).lower() or \"429\" in str(e):\n",
        "                print(f\"⚠️ Rate limit hit on query, waiting 2s...\")\n",
        "                time.sleep(2.0)\n",
        "                result = self.embedding_model.embed_query(text)\n",
        "                print(f\"✅ Query embedded after retry\")\n",
        "                return result\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "# Create rate-limited embedding wrapper\n",
        "rate_limited_embedding = RateLimitedEmbeddings(\n",
        "    embedding_model=watsonx_embedding,\n",
        "    requests_per_second=1.5,\n",
        "    batch_size=8\n",
        ")\n",
        "\n",
        "# Create FAISS database with rate limiting\n",
        "print(\"Creating FAISS vector database with rate limiting...\")\n",
        "faissdb = FAISS.from_documents(\n",
        "    chunks,\n",
        "    rate_limited_embedding,\n",
        "    ids=ids\n",
        ")\n",
        "print(\"✅ FAISS database created successfully!\")\n",
        "\n",
        "# Now this will work\n",
        "query = \"Email policy\"\n",
        "docs = faissdb.similarity_search(query)\n",
        "print(f\"Found {len(docs)} similar documents\")\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEOLk7JddzK-",
        "outputId": "fa2d6786-ff4f-4547-c5ec-187c2e0a4c76"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating FAISS vector database with rate limiting...\n",
            "📊 Processing 215 texts in 27 batches (batch size: 8)\n",
            "🔄 Processing batch 1/27 (8 texts)...\n",
            "✅ Batch 1 completed\n",
            "🔄 Processing batch 2/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.16s...\n",
            "✅ Batch 2 completed\n",
            "🔄 Processing batch 3/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.19s...\n",
            "✅ Batch 3 completed\n",
            "🔄 Processing batch 4/27 (8 texts)...\n",
            "✅ Batch 4 completed\n",
            "🔄 Processing batch 5/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.30s...\n",
            "✅ Batch 5 completed\n",
            "🔄 Processing batch 6/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.28s...\n",
            "✅ Batch 6 completed\n",
            "🔄 Processing batch 7/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.30s...\n",
            "✅ Batch 7 completed\n",
            "🔄 Processing batch 8/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.26s...\n",
            "✅ Batch 8 completed\n",
            "🔄 Processing batch 9/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.29s...\n",
            "✅ Batch 9 completed\n",
            "🔄 Processing batch 10/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.27s...\n",
            "✅ Batch 10 completed\n",
            "🔄 Processing batch 11/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.31s...\n",
            "✅ Batch 11 completed\n",
            "🔄 Processing batch 12/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.05s...\n",
            "✅ Batch 12 completed\n",
            "🔄 Processing batch 13/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.26s...\n",
            "✅ Batch 13 completed\n",
            "🔄 Processing batch 14/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.27s...\n",
            "✅ Batch 14 completed\n",
            "🔄 Processing batch 15/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.25s...\n",
            "✅ Batch 15 completed\n",
            "🔄 Processing batch 16/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.29s...\n",
            "✅ Batch 16 completed\n",
            "🔄 Processing batch 17/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.14s...\n",
            "✅ Batch 17 completed\n",
            "🔄 Processing batch 18/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.24s...\n",
            "✅ Batch 18 completed\n",
            "🔄 Processing batch 19/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.27s...\n",
            "✅ Batch 19 completed\n",
            "🔄 Processing batch 20/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.29s...\n",
            "✅ Batch 20 completed\n",
            "🔄 Processing batch 21/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.25s...\n",
            "✅ Batch 21 completed\n",
            "🔄 Processing batch 22/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.26s...\n",
            "✅ Batch 22 completed\n",
            "🔄 Processing batch 23/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.29s...\n",
            "✅ Batch 23 completed\n",
            "🔄 Processing batch 24/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.30s...\n",
            "✅ Batch 24 completed\n",
            "🔄 Processing batch 25/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.30s...\n",
            "✅ Batch 25 completed\n",
            "🔄 Processing batch 26/27 (8 texts)...\n",
            "⏳ Rate limiting: waiting 0.26s...\n",
            "✅ Batch 26 completed\n",
            "🔄 Processing batch 27/27 (7 texts)...\n",
            "⏳ Rate limiting: waiting 0.30s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Batch 27 completed\n",
            "🎉 All 215 texts embedded successfully!\n",
            "✅ FAISS database created successfully!\n",
            "🔍 Embedding query: 'Email policy'\n",
            "⏳ Rate limiting: waiting 0.15s...\n",
            "✅ Query embedded successfully\n",
            "Found 4 similar documents\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'companypolicies.txt'}, page_content='internet and email usage, including those related to copyright and data protection.'),\n",
              " Document(metadata={'source': 'companypolicies.txt'}, page_content='to this policy. Non-compliance may lead to appropriate disciplinary action, which could include'),\n",
              " Document(metadata={'source': 'companypolicies.txt'}, page_content='This policy serves as a framework for handling discipline and termination. The organization'),\n",
              " Document(metadata={'source': 'companypolicies.txt'}, page_content='Policy Purpose: The Smoking Policy has been established to provide clear guidance and expectations')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e35a453d-7ae6-4755-acc8-7171cd06d471"
      },
      "source": [
        "The retrieve results based on the similarity search seem to be the same as with the Chroma DB.\n",
        "\n",
        "You can try with other queries or documents to see if they follow the same situation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "785159f1-9bb8-4a91-a76d-20bf168cb2f7"
      },
      "source": [
        "### Managing vector store: Adding, updating, and deleting entries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19446899-d6e8-457c-b538-94dabb84041e"
      },
      "source": [
        "There might be situations where new documents come into your RAG application that you want to add to the current vector database, or you might need to delete some existing documents from the database. Additionally, there may be updates to some of the documents in the database that require updating.\n",
        "\n",
        "The following sections will guide you on how to perform these tasks. You will use the Chroma DB as an example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a71722f-5d20-4629-b410-d78250608257"
      },
      "source": [
        "#### Add\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c363e12-79e9-466e-a361-c7967c63c6e6"
      },
      "source": [
        "Imagine you have a new piece of text information that you want to add to the vector database. First, this information should be formatted into a document object.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91be078e-64e5-437c-8e11-b4afa0de6142"
      },
      "outputs": [],
      "source": [
        "text = \"Instructlab is the best open source tool for fine-tuning a LLM.\""
      ],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b8e5cf6-f7bb-4290-9a9f-5ebe550faab6"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document"
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01e55ce5-a1a3-4699-b7a1-a15c5bb5a074"
      },
      "source": [
        "Form the text into a `Document` object named `new_chunk`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c913b458-f27e-45b6-8077-f76d3a7caa1f"
      },
      "outputs": [],
      "source": [
        "new_chunk =  Document(\n",
        "    page_content=text,\n",
        "    metadata={\n",
        "        \"source\": \"ibm.com\",\n",
        "        \"page\": 1\n",
        "    }\n",
        ")"
      ],
      "execution_count": 31
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f78ba89-dbdf-40d5-9096-321cbd437e97"
      },
      "source": [
        "Then, the new chunk should be put into a list as the vector database only accepts documents in a list.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f47ca996-f598-468f-b022-5f211f86582f"
      },
      "outputs": [],
      "source": [
        "new_chunks = [new_chunk]"
      ],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04580f15-b60b-4927-98af-194d0dfc0b0d"
      },
      "source": [
        "Before you add the document to the vector database, since there are 215 chunks with IDs from 0 to 214, if you print ID 215, the document should show no values. Let's validate it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6af4786a-20b7-4dd9-a49c-f24184e134ce",
        "outputId": "16d827e9-c5f4-4780-88c9-0ccb1c1a2935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ids': [], 'embeddings': None, 'metadatas': [], 'documents': [], 'uris': None, 'data': None}\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.get(ids=['215']))"
      ],
      "execution_count": 33
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec2f8f00-f255-48ec-99bc-865a4c474494"
      },
      "source": [
        "Next, you can use the method `.add_documents()` to add this `new_chunk`. In this method, you should assign an ID to the document. Since there are already IDs from 0 to 214, you can assign ID 215 to this document. The ID should be in string format and placed in a list.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ca87641-ffd6-46e3-9ce8-f3739bf6a207",
        "outputId": "3724e70b-aed5-4463-c934-ef2f7b93d784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Processing 1 texts in 1 batches (batch size: 10)\n",
            "🔄 Processing batch 1/1 (1 texts)...\n",
            "✅ Batch 1 completed\n",
            "🎉 All 1 texts embedded successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['215']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "vectordb.add_documents(\n",
        "    new_chunks,\n",
        "    ids=[\"215\"]\n",
        ")"
      ],
      "execution_count": 34
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dda0ee32-9875-4341-af67-862b0faedc48"
      },
      "source": [
        "Now you can count the length of the vector database again to see if it has increased by one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3db300f-353b-4e8b-89f7-99972d6e7090",
        "outputId": "dcf26491-1fc0-4244-c0d1-4c4b05666a0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "216"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "vectordb._collection.count()"
      ],
      "execution_count": 35
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee33982a-963b-4a4d-a4b7-61781026b260"
      },
      "source": [
        "You can then print this newly added document from the database by its ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39bc0fb9-2241-4071-8309-c3215ae3b516",
        "outputId": "c43a1db2-e4c7-4a56-e702-34c05605b251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ids': ['215'], 'embeddings': None, 'metadatas': [{'page': 1, 'source': 'ibm.com'}], 'documents': ['Instructlab is the best open source tool for fine-tuning a LLM.'], 'uris': None, 'data': None}\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.get(ids=['215']))"
      ],
      "execution_count": 36
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffcce84c-21ba-4071-bf95-e5fd581f2afc"
      },
      "source": [
        "#### Update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "529df9f9-ed4b-48e5-a0b4-ba8c626180ca"
      },
      "source": [
        "Imagine you want to update the content of a document that is already stored in the database. The following code demonstrates how to do this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "383acaca-e36a-41dc-88ba-6cb4bc3b06d5"
      },
      "source": [
        "Still, you need to form the updated text into a `Document` object.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "861e635d-04ce-4952-b5d4-90297b3d6f93"
      },
      "outputs": [],
      "source": [
        "update_chunk =  Document(\n",
        "    page_content=\"Instructlab is a perfect open source tool for fine-tuning a LLM.\",\n",
        "    metadata={\n",
        "        \"source\": \"ibm.com\",\n",
        "        \"page\": 1\n",
        "    }\n",
        ")"
      ],
      "execution_count": 37
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf921ae2-c863-4fe0-aef2-88f58a95951c"
      },
      "source": [
        "Then, you can use the method `.update_document()` to update the specific stored information indexing by its ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43a4e2da-47c7-4bf7-be21-1f98f8c1399b",
        "outputId": "ebe7e64b-177d-4230-8447-1c39e8b555a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Processing 1 texts in 1 batches (batch size: 10)\n",
            "🔄 Processing batch 1/1 (1 texts)...\n",
            "✅ Batch 1 completed\n",
            "🎉 All 1 texts embedded successfully!\n"
          ]
        }
      ],
      "source": [
        "vectordb.update_document(\n",
        "    '215',\n",
        "    update_chunk,\n",
        ")"
      ],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca62e639-13ff-4c04-8eb1-431833df2770",
        "outputId": "14a7e90d-30fa-47c8-b862-17fb227877d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ids': ['215'], 'embeddings': None, 'metadatas': [{'page': 1, 'source': 'ibm.com'}], 'documents': ['Instructlab is a perfect open source tool for fine-tuning a LLM.'], 'uris': None, 'data': None}\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.get(ids=['215']))"
      ],
      "execution_count": 39
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d54a461-993a-4169-9441-19173ce65626"
      },
      "source": [
        "As you can see, the document information has been updated.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbad37c7-46e1-4c0c-9ebf-c639a8236a29"
      },
      "source": [
        "#### Delete\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36605981-efb5-4b44-adf0-65051ab845ac"
      },
      "source": [
        "If you want to delete documents from the vector database, you can use the method `_collection.delete()` and specify the document ID to delete it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "138db3db-f8ba-421b-ba05-13c70221f299"
      },
      "outputs": [],
      "source": [
        "vectordb._collection.delete(ids=['215'])"
      ],
      "execution_count": 40
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2292a3b9-959b-480c-95fa-434a649d60c5",
        "outputId": "8f0f64c5-7532-45a8-8bf5-cae945e5357d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ids': [], 'embeddings': None, 'metadatas': [], 'documents': [], 'uris': None, 'data': None}\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.get(ids=['215']))"
      ],
      "execution_count": 41
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "214d3f80-ecdc-40b7-bb86-a060b2cf3b67"
      },
      "source": [
        "As you can see, now that document is empty.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1b6b32a-c206-482c-9433-4002b59d8e5a"
      },
      "source": [
        "# Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a2dfb4f-b5b6-4e06-a886-0b37b0f361ff"
      },
      "source": [
        "### Exercise 1 - Use another query to conduct similarity search.\n",
        "\n",
        "Can you use another query to conduct the similarity search?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7848909b-6b74-416b-b372-f3c0222d087f",
        "outputId": "7038911f-8ee9-42fc-8a08-336eee2cc293"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'companypolicies.txt'}, page_content='Smoking Restrictions: Smoking inside company buildings, offices, meeting rooms, and other enclosed'),\n",
              " Document(metadata={'source': 'companypolicies.txt'}, page_content='Designated Smoking Areas: Smoking is only permitted in designated smoking areas, as marked by'),\n",
              " Document(metadata={'source': 'companypolicies.txt'}, page_content='No Smoking in Company Vehicles: Smoking is not permitted in company vehicles, whether they are'),\n",
              " Document(metadata={'source': 'companypolicies.txt'}, page_content='Policy Purpose: The Smoking Policy has been established to provide clear guidance and expectations')]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "query = \"Smoking policy\"\n",
        "docs = vectordb.similarity_search(query)\n",
        "docs"
      ],
      "execution_count": 42
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0403a7b-4040-44a8-b136-ef50fed6e4c0"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for solution</summary>\n",
        "\n",
        "```python\n",
        "query = \"Smoking policy\"\n",
        "docs = vectordb.similarity_search(query)\n",
        "docs\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31da3550-ef23-463e-99d2-c3d4772c9c83"
      },
      "source": [
        "## Authors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a943633b-d7ba-4c08-b035-3672c5f6be69"
      },
      "source": [
        "[Kang Wang](https://author.skills.network/instructors/kang_wang)\n",
        "\n",
        "Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n",
        "\n",
        "[Cal Page](https://www.linkedin.com/in/cal-page-1084311/)\n",
        "\n",
        "Cal Page is a software engineering wizard who added rate limiting for IBM access along with pulling the token keys from the secret area on juptyr."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a883ff6-9308-45bb-ba45-edab92a53aa9"
      },
      "source": [
        "### Other Contributors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e7808a5-2d31-478f-a515-2b438387d4af"
      },
      "source": [
        "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo)\n",
        "\n",
        "Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "664d09ca-2b0f-4516-bf72-f0235ef5ba8b"
      },
      "source": [
        "```{## Change Log}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5c567ff-41c8-4997-851a-9d9e7cf8ad1f"
      },
      "source": [
        "```{|Date (YYYY-MM-DD)|Version|Changed By|Change Description||-|-|-|-||2024-07-24|0.1|Kang Wang|Create the lab|}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0522f239-8568-4b58-b655-c7c82cefbe9b"
      },
      "source": [
        "Copyright © IBM Corporation. All rights reserved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\n",
        "# This file is autogenerated by pip-compile with Python 3.12\n",
        "# by the following command:\n",
        "#\n",
        "#    pip-compile\n",
        "#\n",
        "aiohappyeyeballs==2.6.1\n",
        "    # via aiohttp\n",
        "aiohttp==3.12.13\n",
        "    # via\n",
        "    #   langchain\n",
        "    #   langchain-community\n",
        "aiosignal==1.3.2\n",
        "    # via aiohttp\n",
        "annotated-types==0.7.0\n",
        "    # via pydantic\n",
        "anyio==4.9.0\n",
        "    # via\n",
        "    #   httpx\n",
        "    #   starlette\n",
        "    #   watchfiles\n",
        "asgiref==3.8.1\n",
        "    # via opentelemetry-instrumentation-asgi\n",
        "attrs==25.3.0\n",
        "    # via aiohttp\n",
        "backoff==2.2.1\n",
        "    # via posthog\n",
        "bcrypt==4.3.0\n",
        "    # via chromadb\n",
        "build==1.2.2.post1\n",
        "    # via chromadb\n",
        "cachetools==5.5.2\n",
        "    # via google-auth\n",
        "certifi==2025.6.15\n",
        "    # via\n",
        "    #   httpcore\n",
        "    #   httpx\n",
        "    #   ibm-watsonx-ai\n",
        "    #   kubernetes\n",
        "    #   pulsar-client\n",
        "    #   requests\n",
        "charset-normalizer==3.4.2\n",
        "    # via requests\n",
        "chroma-hnswlib==0.7.3\n",
        "    # via chromadb\n",
        "chromadb==0.4.24\n",
        "    # via -r requirements.in\n",
        "click==8.2.1\n",
        "    # via\n",
        "    #   typer\n",
        "    #   uvicorn\n",
        "coloredlogs==15.0.1\n",
        "    # via onnxruntime\n",
        "dataclasses-json==0.6.7\n",
        "    # via langchain-community\n",
        "distro==1.9.0\n",
        "    # via posthog\n",
        "durationpy==0.10\n",
        "    # via kubernetes\n",
        "faiss-cpu==1.8.0\n",
        "    # via -r requirements.in\n",
        "fastapi==0.115.13\n",
        "    # via chromadb\n",
        "filelock==3.18.0\n",
        "    # via huggingface-hub\n",
        "flatbuffers==25.2.10\n",
        "    # via onnxruntime\n",
        "frozenlist==1.7.0\n",
        "    # via\n",
        "    #   aiohttp\n",
        "    #   aiosignal\n",
        "fsspec==2025.5.1\n",
        "    # via huggingface-hub\n",
        "google-auth==2.40.3\n",
        "    # via kubernetes\n",
        "googleapis-common-protos==1.70.0\n",
        "    # via opentelemetry-exporter-otlp-proto-grpc\n",
        "greenlet==3.2.3\n",
        "    # via sqlalchemy\n",
        "grpcio==1.73.0\n",
        "    # via\n",
        "    #   chromadb\n",
        "    #   opentelemetry-exporter-otlp-proto-grpc\n",
        "h11==0.16.0\n",
        "    # via\n",
        "    #   httpcore\n",
        "    #   uvicorn\n",
        "hf-xet==1.1.5\n",
        "    # via huggingface-hub\n",
        "httpcore==1.0.9\n",
        "    # via httpx\n",
        "httptools==0.6.4\n",
        "    # via uvicorn\n",
        "httpx==0.28.1\n",
        "    # via langsmith\n",
        "huggingface-hub==0.33.0\n",
        "    # via tokenizers\n",
        "humanfriendly==10.0\n",
        "    # via coloredlogs\n",
        "ibm-cos-sdk==2.13.6\n",
        "    # via ibm-watsonx-ai\n",
        "ibm-cos-sdk-core==2.13.6\n",
        "    # via\n",
        "    #   ibm-cos-sdk\n",
        "    #   ibm-cos-sdk-s3transfer\n",
        "ibm-cos-sdk-s3transfer==2.13.6\n",
        "    # via ibm-cos-sdk\n",
        "ibm-watsonx-ai==1.0.4\n",
        "    # via\n",
        "    #   -r requirements.in\n",
        "    #   langchain-ibm\n",
        "idna==3.10\n",
        "    # via\n",
        "    #   anyio\n",
        "    #   httpx\n",
        "    #   requests\n",
        "    #   yarl\n",
        "importlib-metadata==8.7.0\n",
        "    # via\n",
        "    #   ibm-watsonx-ai\n",
        "    #   opentelemetry-api\n",
        "importlib-resources==6.5.2\n",
        "    # via chromadb\n",
        "jmespath==1.0.1\n",
        "    # via\n",
        "    #   ibm-cos-sdk\n",
        "    #   ibm-cos-sdk-core\n",
        "jsonpatch==1.33\n",
        "    # via langchain-core\n",
        "jsonpointer==3.0.0\n",
        "    # via jsonpatch\n",
        "kubernetes==33.1.0\n",
        "    # via chromadb\n",
        "langchain==0.2.1\n",
        "    # via\n",
        "    #   -r requirements.in\n",
        "    #   langchain-community\n",
        "langchain-community==0.2.1\n",
        "    # via -r requirements.in\n",
        "langchain-core==0.2.43\n",
        "    # via\n",
        "    #   langchain\n",
        "    #   langchain-community\n",
        "    #   langchain-ibm\n",
        "    #   langchain-text-splitters\n",
        "langchain-ibm==0.1.7\n",
        "    # via -r requirements.in\n",
        "langchain-text-splitters==0.2.4\n",
        "    # via langchain\n",
        "langsmith==0.1.147\n",
        "    # via\n",
        "    #   langchain\n",
        "    #   langchain-community\n",
        "    #   langchain-core\n",
        "lomond==0.3.3\n",
        "    # via ibm-watsonx-ai\n",
        "markdown-it-py==3.0.0\n",
        "    # via rich\n",
        "marshmallow==3.26.1\n",
        "    # via dataclasses-json\n",
        "mdurl==0.1.2\n",
        "    # via markdown-it-py\n",
        "mmh3==5.1.0\n",
        "    # via chromadb\n",
        "mpmath==1.3.0\n",
        "    # via sympy\n",
        "multidict==6.5.0\n",
        "    # via\n",
        "    #   aiohttp\n",
        "    #   yarl\n",
        "mypy-extensions==1.1.0\n",
        "    # via typing-inspect\n",
        "numpy==1.26.4\n",
        "    # via\n",
        "    #   chroma-hnswlib\n",
        "    #   chromadb\n",
        "    #   faiss-cpu\n",
        "    #   langchain\n",
        "    #   langchain-community\n",
        "    #   onnxruntime\n",
        "    #   pandas\n",
        "oauthlib==3.3.1\n",
        "    # via\n",
        "    #   kubernetes\n",
        "    #   requests-oauthlib\n",
        "onnxruntime==1.22.0\n",
        "    # via chromadb\n",
        "opentelemetry-api==1.34.1\n",
        "    # via\n",
        "    #   chromadb\n",
        "    #   opentelemetry-exporter-otlp-proto-grpc\n",
        "    #   opentelemetry-instrumentation\n",
        "    #   opentelemetry-instrumentation-asgi\n",
        "    #   opentelemetry-instrumentation-fastapi\n",
        "    #   opentelemetry-sdk\n",
        "    #   opentelemetry-semantic-conventions\n",
        "opentelemetry-exporter-otlp-proto-common==1.34.1\n",
        "    # via opentelemetry-exporter-otlp-proto-grpc\n",
        "opentelemetry-exporter-otlp-proto-grpc==1.34.1\n",
        "    # via chromadb\n",
        "opentelemetry-instrumentation==0.55b1\n",
        "    # via\n",
        "    #   opentelemetry-instrumentation-asgi\n",
        "    #   opentelemetry-instrumentation-fastapi\n",
        "opentelemetry-instrumentation-asgi==0.55b1\n",
        "    # via opentelemetry-instrumentation-fastapi\n",
        "opentelemetry-instrumentation-fastapi==0.55b1\n",
        "    # via chromadb\n",
        "opentelemetry-proto==1.34.1\n",
        "    # via\n",
        "    #   opentelemetry-exporter-otlp-proto-common\n",
        "    #   opentelemetry-exporter-otlp-proto-grpc\n",
        "opentelemetry-sdk==1.34.1\n",
        "    # via\n",
        "    #   chromadb\n",
        "    #   opentelemetry-exporter-otlp-proto-grpc\n",
        "opentelemetry-semantic-conventions==0.55b1\n",
        "    # via\n",
        "    #   opentelemetry-instrumentation\n",
        "    #   opentelemetry-instrumentation-asgi\n",
        "    #   opentelemetry-instrumentation-fastapi\n",
        "    #   opentelemetry-sdk\n",
        "opentelemetry-util-http==0.55b1\n",
        "    # via\n",
        "    #   opentelemetry-instrumentation-asgi\n",
        "    #   opentelemetry-instrumentation-fastapi\n",
        "orjson==3.10.18\n",
        "    # via\n",
        "    #   chromadb\n",
        "    #   langsmith\n",
        "overrides==7.7.0\n",
        "    # via chromadb\n",
        "packaging==24.2\n",
        "    # via\n",
        "    #   build\n",
        "    #   huggingface-hub\n",
        "    #   ibm-watsonx-ai\n",
        "    #   langchain-core\n",
        "    #   marshmallow\n",
        "    #   onnxruntime\n",
        "    #   opentelemetry-instrumentation\n",
        "pandas==2.1.4\n",
        "    # via ibm-watsonx-ai\n",
        "posthog==5.4.0\n",
        "    # via chromadb\n",
        "propcache==0.3.2\n",
        "    # via\n",
        "    #   aiohttp\n",
        "    #   yarl\n",
        "protobuf==5.29.5\n",
        "    # via\n",
        "    #   googleapis-common-protos\n",
        "    #   onnxruntime\n",
        "    #   opentelemetry-proto\n",
        "pulsar-client==3.7.0\n",
        "    # via chromadb\n",
        "pyasn1==0.6.1\n",
        "    # via\n",
        "    #   pyasn1-modules\n",
        "    #   rsa\n",
        "pyasn1-modules==0.4.2\n",
        "    # via google-auth\n",
        "pydantic==2.11.7\n",
        "    # via\n",
        "    #   chromadb\n",
        "    #   fastapi\n",
        "    #   langchain\n",
        "    #   langchain-core\n",
        "    #   langsmith\n",
        "pydantic-core==2.33.2\n",
        "    # via pydantic\n",
        "pygments==2.19.1\n",
        "    # via rich\n",
        "pypika==0.48.9\n",
        "    # via chromadb\n",
        "pyproject-hooks==1.2.0\n",
        "    # via build\n",
        "python-dateutil==2.9.0.post0\n",
        "    # via\n",
        "    #   ibm-cos-sdk-core\n",
        "    #   kubernetes\n",
        "    #   pandas\n",
        "    #   posthog\n",
        "python-dotenv==1.1.0\n",
        "    # via uvicorn\n",
        "pytz==2025.2\n",
        "    # via pandas\n",
        "pyyaml==6.0.2\n",
        "    # via\n",
        "    #   chromadb\n",
        "    #   huggingface-hub\n",
        "    #   kubernetes\n",
        "    #   langchain\n",
        "    #   langchain-community\n",
        "    #   langchain-core\n",
        "    #   uvicorn\n",
        "requests==2.32.2\n",
        "    # via\n",
        "    #   chromadb\n",
        "    #   huggingface-hub\n",
        "    #   ibm-cos-sdk-core\n",
        "    #   ibm-watsonx-ai\n",
        "    #   kubernetes\n",
        "    #   langchain\n",
        "    #   langchain-community\n",
        "    #   langsmith\n",
        "    #   posthog\n",
        "    #   requests-oauthlib\n",
        "    #   requests-toolbelt\n",
        "requests-oauthlib==2.0.0\n",
        "    # via kubernetes\n",
        "requests-toolbelt==1.0.0\n",
        "    # via langsmith\n",
        "rich==14.0.0\n",
        "    # via typer\n",
        "rsa==4.9.1\n",
        "    # via google-auth\n",
        "shellingham==1.5.4\n",
        "    # via typer\n",
        "six==1.17.0\n",
        "    # via\n",
        "    #   kubernetes\n",
        "    #   lomond\n",
        "    #   posthog\n",
        "    #   python-dateutil\n",
        "sniffio==1.3.1\n",
        "    # via anyio\n",
        "sqlalchemy==2.0.41\n",
        "    # via\n",
        "    #   langchain\n",
        "    #   langchain-community\n",
        "starlette==0.46.2\n",
        "    # via fastapi\n",
        "sympy==1.14.0\n",
        "    # via onnxruntime\n",
        "tabulate==0.9.0\n",
        "    # via ibm-watsonx-ai\n",
        "tenacity==8.5.0\n",
        "    # via\n",
        "    #   chromadb\n",
        "    #   langchain\n",
        "    #   langchain-community\n",
        "    #   langchain-core\n",
        "tokenizers==0.21.1\n",
        "    # via chromadb\n",
        "tqdm==4.67.1\n",
        "    # via\n",
        "    #   chromadb\n",
        "    #   huggingface-hub\n",
        "typer==0.16.0\n",
        "    # via chromadb\n",
        "typing-extensions==4.14.0\n",
        "    # via\n",
        "    #   anyio\n",
        "    #   chromadb\n",
        "    #   fastapi\n",
        "    #   huggingface-hub\n",
        "    #   langchain-core\n",
        "    #   opentelemetry-api\n",
        "    #   opentelemetry-exporter-otlp-proto-grpc\n",
        "    #   opentelemetry-sdk\n",
        "    #   opentelemetry-semantic-conventions\n",
        "    #   pydantic\n",
        "    #   pydantic-core\n",
        "    #   sqlalchemy\n",
        "    #   typer\n",
        "    #   typing-inspect\n",
        "    #   typing-inspection\n",
        "typing-inspect==0.9.0\n",
        "    # via dataclasses-json\n",
        "typing-inspection==0.4.1\n",
        "    # via pydantic\n",
        "tzdata==2025.2\n",
        "    # via pandas\n",
        "urllib3==2.5.0\n",
        "    # via\n",
        "    #   ibm-cos-sdk-core\n",
        "    #   ibm-watsonx-ai\n",
        "    #   kubernetes\n",
        "    #   requests\n",
        "uvicorn[standard]==0.34.3\n",
        "    # via chromadb\n",
        "uvloop==0.21.0\n",
        "    # via uvicorn\n",
        "watchfiles==1.1.0\n",
        "    # via uvicorn\n",
        "websocket-client==1.8.0\n",
        "    # via kubernetes\n",
        "websockets==15.0.1\n",
        "    # via uvicorn\n",
        "wrapt==1.17.2\n",
        "    # via opentelemetry-instrumentation\n",
        "yarl==1.20.1\n",
        "    # via aiohttp\n",
        "zipp==3.23.0\n",
        "    # via importlib-metadata\n"
      ],
      "metadata": {
        "id": "kmWxKaUMegsE"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "prev_pub_hash": "6fda369b5a7cc6dc192cb9e100ce249a7aa42b806e29c8bd6f0a7d28a58a3dd7",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}