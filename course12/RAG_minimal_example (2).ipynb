{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Retriever-Augmented Generation (RAG) is a powerful architecture that **combines retrieval-based and generation-based approaches** to build more accurate and scalable NLP systems—especially for tasks like **open-domain question answering**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 What Is Retriever-Augmented Generation (RAG)?\n",
        "\n",
        "RAG enhances Large Language Models (LLMs) by **injecting external knowledge** dynamically at inference time. Instead of relying solely on the model’s internal parameters (which can be limited or outdated), RAG **retrieves relevant documents** from a large corpus and uses them to **inform the generation** of a final answer.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 General Flow of a RAG Pipeline\n",
        "\n",
        "Here’s the step-by-step flow:\n",
        "\n",
        "```\n",
        "  ┌───────────────────────┐\n",
        "  │     User Query        │\n",
        "  └─────────┬─────────────┘\n",
        "            ↓\n",
        "  ┌───────────────────────┐\n",
        "  │  Question Encoder (e.g. DPR) ─────┐\n",
        "  └───────────────────────┘           │\n",
        "            ↓                         ▼\n",
        "  ┌───────────────────────┐    ┌───────────────────────┐\n",
        "  │ FAISS or other index   │<───┤ Context Encoder (DPR) │\n",
        "  │ (Document Embeddings)  │    └───────────────────────┘\n",
        "  └─────────┬─────────────┘\n",
        "            │\n",
        "     Top-k Relevant Passages\n",
        "            ↓\n",
        "  ┌────────────────────────────┐\n",
        "  │ Concatenate query + context│\n",
        "  └─────────┬──────────────────┘\n",
        "            ↓\n",
        "  ┌────────────────────────────┐\n",
        "  │   Generator Model (e.g. GPT2)│\n",
        "  └─────────┬──────────────────┘\n",
        "            ↓\n",
        "      Final Generated Answer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Key Components of a RAG System\n",
        "\n",
        "### 1. **Retriever**\n",
        "\n",
        "* **Goal**: Find relevant documents/passages from a corpus.\n",
        "* **Common choice**: Dense Passage Retriever (DPR)\n",
        "* **Parts**:\n",
        "\n",
        "  * `DPRQuestionEncoder`: encodes the user query into a dense vector\n",
        "  * `DPRContextEncoder`: pre-encodes documents in the corpus\n",
        "* **Similarity metric**: FAISS Index with L2 or inner product (dot-product) distance\n",
        "\n",
        "### 2. **Generator**\n",
        "\n",
        "* **Goal**: Generate natural language output based on the query + retrieved context\n",
        "* **Common choice**: GPT-2, BART, T5, etc.\n",
        "* **Input**: Query + top-k retrieved passages (concatenated)\n",
        "* **Output**: Final response or answer\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 What Gets Trained?\n",
        "\n",
        "There are **two major training phases**, depending on the use case:\n",
        "\n",
        "### ▶️ **Pretraining (optional)**\n",
        "\n",
        "* You may start with pretrained models:\n",
        "\n",
        "  * DPR (retriever): already trained on QA tasks like Natural Questions.\n",
        "  * Generator: pretrained language model (e.g., GPT-2 or BART).\n",
        "\n",
        "### 🏋️‍♀️ **Fine-tuning**\n",
        "\n",
        "You can fine-tune:\n",
        "\n",
        "1. **Retriever**:\n",
        "\n",
        "   * Use **contrastive learning** (positive vs negative passages).\n",
        "   * Objective: bring questions closer to correct context embeddings.\n",
        "\n",
        "2. **Generator**:\n",
        "\n",
        "   * Fine-tune to condition on query + retrieved documents to generate better outputs.\n",
        "   * Loss: language modeling loss (e.g., cross-entropy).\n",
        "\n",
        "### 👥 End-to-End (optional but advanced):\n",
        "\n",
        "* Train both retriever and generator together.\n",
        "* This is **more complex** and **less stable**, so often not done unless needed.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Why Use RAG?\n",
        "\n",
        "### ✅ Advantages:\n",
        "\n",
        "* Reduces hallucination by grounding output in actual retrieved facts.\n",
        "* Extensible to large corpora without retraining the LLM.\n",
        "* Memory-efficient: doesn't force the model to memorize all world knowledge.\n",
        "\n",
        "### ❌ Limitations:\n",
        "\n",
        "* Retrieval quality heavily affects generation quality.\n",
        "* Concatenating long contexts can exceed model token limits (e.g., GPT2's 1024).\n",
        "* Retrieval + generation latency is higher than generation-only.\n",
        "\n",
        "---\n",
        "\n",
        "## Example in Plain Terms\n",
        "\n",
        "> Q: “What is our company’s mobile phone policy?”\n",
        "\n",
        "* **Step 1**: Question is encoded into a vector.\n",
        "* **Step 2**: That vector is used to **search a corpus** of HR documents using FAISS.\n",
        "* **Step 3**: Top 5 matching paragraphs are retrieved.\n",
        "* **Step 4**: These paragraphs are **fed into GPT-2** along with the original question.\n",
        "* **Step 5**: GPT-2 **generates an answer**, grounded in the retrieved data.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Component        | Model Type | Purpose                    | Trainable?    |\n",
        "| ---------------- | ---------- | -------------------------- | ------------- |\n",
        "| Question Encoder | DPR        | Encode user query          | Yes           |\n",
        "| Context Encoder  | DPR        | Encode documents for FAISS | Yes           |\n",
        "| Retriever Index  | FAISS      | Fast similarity search     | No (prebuilt) |\n",
        "| Generator        | GPT2/BART  | Generate final answer      | Yes           |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like a diagram or a PyTorch code version of a minimal RAG implementation?\n"
      ],
      "metadata": {
        "id": "pmx2MddQmoVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers faiss-cpu torch"
      ],
      "metadata": {
        "id": "jI_nMxu1mQBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "from transformers import (\n",
        "    DPRContextEncoder, DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n",
        "    AutoTokenizer, AutoModelForCausalLM\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 1: Prepare a small corpus of documents\n",
        "# -----------------------------------------------------------------------------------\n",
        "documents = [\n",
        "    \"The company offers 15 days of paid vacation per year.\",\n",
        "    \"Employees should submit reimbursement forms within 30 days.\",\n",
        "    \"Mobile phones must be secured with a company-approved password.\",\n",
        "    \"Remote work is allowed up to 3 days per week.\",\n",
        "    \"Drinking alcohol during work hours is strictly prohibited.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "FTQ4k9FO8g7o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 2: Load DPR Context Encoder and Tokenizer\n",
        "# These will convert documents into dense embeddings\n",
        "# -----------------------------------------------------------------------------------\n",
        "context_encoder =   DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "context_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "#context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# Tokenize and encode each document\n",
        "context_embeddings = []\n",
        "for doc in documents:\n",
        "    inputs = context_tokenizer(doc, return_tensors='pt', max_length=256, truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        embedding = context_encoder(**inputs).pooler_output  # (1, 768)\n",
        "    context_embeddings.append(embedding.cpu().numpy())\n",
        "\n",
        "# Stack into numpy array for FAISS\n",
        "context_embeddings_np = np.vstack(context_embeddings).astype('float32')  # Shape: (5, 768)\n",
        "\n",
        "# Show each document and the first 5 values of its embedding vector\n",
        "for i, (doc, emb) in enumerate(zip(documents, context_embeddings_np)):\n",
        "    print(f\"\\n📄 Document {i+1}:\")\n",
        "    print(doc)\n",
        "    print(f\"\\n🔢 First 5 values of embedding vector (shape: {emb.shape}):\")\n",
        "    print(emb[:5])  # Only show the first 5 values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZyXjI_n-jon",
        "outputId": "12e9ff2a-9492-4ed8-f926-cd4f201154e5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Document 1:\n",
            "The company offers 15 days of paid vacation per year.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[ 0.42181766  0.16109067  0.3723451  -0.09172494  0.28965074]\n",
            "\n",
            "📄 Document 2:\n",
            "Employees should submit reimbursement forms within 30 days.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[ 0.3361708   0.27786404  0.5407016  -0.33155122  0.07597482]\n",
            "\n",
            "📄 Document 3:\n",
            "Mobile phones must be secured with a company-approved password.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[0.24592635 0.49385566 0.31940734 0.09724129 0.7529858 ]\n",
            "\n",
            "📄 Document 4:\n",
            "Remote work is allowed up to 3 days per week.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[0.18697365 0.00790876 0.4691345  0.08455902 0.34109434]\n",
            "\n",
            "📄 Document 5:\n",
            "Drinking alcohol during work hours is strictly prohibited.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[ 0.58111304  0.6653758   0.17271869 -0.36189055 -0.12203238]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 3: Create FAISS index\n",
        "# This allows us to retrieve similar documents by vector similarity\n",
        "# -----------------------------------------------------------------------------------\n",
        "embedding_dim = context_embeddings_np.shape[1]\n",
        "index = faiss.IndexFlatL2(embedding_dim)  # L2 = Euclidean distance\n",
        "index.add(context_embeddings_np)  # Add document embeddings to index\n"
      ],
      "metadata": {
        "id": "NSeJ1OwW-1Az"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 4: Load DPR Question Encoder\n",
        "# This will embed the user query in the same vector space\n",
        "# -----------------------------------------------------------------------------------\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydiJueDj-7hL",
        "outputId": "b86bbb20-45f0-489b-d8f1-a2653107b3b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 5: Define a query and retrieve top-k relevant documents\n",
        "# -----------------------------------------------------------------------------------\n",
        "query = \"What is the mobile phone policy?\"\n",
        "\n",
        "# Tokenize and encode the question\n",
        "inputs = question_tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    query_embedding = question_encoder(**inputs).pooler_output.cpu().numpy()\n",
        "\n",
        "# Search FAISS for top 2 closest documents\n",
        "D, I = index.search(query_embedding, k=2)\n",
        "\n",
        "print(\"Top matching documents:\")\n",
        "for idx in I[0]:\n",
        "    print(\"-\", documents[idx])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkwV1lEo_B87",
        "outputId": "f8657628-7625-4107-c056-5a85e8a06d36"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matching documents:\n",
            "- Mobile phones must be secured with a company-approved password.\n",
            "- The company offers 15 days of paid vacation per year.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[User Query] ──▶ DPR Question Encoder ──▶ vector\n",
        "                                     │\n",
        "                                     ▼\n",
        "                           Search FAISS Index\n",
        "                                     │\n",
        "                                     ▼\n",
        "              Top-k Relevant Docs (raw text) ──▶ GPT-2 ──▶ Final Answer\n"
      ],
      "metadata": {
        "id": "wSBgzuMY-pj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 6: Load GPT-2 for generation\n",
        "# -----------------------------------------------------------------------------------\n",
        "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
        "\n",
        "gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "gpt_model.eval()\n",
        "\n",
        "# Set special token to avoid warnings\n",
        "gpt_model.generation_config.pad_token_id = gpt_tokenizer.eos_token_id\n"
      ],
      "metadata": {
        "id": "7SPUjvBk_EqS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 7a: Generate answer WITHOUT context\n",
        "# -----------------------------------------------------------------------------------\n",
        "def generate_without_context(query):\n",
        "    inputs = gpt_tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "    output = gpt_model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
        "    return gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nAnswer without context:\")\n",
        "print(generate_without_context(query))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRD6T46L_HCu",
        "outputId": "844e402e-bb32-43b8-ef74-6e4d3e11c49c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer without context:\n",
            "What is the mobile phone policy?\n",
            "\n",
            "The mobile phone policy is a policy that allows you to use your mobile phone for any purpose. It is a policy that allows you to use your mobile phone for any purpose. It is a policy that allows you to use your mobile phone for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 7a: Generate answer WITHOUT context\n",
        "# -----------------------------------------------------------------------------------\n",
        "def generate_without_context(query):\n",
        "    inputs = gpt_tokenizer(query, return_tensors=\"pt\", padding=True).to(device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    output = gpt_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=50\n",
        "    )\n",
        "    return gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\nAnswer without context:\")\n",
        "print(generate_without_context(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10nQ-aDFCIrG",
        "outputId": "656de8d4-e8bd-4356-aa2c-9327c1cce1d8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer without context:\n",
            "What is the mobile phone policy?\n",
            "\n",
            "The mobile phone policy is a policy that allows you to use your mobile phone for any purpose. It is a policy that allows you to use your mobile phone for any purpose. It is a policy that allows you to use your mobile phone for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great — you're now entering the **generation phase** of the RAG pipeline. Let’s walk through:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **Purpose of Step 7a:**\n",
        "\n",
        "You're **testing GPT-2 on its own**, without feeding it any extra knowledge (i.e., *not* using FAISS-retrieved documents).\n",
        "\n",
        "> 🎯 This is a baseline. You're asking:\n",
        "> “What does GPT-2 already know about the question just from pretraining?”\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 What This Code Does (Line by Line):\n",
        "\n",
        "### 1. **Define a function: `generate_without_context(query)`**\n",
        "\n",
        "```python\n",
        "def generate_without_context(query):\n",
        "```\n",
        "\n",
        "This function takes a **query string** (like `\"What is the mobile phone policy?\"`) and returns a GPT-2 generated response.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Tokenize the input**\n",
        "\n",
        "```python\n",
        "inputs = gpt_tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "```\n",
        "\n",
        "* Converts the string into token IDs using the **GPT-2 tokenizer**\n",
        "* Wraps in a PyTorch tensor\n",
        "* Moves it to the correct device (`cpu` or `cuda`)\n",
        "\n",
        "Example: `\"what is lunch?\"` → `[15496, 318, 17944, 30]`\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Generate a response**\n",
        "\n",
        "```python\n",
        "output = gpt_model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
        "```\n",
        "\n",
        "* Uses GPT-2 to **generate up to 50 new tokens** starting from the prompt\n",
        "* No extra knowledge — just what GPT-2 already learned during pretraining\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Decode the output tokens**\n",
        "\n",
        "```python\n",
        "return gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "```\n",
        "\n",
        "* Converts token IDs back into a human-readable string\n",
        "* Removes special tokens like \\`\\` if any\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Call the function and print**\n",
        "\n",
        "```python\n",
        "print(\"\\nAnswer without context:\")\n",
        "print(generate_without_context(query))\n",
        "```\n",
        "\n",
        "* Runs your function with the original query\n",
        "* Shows what GPT-2 says on its own\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Example Output\n",
        "\n",
        "Query:\n",
        "\n",
        "```python\n",
        "\"What is the mobile phone policy?\"\n",
        "```\n",
        "\n",
        "GPT-2 might say:\n",
        "\n",
        "```text\n",
        "\"The mobile phone policy may vary by organization. Employees are usually expected to keep phones off during meetings...\"\n",
        "```\n",
        "\n",
        "> But **it could also be vague, wrong, or hallucinated** — because it doesn't know your specific documents yet.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Why This Step Matters\n",
        "\n",
        "This is your **control group** in the experiment.\n",
        "\n",
        "Later, you'll compare it to:\n",
        "\n",
        "* ✅ `generate_with_context(retrieved_docs + query)`\n",
        "\n",
        "And see how **RAG improves accuracy** by injecting relevant info from FAISS.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary Table\n",
        "\n",
        "| Step                       | Purpose                                           |\n",
        "| -------------------------- | ------------------------------------------------- |\n",
        "| `generate_without_context` | Run GPT-2 by itself, no external help             |\n",
        "| Tokenizer                  | Converts text to token IDs for GPT-2              |\n",
        "| `generate(...)`            | GPT-2 makes predictions based on pretraining only |\n",
        "| Output                     | Baseline answer — can be vague, biased, or wrong  |\n",
        "\n",
        "---\n",
        "\n",
        "Let me know when you're ready to explain `generate_with_context()` — that's where RAG shines.\n"
      ],
      "metadata": {
        "id": "cqUfIdqHAbyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 7b: Generate answer WITH top-k context\n",
        "# -----------------------------------------------------------------------------------\n",
        "def generate_with_context(query, retrieved_docs):\n",
        "    full_input = query + \" \" + \" \".join(retrieved_docs)\n",
        "    inputs = gpt_tokenizer(full_input, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    output = gpt_model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
        "    return gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Use the top 2 FAISS matches\n",
        "retrieved = [documents[i] for i in I[0]]\n",
        "\n",
        "print(\"\\nAnswer with retrieved context:\")\n",
        "print(generate_with_context(query, retrieved))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbibYxEv_JSA",
        "outputId": "fe788e87-7b3d-4096-ac16-4483db88976e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer with retrieved context:\n",
            "What is the mobile phone policy? Mobile phones must be secured with a company-approved password. The company offers 15 days of paid vacation per year.\n",
            "\n",
            "What is the mobile phone policy? Mobile phones must be secured with a company-approved password. The company offers 15 days of paid vacation per year. What is the mobile phone policy? Mobile phones must be secured with a company-approved password\n"
          ]
        }
      ]
    }
  ]
}