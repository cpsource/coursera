{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9634585d-bd2d-46ac-aed7-9420b20e0a54"
      },
      "source": [
        "<p style=\"text-align:center\">\n",
        "    <a href=\"https://skills.network\" target=\"_blank\">\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
        "    </a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68d40459-635a-46a2-ae24-60e886dd509b"
      },
      "source": [
        "# **In-Context Engineering and Prompt Templates**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "671918e8-5415-430f-b023-52bf47f947d3"
      },
      "source": [
        "Estimated time needed: **30** minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbc8b5fe-21c1-42c4-866d-0e77032d5001"
      },
      "source": [
        "## Overview\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "293395ed-53d7-4649-855e-0a79f09c506e"
      },
      "source": [
        "You're stepping into the world of prompt engineering, where each command you craft has the power to guide intelligent LLM systems toward specific outcomes. In this tutorial, you will explore the foundational aspects of prompt engineering, dive into advanced techniques of in-context learning, such as few-shot and self-consistent learning, and learn how to effectively use tools like Langchain.\n",
        "\n",
        "Start by understanding the basics—how to formulate prompts that communicate effectively with AI. From there, we'll explore how the Langchain prompt template can simplify and enhance this process, making it more structured and efficient.\n",
        "\n",
        "As you progress, you'll learn to apply these skills in practical scenarios, creating sophisticated applications like QA bots and text summarization tools. By using the Langchain prompt template, you'll see firsthand how structured prompting can streamline the development of these applications, transforming complex requirements into clear, concise tasks for AI.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fae8a494-4815-4cab-b805-1b8d544fdae5"
      },
      "source": [
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ai8G4tOU4mksEYfv5wsghA/prompt%20engineering.png\" width=\"50%\" alt=\"indexing\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbe410d4-c898-4bdd-be30-ced0571f29a9"
      },
      "source": [
        "By the end of this tutorial, you'll not only master the different techniques of prompt engineering but also acquire hands-on experience in applying these techniques to real-world problems, ensuring you're well-prepared to harness the full potential of AI in various settings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e06a6b2-0fb2-4d2c-8893-19192ba59e66"
      },
      "source": [
        "## __Table of Contents__\n",
        "\n",
        "<ol>\n",
        "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
        "    <li>\n",
        "        <a href=\"#Setup\">Setup</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
        "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
        "            <li><a href=\"#Setup-LLM\">Setup LLM</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "    <li>\n",
        "        <a href=\"#Prompt-engineering\">Prompt engineering</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#First-basic-prompt\">First basic prompt</a></li>\n",
        "            <li><a href=\"#Zero-shot-prompt\">Zero-shot prompt</a></li>\n",
        "            <li><a href=\"#One-shot-prompt\">One-shot prompt</a></li>\n",
        "            <li><a href=\"#Few-shot-prompt\">Few-shot prompt</a></li>\n",
        "            <li><a href=\"#Chain-of-thought-(CoT)-prompting\">Chain-of-thought (CoT) prompting</a></li>\n",
        "            <li><a href=\"#Self-consistency\">Self-consistency</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "    <li>\n",
        "        <a href=\"#Applications\">Applications</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Prompt-template\">Prompt template</a></li>\n",
        "            <li><a href=\"#Text-summarization\">Text summarization</a></li>\n",
        "            <li><a href=\"#Question-answering\">Question answering</a></li>\n",
        "            <li><a href=\"#Text-classification\">Text classification</a></li>\n",
        "            <li><a href=\"#Code-generation\">Code generation</a></li>\n",
        "            <li><a href=\"#Role-playing\">Role playing</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "</ol>\n",
        "\n",
        "<a href=\"#Exercises\">Exercises</a>\n",
        "<ol>\n",
        "    <li><a href=\"#Exercise-1:-Change-parameters-for-the-LLM\">Exercise 1: Change parameters for the LLM</a></li>\n",
        "    <li><a href=\"#Exercise-2:-Observe-how-LLM-thinks\">Exercise 2: Observe how LLM thinks</a></li>\n",
        "    <li><a href=\"#Exercise-3:-Revise-the-text-classification-agent-to-one-shot-learning\">Exercise 3: Revise the text classification agent to one-shot learning</a></li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25fd114b-8779-4b5f-aa16-b8606b858d77"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "After completing this lab, you will be able to:\n",
        "\n",
        "- **Understand the basics of prompt engineering**: Gain a solid foundation in how to effectively communicate with LLM using prompts, setting the stage for more advanced techniques.\n",
        "\n",
        "- **Master advanced prompt techniques**: Learn and apply advanced prompt engineering techniques such as few-shot and self-consistent learning to optimize the LLM's response.\n",
        "\n",
        "- **Utilize LangChain prompt template**: Become proficient in using LangChain's prompt template to structure and optimize your interactions with LLM.\n",
        "\n",
        "- **Develop practical LLM agents**: Acquire the skills to create and implement agents such as QA bots and text summarization using the Langchain prompt template, translating theoretical knowledge into practical solutions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4f0f342-8d15-4cf3-8386-7998f889a0d0"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fac22a01-9328-4231-a2c1-55065d095e12"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "076c9af8-ebcd-4d01-9731-c6b3093976c0"
      },
      "source": [
        "For this lab, you will be using the following libraries:\n",
        "\n",
        "*   [`ibm-watsonx-ai`](https://ibm.github.io/watson-machine-learning-sdk/index.html) for using LLMs from IBM's watsonx.ai.\n",
        "*   [`langchain`](https://www.langchain.com/) for using langchain's different chain and prompt functions.\n",
        "*   [`langchain-ibm`](https://python.langchain.com/v0.1/docs/integrations/llms/ibm_watsonx/) provides integration between langchain and ibm-watsonx-ai.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56797ab7-dc97-4b08-a231-1c1d3804ee30"
      },
      "source": [
        "### Installing required libraries\n",
        "\n",
        "The following required libraries are __not__ preinstalled in the Skills Network Labs environment. __You must run the following cell__ to install them:\n",
        "\n",
        "**Note:** The version has been pinned here to specify the version. It's recommended that you do this as well. Even though the library will be updated in the future, the library could still support this lab work.\n",
        "\n",
        "This might take approximately 1-2 minutes.\n",
        "\n",
        "`%%capture` has been used to capture the installation, you won't see the output process. But once the installation is done, you will see a number beside the cell.\n",
        "\n",
        "***Note : After installing please ensure that you restart the kernel and execute the subsequent cells.***\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMuYmP7vC7Zi",
        "outputId": "68aeeda5-7829-4f96-b9db-63dfaf5fb88c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: anyio==4.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (4.9.0)\n",
            "Requirement already satisfied: certifi==2025.6.15 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer==3.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (3.4.2)\n",
            "Requirement already satisfied: greenlet==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (3.2.3)\n",
            "Requirement already satisfied: h11==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (0.16.0)\n",
            "Requirement already satisfied: httpcore==1.0.9 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (1.0.9)\n",
            "Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (0.28.1)\n",
            "Requirement already satisfied: ibm-cos-sdk==2.14.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (2.14.2)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.14.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (2.14.2)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.14.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (2.14.2)\n",
            "Requirement already satisfied: ibm-watsonx-ai==1.3.26 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (1.3.26)\n",
            "Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (3.10)\n",
            "Requirement already satisfied: jmespath==1.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 46)) (1.0.1)\n",
            "Requirement already satisfied: jsonpatch==1.33 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (1.33)\n",
            "Requirement already satisfied: jsonpointer==3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 52)) (3.0.0)\n",
            "Requirement already satisfied: langchain==0.3.25 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (0.3.25)\n",
            "Requirement already satisfied: langchain-core==0.3.65 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 56)) (0.3.65)\n",
            "Requirement already satisfied: langchain-ibm==0.3.12 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 61)) (0.3.12)\n",
            "Requirement already satisfied: langchain-text-splitters==0.3.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 63)) (0.3.8)\n",
            "Requirement already satisfied: langsmith==0.3.45 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 65)) (0.3.45)\n",
            "Requirement already satisfied: lomond==0.3.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 69)) (0.3.3)\n",
            "Requirement already satisfied: numpy==2.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 71)) (2.3.0)\n",
            "Requirement already satisfied: orjson==3.10.18 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 73)) (3.10.18)\n",
            "Requirement already satisfied: packaging==24.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 75)) (24.2)\n",
            "Requirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 80)) (2.2.3)\n",
            "Requirement already satisfied: pydantic==2.11.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 82)) (2.11.7)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 87)) (2.33.2)\n",
            "Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 89)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 93)) (2025.2)\n",
            "Requirement already satisfied: pyyaml==6.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 95)) (6.0.2)\n",
            "Requirement already satisfied: requests==2.32.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 99)) (2.32.4)\n",
            "Requirement already satisfied: requests-toolbelt==1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 106)) (1.0.0)\n",
            "Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 108)) (1.17.0)\n",
            "Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 112)) (1.3.1)\n",
            "Requirement already satisfied: sqlalchemy==2.0.41 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 114)) (2.0.41)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 116)) (0.9.0)\n",
            "Requirement already satisfied: tenacity==9.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 118)) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions==4.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 120)) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection==0.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 128)) (0.4.1)\n",
            "Requirement already satisfied: tzdata==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 130)) (2025.2)\n",
            "Requirement already satisfied: urllib3==2.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 132)) (2.5.0)\n",
            "Requirement already satisfied: zstandard==0.23.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 137)) (0.23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa7c26ea-ecb9-4756-a125-554a0662843c"
      },
      "source": [
        "### Importing required libraries\n",
        "\n",
        "_It is recommended that you import all required libraries in one place (here):_\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xxx\n",
        "#!pip install --upgrade ibm-watsonx-ai\n",
        "#!pip install --upgrade ibm-watsonx-ai\n",
        "!pip install langchain-ibm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAmwN4zaEoaR",
        "outputId": "d14a9555-ec69-495a-b44c-9839885079bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-ibm in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: ibm-watsonx-ai<2.0.0,>=1.3.18 in /usr/local/lib/python3.11/dist-packages (from langchain-ibm) (1.3.26)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.39 in /usr/local/lib/python3.11/dist-packages (from langchain-ibm) (0.3.65)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (0.28.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2.5.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2025.6.15)\n",
            "Requirement already satisfied: lomond in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (24.2)\n",
            "Requirement already satisfied: ibm-cos-sdk<2.15.0,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2.14.2)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.3.45)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (4.14.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.39->langchain-ibm) (2.11.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (0.16.0)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.14.2 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2.14.2)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.14.2 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2.14.2)\n",
            "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ibm-cos-sdk-core==2.14.2->ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2.9.0.post0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.3.45->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.39->langchain-ibm) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (3.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from lomond->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai<2.0.0,>=1.3.18->langchain-ibm) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watsonx_ai.foundation_models import Model\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
        "from langchain_ibm import WatsonxLLM  # Changed this line\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n"
      ],
      "metadata": {
        "id": "0FGEP1cqEV-Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from ibm_watsonx_ai.foundation_models import ModelInference\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from langchain_ibm import WatsonxLLM\n",
        "from google.colab import userdata\n",
        "\n",
        "# VALIDATE KEYS ONCE (run this cell first)\n",
        "def validate_keys():\n",
        "    IBM_API_KEY = userdata.get('IBM_API_KEY')\n",
        "    IBM_PROJECT_ID = userdata.get('IBM_PROJECT_ID')\n",
        "\n",
        "    print(\"✅ Keys loaded from secrets!\" if IBM_API_KEY else \"❌ No keys found\")\n",
        "\n",
        "    if IBM_API_KEY:\n",
        "        print(f\"API Key length: {len(IBM_API_KEY)}\")\n",
        "        print(f\"API Key starts with: {IBM_API_KEY[:10]}...\")\n",
        "        print(f\"Project ID: {IBM_PROJECT_ID}\")\n",
        "\n",
        "        # Check for common issues\n",
        "        if ' ' in IBM_API_KEY:\n",
        "            print(\"⚠️  WARNING: API key contains spaces!\")\n",
        "        if IBM_API_KEY.startswith('Bearer '):\n",
        "            print(\"⚠️  WARNING: Remove 'Bearer ' from the start of your API key\")\n",
        "        if len(IBM_API_KEY) < 30:\n",
        "            print(\"⚠️  WARNING: API key seems too short\")\n",
        "\n",
        "        return IBM_API_KEY, IBM_PROJECT_ID\n",
        "    return None, None\n",
        "\n",
        "# Run validation\n",
        "api_key, project_id = validate_keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoAzO68Fj7GS",
        "outputId": "5b3cf831-3bea-493a-8ba1-1e48dd830f6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Keys loaded from secrets!\n",
            "API Key length: 44\n",
            "API Key starts with: am7HHaQuCo...\n",
            "Project ID: 839fdc16-c311-4693-aaa0-120c337fe937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.colab import userdata\n",
        "\n",
        "def test_api_key():\n",
        "    \"\"\"Test if your IBM API key can get an authentication token\"\"\"\n",
        "\n",
        "    # Get your API key\n",
        "    IBM_API_KEY = userdata.get('IBM_API_KEY')\n",
        "\n",
        "    if not IBM_API_KEY:\n",
        "        print(\"❌ No API key found in secrets\")\n",
        "        return False\n",
        "\n",
        "    # Clean the key\n",
        "    IBM_API_KEY = IBM_API_KEY.strip()\n",
        "\n",
        "    print(f\"🔍 Testing API key: {IBM_API_KEY[:10]}...{IBM_API_KEY[-4:]}\")\n",
        "    print(f\"🔍 Key length: {len(IBM_API_KEY)}\")\n",
        "\n",
        "    # Test the key with IBM's token service\n",
        "    url = \"https://iam.cloud.ibm.com/identity/token\"\n",
        "    url = \"https://iam.cloud.ibm.com/identity/token\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
        "        \"Accept\": \"application/json\"\n",
        "    }\n",
        "    data = {\n",
        "        \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n",
        "        \"apikey\": IBM_API_KEY\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, data=data, timeout=10)\n",
        "\n",
        "        print(f\"🌐 Token request status: {response.status_code}\")\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            print(\"✅ API key is VALID! Token retrieved successfully.\")\n",
        "            token_data = response.json()\n",
        "            print(f\"🎫 Token type: {token_data.get('token_type', 'unknown')}\")\n",
        "            return True\n",
        "        else:\n",
        "            print(\"❌ API key is INVALID!\")\n",
        "            print(f\"📝 Error response: {response.text}\")\n",
        "\n",
        "            # Common error explanations\n",
        "            if response.status_code == 400:\n",
        "                print(\"\\n💡 Common causes of 400 error:\")\n",
        "                print(\"   • API key format is incorrect\")\n",
        "                print(\"   • API key contains extra characters/spaces\")\n",
        "                print(\"   • API key has been deleted or deactivated\")\n",
        "                print(\"   • Wrong type of key (not an IAM API key)\")\n",
        "\n",
        "            return False\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Network error: {e}\")\n",
        "        return False\n",
        "\n",
        "# Run the test\n",
        "test_api_key()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duQi1lKNlaxA",
        "outputId": "16cec81e-b763-460a-a2ee-c480fb852634"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Testing API key: am7HHaQuCo...H_8-\n",
            "🔍 Key length: 44\n",
            "🌐 Token request status: 200\n",
            "✅ API key is VALID! Token retrieved successfully.\n",
            "🎫 Token type: Bearer\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c22c9933-e5fd-47fd-bc05-c14a8e8881a0"
      },
      "source": [
        "### Setup LLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59954795-f1fd-4e51-88cf-319ca0968a3f"
      },
      "source": [
        "In this section, you will build an LLM model from IBM watsonx.ai.\n",
        "\n",
        "The following code initializes a Mixtral model on IBM's watsonx.ai platform and wraps it into a function that could allow repeat use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dcc7dd3-abe2-4874-8187-96e433242337"
      },
      "source": [
        "Some key parameters are explained here:\n",
        "- `model_id` specifies which model you want to use. There are various model options available; refer to the [Foundation Models](https://ibm.github.io/watsonx-ai-python-sdk/foundation_models.html) documentation for more options. In this tutorial, you'll use the `mixtral-8x7b-instruct-v01` model.\n",
        "- `parameters` define the model's configuration. Set five commonly used parameters for this tutorial. To explore additional commonly used parameters, you can run the code `GenParams().get_example_values()` to see. If no custom parameters are passed to the function, the model will use `default_params`.\n",
        "- `credentials` and `project_id` are necessary parameters to successfully run LLMs from watsonx.ai. (Keep `credentials` and `project_id` as they are now so that you do not need to create your own keys to run models. This supports you in running the model inside this lab environment. However, if you want to run the model locally, refer to this [tutorial](https://medium.com/the-power-of-ai/ibm-watsonx-ai-the-interface-and-api-e8e1c7227358) for creating your own keys.\n",
        "- `Model()` is used to wrap the parameters for the model and then call it with `WatsonxLLM()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46a184ca-ba47-4f66-9d03-5cc7a0691435"
      },
      "source": [
        "Run the following code, you will initialize a LLM model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c2e3207-1ce3-4fd9-920f-4584c405a89c"
      },
      "source": [
        "Let's run the following code to see some other commonly used parameters and their default value.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from langchain_ibm import WatsonxLLM\n",
        "from google.colab import userdata\n",
        "\n",
        "def llm_model(prompt_txt, params=None):\n",
        "    model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
        "\n",
        "    default_params = {\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"min_new_tokens\": 0,\n",
        "        \"temperature\": 0.5,\n",
        "        \"top_p\": 0.2,\n",
        "        \"top_k\": 1\n",
        "    }\n",
        "\n",
        "    if params:\n",
        "        default_params.update(params)\n",
        "\n",
        "    # Get credentials\n",
        "    IBM_API_KEY = userdata.get('IBM_API_KEY')\n",
        "    IBM_PROJECT_ID = userdata.get('IBM_PROJECT_ID')\n",
        "\n",
        "    # Validate and clean the keys\n",
        "    if not IBM_API_KEY or not IBM_PROJECT_ID:\n",
        "        raise ValueError(\"Missing IBM_API_KEY or IBM_PROJECT_ID in Colab secrets\")\n",
        "\n",
        "    IBM_API_KEY = IBM_API_KEY.strip()\n",
        "    IBM_PROJECT_ID = IBM_PROJECT_ID.strip()\n",
        "\n",
        "    # Create WatsonxLLM directly (no ModelInference wrapper needed)\n",
        "    mixtral_llm = WatsonxLLM(\n",
        "        model_id=model_id,\n",
        "        url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "        apikey=IBM_API_KEY,\n",
        "        project_id=IBM_PROJECT_ID,\n",
        "        params={\n",
        "            GenParams.MAX_NEW_TOKENS: default_params[\"max_new_tokens\"],\n",
        "            GenParams.MIN_NEW_TOKENS: default_params[\"min_new_tokens\"],\n",
        "            GenParams.TEMPERATURE: default_params[\"temperature\"],\n",
        "            GenParams.TOP_P: default_params[\"top_p\"],\n",
        "            GenParams.TOP_K: default_params[\"top_k\"]\n",
        "        }\n",
        "    )\n",
        "\n",
        "    response = mixtral_llm.invoke(prompt_txt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "9qqMgRTyxuZl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dcb28c4-041c-456e-b622-502900fe22bb",
        "outputId": "ac234fc9-248c-4aa3-8c00-b65f0008da18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'decoding_method': 'sample',\n",
              " 'length_penalty': {'decay_factor': 2.5, 'start_index': 5},\n",
              " 'temperature': 0.5,\n",
              " 'top_p': 0.2,\n",
              " 'top_k': 1,\n",
              " 'random_seed': 33,\n",
              " 'repetition_penalty': 2,\n",
              " 'min_new_tokens': 50,\n",
              " 'max_new_tokens': 200,\n",
              " 'stop_sequences': ['fail'],\n",
              " 'time_limit': 600000,\n",
              " 'truncate_input_tokens': 200,\n",
              " 'prompt_variables': {'object': 'brain'},\n",
              " 'return_options': {'input_text': True,\n",
              "  'generated_tokens': True,\n",
              "  'input_tokens': True,\n",
              "  'token_logprobs': True,\n",
              "  'token_ranks': False,\n",
              "  'top_n_tokens': False}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "GenParams().get_example_values()"
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b13bda73-68c0-4e94-b945-3dd02261b593"
      },
      "source": [
        "## Prompt engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42804af5-c62f-428f-ba81-05c9fa67c2b7"
      },
      "source": [
        "### First basic prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "018e53f5-3a5e-41fd-869c-d079deefed10"
      },
      "source": [
        "In this example, let's introduce a basic prompt that utilizes specific parameters to guide the language model's response. You'll then define a simple prompt and retrieve the model's response,\n",
        "\n",
        "The prompt used is \"The wind is\". Let the model generate itself.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7aa5fa5-82e1-4b60-92d7-419b8010232a",
        "outputId": "63078544-242c-4559-ff41-8898ef1d25a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: The wind is\n",
            "\n",
            "response :  howling outside, and the rain is coming down in sheets. It’s the perfect weather for a cozy night in with a good book. But what to read? If you’re looking for something to take your mind off the storm, here are five great books to curl up with on a rainy night.\n",
            "\n",
            "## 1. The Secret History by Donna Tartt\n",
            "\n",
            "This gripping novel tells the story of a group of classics students at an elite New England college who become embroiled in a murder conspiracy. With its richly drawn characters and atmospheric setting, The Secret History is\n",
            "\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "    \"max_new_tokens\": 128,\n",
        "    \"min_new_tokens\": 10,\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\": 0.2,\n",
        "    \"top_k\": 1\n",
        "}\n",
        "\n",
        "prompt = \"The wind is\"\n",
        "\n",
        "response = llm_model(prompt, params)\n",
        "print(f\"prompt: {prompt}\\n\")\n",
        "print(f\"response : {response}\\n\")"
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec83731e-4c11-4c3c-bcd1-cce5822a30a4"
      },
      "source": [
        "As you can see from the response, the model continues generating content following the initial prompt, \"The wind is\". You might notice that the response appears truncated or incomplete. This is because you have set the `max_new_tokens,` which restricts the number of tokens the model can generate.\n",
        "\n",
        "Try to adjust the parameters and observe the difference in the response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23d83359-4bfe-42a5-a145-3b20ee05129e"
      },
      "source": [
        "### Zero-shot prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d8b7f05-36b8-447d-9fd3-79ee04a60b98"
      },
      "source": [
        "Here is an example of a zero-shot prompt.\n",
        "\n",
        "Zero-shot learning is crucial for testing a model's ability to apply its pre-trained knowledge to new, unseen tasks without additional training. This capability is valuable for gauging the model's generalization skills.\n",
        "\n",
        "In this example, let's demonstrate a zero-shot learning scenario using a prompt that asks the model to classify a statement without any prior specific training on similar tasks. The prompt requests the model to assess the truthfulness of the statement: \"The Eiffel Tower is located in Berlin.\". After defining the prompt, you'll execute it with default parameters and print the response.\n",
        "\n",
        "This approach helps you understand how well the model can handle direct questions based on its underlying knowledge and reasoning abilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5edfb9f-2c5b-4362-b821-7d4a88d27cd0"
      },
      "source": [
        "Try running the prompt to see the model's capacity to correctly analyze and respond to factual inaccuracies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6d5d592-ee61-4366-b6ce-fc438dd48bfd",
        "outputId": "ee88a0d2-2641-4a61-8d27-046b21e2992c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: Classify the following statement as true or false: \n",
            "            'The Eiffel Tower is located in Berlin.'\n",
            "\n",
            "            Answer:\n",
            "\n",
            "\n",
            "response :             False. The Eiffel Tower is located in Paris, France, not Berlin, Germany.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following statement as true or false:\n",
        "            'The Eiffel Tower is located in Berlin.'\n",
        "\n",
        "            Answer:\n",
        "\"\"\"\n",
        "response = llm_model(prompt, params)\n",
        "print(f\"prompt: {prompt}\\n\")\n",
        "print(f\"response : {response}\\n\")"
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1195c10-cc03-4fe0-9374-f6bd69040a84"
      },
      "source": [
        "The model responds with the 'False' answer, which is correct. It also gives the reason for it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9a7f52f-f126-4fa6-a112-d5d1391e8370"
      },
      "source": [
        "### One-shot prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c001101b-5d0c-4bd1-b86a-f29e13cfd822"
      },
      "source": [
        "Here is a one-shot learning example where the model is given a single example to help guide its translation from English to French.\n",
        "\n",
        "The prompt provides a sample translation pairing, \"How is the weather today?\" translated to \"Comment est le temps aujourd'hui?\" This example serves as a guide for the model to understand the task context and desired format. The model is then tasked with translating a new sentence, \"Where is the nearest supermarket?\" without further guidance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0a0bf5f-a492-429a-88cf-924fbd90cf05",
        "outputId": "5febf476-9389-41fd-c241-968541447a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: Here is an example of translating a sentence from English to French:\n",
            "\n",
            "            English: “How is the weather today?”\n",
            "            French: “Comment est le temps aujourd'hui?”\n",
            "            \n",
            "            Now, translate the following sentence from English to French:\n",
            "            \n",
            "            English: “Where is the nearest supermarket?”\n",
            "            \n",
            "\n",
            "\n",
            "response :             French: “Où est le supermarché le plus proche?”\n",
            "\n",
            "           \n",
            "\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "    \"max_new_tokens\": 20,\n",
        "    \"temperature\": 0.1,\n",
        "}\n",
        "\n",
        "prompt = \"\"\"Here is an example of translating a sentence from English to French:\n",
        "\n",
        "            English: “How is the weather today?”\n",
        "            French: “Comment est le temps aujourd'hui?”\n",
        "\n",
        "            Now, translate the following sentence from English to French:\n",
        "\n",
        "            English: “Where is the nearest supermarket?”\n",
        "\n",
        "\"\"\"\n",
        "response = llm_model(prompt, params)\n",
        "print(f\"prompt: {prompt}\\n\")\n",
        "print(f\"response : {response}\\n\")"
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d5dd8de-35f6-430f-bb68-1be74f8e6dee"
      },
      "source": [
        "The model's response shows how it applies the structure and context provided by the initial example to translate the new sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b729237-32ec-4ed9-b3fc-d84c5f95e9aa"
      },
      "source": [
        "Consider experimenting with different sentences or adjusting the parameters to see how these changes impact the model's translations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18f40cfe-0be5-4fac-89be-45724dab5c24"
      },
      "source": [
        "### Few-shot prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4e1b2bb-f77d-499a-861a-c4570a5ce8f2"
      },
      "source": [
        "Here is an example of few-shot learning by classifying emotions from text statements.\n",
        "\n",
        "Let's provide the model with three examples, each labeled with an appropriate emotion—joy, frustration, and sadness—to establish a pattern or guideline on how to categorize emotions in statements.\n",
        "\n",
        "After presenting these examples, let's challenge the model with a new statement: \"That movie was so scary I had to cover my eyes.\" The task for the model is to classify the emotion expressed in this new statement based on the learning from the provided examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a0fd813-2007-4afc-91a2-415e025209a1",
        "outputId": "d4358f13-1287-418c-b853-7a341e95e7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: Here are few examples of classifying emotions in statements:\n",
            "\n",
            "           Statement: 'I just won my first marathon!'\n",
            "           Emotion: Joy\n",
            "           \n",
            "           Statement: 'I can't believe I lost my keys again.'\n",
            "           Emotion: Frustration\n",
            "           \n",
            "           Statement: 'My best friend is moving to another country.'\n",
            "           Emotion: Sadness\n",
            "           \n",
            "           Now, classify the emotion in the following statement:\n",
            "           Statement: 'That movie was so scary I had to cover my eyes.’\n",
            "           \n",
            "\n",
            "\n",
            "\n",
            "response : Answer: Fear. The person is express\n",
            "\n"
          ]
        }
      ],
      "source": [
        " #parameters  `max_new_tokens` to 10, which constrains the model to generate brief responses\n",
        "\n",
        "params = {\n",
        "    \"max_new_tokens\": 10,\n",
        "}\n",
        "\n",
        "prompt = \"\"\"Here are few examples of classifying emotions in statements:\n",
        "\n",
        "            Statement: 'I just won my first marathon!'\n",
        "            Emotion: Joy\n",
        "\n",
        "            Statement: 'I can't believe I lost my keys again.'\n",
        "            Emotion: Frustration\n",
        "\n",
        "            Statement: 'My best friend is moving to another country.'\n",
        "            Emotion: Sadness\n",
        "\n",
        "            Now, classify the emotion in the following statement:\n",
        "            Statement: 'That movie was so scary I had to cover my eyes.’\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "response = llm_model(prompt, params)\n",
        "print(f\"prompt: {prompt}\\n\")\n",
        "print(f\"response : {response}\\n\")"
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b882e1a1-a235-4242-a695-cc2436947c13"
      },
      "source": [
        "The parameters are set with `max_new_tokens` to 10, which constrains the model to generate brief responses, focusing on the essential output without elaboration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35db2ca5-53f8-48eb-8f8b-5bff6b645b0c"
      },
      "source": [
        "The model's response demonstrates its ability to use the provided few examples to understand and classify the emotion of the new statement effectively following the same pattern in examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09b4d220-2fd7-4f7b-8bb0-1a2e3e0b4332"
      },
      "source": [
        "### Chain-of-thought (CoT) prompting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50581598-dd72-47ea-9bbf-566bbf26454f"
      },
      "source": [
        "Here is an example of the Chain-of-Thought (CoT) prompting technique, designed to guide the model through a sequence of reasoning steps to solve a problem. In this example, the problem is a simple arithmetic question: “A store had 22 apples. They sold 15 apples today and received a new delivery of 8 apples. How many apples are there now?”\n",
        "\n",
        "The CoT technique involves structuring the prompt by instructing the model to “Break down each step of your calculation.” This encourages the model to include explicit reasoning steps, mimicking human-like problem-solving processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bf39d78-5c0b-4b11-acec-9cb8d8331e99",
        "outputId": "7ea33863-24f3-414b-bb2c-36755b6986d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples. \n",
            "            How many apples are there now?’\n",
            "\n",
            "            Break down each step of your calculation\n",
            "\n",
            "\n",
            "\n",
            "response :             Step 1: Add the number of apples the store had at the start of the day to the number of apples they got in the new delivery.\n",
            "\n",
            "            22 apples (initial amount) + 8 apples (new delivery) = 30 apples\n",
            "\n",
            "            Step 2: Subtract the number of apples the store sold today from the total number of apples after the new delivery.\n",
            "\n",
            "            30 apples (total after new delivery) - 15 apples (sold) = 15 apples\n",
            "\n",
            "            Answer: \\boxed{15}.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"temperature\": 0.5,\n",
        "}\n",
        "\n",
        "prompt = \"\"\"Consider the problem: 'A store had 22 apples. They sold 15 apples today and got a new delivery of 8 apples.\n",
        "            How many apples are there now?’\n",
        "\n",
        "            Break down each step of your calculation\n",
        "\n",
        "\"\"\"\n",
        "response = llm_model(prompt, params)\n",
        "print(f\"prompt: {prompt}\\n\")\n",
        "print(f\"response : {response}\\n\")"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0979ecc7-fd7f-43cc-92d5-7f5cb398cb5f"
      },
      "source": [
        "From the response of the model, you can see the prompt directs the model to:\n",
        "\n",
        "1. Add the initial number of apples to the apples received in the new delivery.\n",
        "2. Subtract the number of apples sold from the sum obtained in the first step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b43ed23-8455-42c4-a6bc-034d741c1023"
      },
      "source": [
        "By breaking down the problem into specific steps, the model is better able to understand the sequence of operations required to arrive at the correct answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f28f2e9-b837-41cb-ba7c-4fb705847f65"
      },
      "source": [
        "### Self-consistency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bffda872-f719-467b-974d-bbd134817f9a"
      },
      "source": [
        "This example demonstrates the self-consistency technique in reasoning through multiple calculations for a single problem. The problem posed is: “When I was 6, my sister was half my age. Now I am 70, what age is my sister?”\n",
        "\n",
        "The prompt instructs, “Provide three independent calculations and explanations, then determine the most consistent result.” This encourages the model to engage in critical thinking and consistency checking, which are vital for complex decision-making processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2e55cdc-fa04-46d6-bf18-f0c5ff636e54",
        "outputId": "253e19a9-4c08-4e96-c410-3f79eb8e52d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt: When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
            "\n",
            "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
            "\n",
            "\n",
            "\n",
            "response : Calculation 1:\n",
            "When my sister was half my age, I was twice as old as she was. Let's call her current age \"S\" and my current age \"M\". At that time, M = 2S. Now, M = 70, so we can solve for S:\n",
            "\n",
            "S = M / 2\n",
            "S = 70 / 2\n",
            "S = 35\n",
            "\n",
            "Final answer: My sister is 35 years old.\n",
            "\n",
            "Calculation 2:\n",
            "We could consider this a problem of ratios. When I was 6, my sister was 3 (half my age). The ratio of our ages has remained constant, so if I am now 70, my sister would still be half my age:\n",
            "\n",
            "S = M / 2\n",
            "S = 70 / 2\n",
            "S = 35\n",
            "\n",
            "Final answer: My sister is 35 years old.\n",
            "\n",
            "Calculation 3:\n",
            "Another way to approach this is by looking at the difference in our ages. When I was 6, my sister was 3; the difference was 3 years. This difference remains constant, so when I am 70, my sister would be 70 - 3 = 67 years old.\n",
            "\n",
            "However, this calculation contradicts the information given in the question that my sister was half my age when I was 6. So, this method seems to be incorrect due to a misunderstanding of the problem statement.\n",
            "\n",
            "Ranking of calculations from best to worst:\n",
            "1. Calculation 1: This method directly applies the information provided in the question and solves for the unknown using simple algebra.\n",
            "2. Calculation 2: This method also arrives at the correct answer, but it does so indirectly by considering the ratio of our ages.\n",
            "3. Calculation 3: This method is inconsistent with the information provided in the question and leads to an incorrect answer.\n",
            "\n",
            "Best and final answer: My sister is 35 years old.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "    \"max_new_tokens\": 512,\n",
        "}\n",
        "\n",
        "prompt = \"\"\"When I was 6, my sister was half of my age. Now I am 70, what age is my sister?\n",
        "\n",
        "            Provide three independent calculations and explanations, then determine the most consistent result.\n",
        "\n",
        "\"\"\"\n",
        "response = llm_model(prompt, params)\n",
        "print(f\"prompt: {prompt}\\n\")\n",
        "print(f\"response : {response}\\n\")"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6133c22c-1f6d-4d01-aac8-4b9f62f41647"
      },
      "source": [
        "The model's response shows that it provides three different calculations and explanations. Each calculation attempts to derive the sister's age using different logical approaches.\n",
        "\n",
        "Self-consistency can help identify the most accurate and reliable answer in scenarios where multiple plausible solutions exist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfd01ea6-e781-4635-97db-e23f094d82e7"
      },
      "source": [
        "## Applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eb8b351-81cc-4e82-8984-9ceb57e84cd5"
      },
      "source": [
        "In this section, you will show you how to use the prompt template from Langchain to create more structured and reproducible prompts. You will also learn to create some applications based on the prompt template.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee3a614f-f114-4b6f-9ada-313b8a572766"
      },
      "source": [
        "### Prompt template\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f496e59-3649-43f2-8b2d-866c42353311"
      },
      "source": [
        "[Prompt template](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates) is a key concept in langchain, it helps to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "278f1720-73fd-47bd-840d-a4e0e0a6d874"
      },
      "source": [
        "To use the prompt template, you need to initialize a LLM first.\n",
        "\n",
        "You can still use the `mixtral-8x7b-instruct-v01` from watsonx.ai.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "c6885225-e860-425b-9e07-2310803a6594",
        "outputId": "378410ad-41ab-4509-b97e-351436f3eacf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Model.__init__() got an unexpected keyword argument 'api_key'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-1493348741.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIBM_API_KEY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m model = Model(\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Model.__init__() got an unexpected keyword argument 'api_key'"
          ]
        }
      ],
      "source": [
        "xxx - obsolete\n",
        "model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
        "\n",
        "parameters = {\n",
        "    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
        "    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
        "}\n",
        "\n",
        "credentials = {\n",
        "    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
        "}\n",
        "\n",
        "# Get credentials\n",
        "IBM_API_KEY = userdata.get('IBM_API_KEY')\n",
        "IBM_PROJECT_ID = userdata.get('IBM_PROJECT_ID')\n",
        "\n",
        "# Validate and clean the keys\n",
        "if not IBM_API_KEY or not IBM_PROJECT_ID:\n",
        "    raise ValueError(\"Missing IBM_API_KEY or IBM_PROJECT_ID in Colab secrets\")\n",
        "\n",
        "IBM_API_KEY = IBM_API_KEY.strip()\n",
        "IBM_PROJECT_ID = IBM_PROJECT_ID.strip()\n",
        "\n",
        "project_id = IBM_PROJECT_ID\n",
        "api_key = IBM_API_KEY\n",
        "\n",
        "model = Model(\n",
        "    model_id=model_id,\n",
        "    params=parameters,\n",
        "    credentials=credentials,\n",
        "    project_id=project_id,\n",
        "    api_key=api_key\n",
        "\n",
        ")\n",
        "\n",
        "mixtral_llm = WatsonxLLM(model=model)\n",
        "mixtral_llm"
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
        "from langchain_ibm import WatsonxLLM\n",
        "from google.colab import userdata\n",
        "\n",
        "model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
        "\n",
        "# Get credentials\n",
        "IBM_API_KEY = userdata.get('IBM_API_KEY')\n",
        "IBM_PROJECT_ID = userdata.get('IBM_PROJECT_ID')\n",
        "\n",
        "# Validate and clean the keys\n",
        "if not IBM_API_KEY or not IBM_PROJECT_ID:\n",
        "    raise ValueError(\"Missing IBM_API_KEY or IBM_PROJECT_ID in Colab secrets\")\n",
        "\n",
        "IBM_API_KEY = IBM_API_KEY.strip()\n",
        "IBM_PROJECT_ID = IBM_PROJECT_ID.strip()\n",
        "\n",
        "# Create WatsonxLLM directly - no ModelInference needed\n",
        "mixtral_llm = WatsonxLLM(\n",
        "    model_id=model_id,\n",
        "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
        "    apikey=IBM_API_KEY,\n",
        "    project_id=IBM_PROJECT_ID,\n",
        "    params={\n",
        "        GenParams.MAX_NEW_TOKENS: 256,\n",
        "        GenParams.TEMPERATURE: 0.5,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"✅ Model created successfully!\")\n",
        "print(mixtral_llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yark3bFXx-Ru",
        "outputId": "d6b9f2f1-ff3e-48d8-f6fa-b2744b20feee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model created successfully!\n",
            "\u001b[1mWatsonxLLM\u001b[0m\n",
            "Params: {'model_id': 'mistralai/mixtral-8x7b-instruct-v01', 'deployment_id': None, 'params': {'max_new_tokens': 256, 'temperature': 0.5}, 'project_id': '839fdc16-c311-4693-aaa0-120c337fe937', 'space_id': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdacc63-e259-42ac-8a95-e12e6eb50fba"
      },
      "source": [
        "Use the `PromptTemplate` to create a template for a string-based prompt. In this template, you'll define two parameters: `adjective` and `content`. These parameters allow for the reuse of the prompt across different situations. For instance, to adapt the prompt to various contexts, simply pass the relevant values to these parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e55d4548-7f9d-4fe0-89de-c9c3b9b53f90",
        "outputId": "63d42661-c995-4cf0-ba7e-0aad4434a0a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['adjective', 'content'], input_types={}, partial_variables={}, template='Tell me a {adjective} joke about {content}.\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "template = \"\"\"Tell me a {adjective} joke about {content}.\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb09443-7864-4547-8469-eb3bc6980e48"
      },
      "source": [
        "Now, let's take a look at how the prompt has been formatted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "443775df-d59f-4e1a-a193-63efc6b79295",
        "outputId": "b7be5e2b-ff79-4943-d661-e18017bf0833"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a funny joke about chickens.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "prompt.format(adjective=\"funny\", content=\"chickens\")"
      ],
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a074082-6460-431b-aacf-06f61ec30480"
      },
      "source": [
        "From the response, you can see that the prompt is formatted according to the specified context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "167a74b0-a898-4a55-a8a9-900eca85ba55"
      },
      "source": [
        "The following code will wrap the formatted prompt into the LLMChain, and then invoke the prompt to get the response from the LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c8207a2-6d93-4d80-8ef0-582b3c66ffe9",
        "outputId": "0d317b92-5f44-460e-ba40-0cf0091cdb05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Why did the chicken cross the playground? To get to the other slide.\n"
          ]
        }
      ],
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm)\n",
        "response = llm_chain.invoke(input = {\"adjective\": \"funny\", \"content\": \"chickens\"})\n",
        "print(response[\"text\"])"
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d7ee1b6-dd8d-46d3-82a2-115940dd1bdb"
      },
      "source": [
        "From the response, you can see the LLM came up with a funny joke about chickens.\n",
        "\n",
        "To use this prompt in another context, simply replace the variables accordingly\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f8204ed-2bfc-45c0-90da-ef8d9275e153",
        "outputId": "1282265d-2514-4c43-958c-a143248cc7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What's the difference between a dead fish and a dead lawyer on the side of the road? The fish smells better.\n"
          ]
        }
      ],
      "source": [
        "response = llm_chain.invoke(input = {\"adjective\": \"sad\", \"content\": \"fish\"})\n",
        "print(response[\"text\"])"
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcd1ccee-2011-429f-88b6-36632c4f5c22"
      },
      "source": [
        "In the following sections, you will learn how to create agents capable of completing various tasks using prompt templates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7327404d-4b44-40ed-b5d6-e78ed3898f36"
      },
      "source": [
        "### Text summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fddee98-e94e-43d2-beac-5c1a045908ff"
      },
      "source": [
        "Here is a text summarization agent designed to help summarize the content you provide to the LLM.\n",
        "\n",
        "You can store the content to be summarized in a variable, allowing for repeated use of the prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17815188-5543-4d25-8131-72974f14f540",
        "outputId": "3b856afa-a9be-40ce-c8bf-0e28e42cd8ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation, by enhancing productivity and contributing to a more interconnected and informed society through innovations such as artificial intelligence, machine learning, and the Internet of Things.\n"
          ]
        }
      ],
      "source": [
        "content = \"\"\"\n",
        "        The rapid advancement of technology in the 21st century has transformed various industries, including healthcare, education, and transportation.\n",
        "        Innovations such as artificial intelligence, machine learning, and the Internet of Things have revolutionized how we approach everyday tasks and complex problems.\n",
        "        For instance, AI-powered diagnostic tools are improving the accuracy and speed of medical diagnoses, while smart transportation systems are making cities more efficient and reducing traffic congestion.\n",
        "        Moreover, online learning platforms are making education more accessible to people around the world, breaking down geographical and financial barriers.\n",
        "        These technological developments are not only enhancing productivity but also contributing to a more interconnected and informed society.\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"Summarize the {content} in one sentence.\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm)\n",
        "response = llm_chain.invoke(input = {\"content\": content})\n",
        "print(response[\"text\"])"
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac6c9385-fd71-4cd5-abce-b9a74ad6aa4e"
      },
      "source": [
        "### Question answering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a37be47-c882-4f08-9c3b-f15ee238627d"
      },
      "source": [
        "Here is a Q&A agent.\n",
        "\n",
        "This agent enables the LLM to learn from the provided content and answer questions based on what it has learned. Occasionally, if the LLM does not have sufficient information, it might generate a speculative answer. To manage this, you'll specifically instruct it to respond with \"Unsure about the answer\" if it is uncertain about the correct response.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f85fa524-d4d3-4252-ba4a-9be649012874",
        "outputId": "d510df1c-2d0e-4665-c9a3-f2ad2ca617e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid.\n",
            "\n",
            "            Confidence:\n",
            "            \n",
            "        90%\n"
          ]
        }
      ],
      "source": [
        "content = \"\"\"\n",
        "        The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets.\n",
        "        The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid.\n",
        "        The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
        "\"\"\"\n",
        "\n",
        "question = \"Which planets in the solar system are rocky and solid?\"\n",
        "\n",
        "template = \"\"\"\n",
        "            Answer the {question} based on the {content}.\n",
        "            Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "            Answer:\n",
        "\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "output_key = \"answer\"\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)\n",
        "response = llm_chain.invoke(input = {\"question\":question ,\"content\": content})\n",
        "print(response[\"answer\"])"
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f4592fd-1efe-4a16-9935-9e495e16a086"
      },
      "source": [
        "### Text classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "150b75c0-d2de-4586-b813-09dc27c96645"
      },
      "source": [
        "Here is a text classification agent designed to categorize text into predefined categories. This example employs zero-shot learning, where the agent classifies text without prior exposure to related examples.\n",
        "\n",
        "Can you revise it to the one-shot learning or few-shot learning in the exercises?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "f720e0fb-7263-40f5-9a4a-b9abe2464305",
        "outputId": "4d63d42c-415a-43ff-8811-e03667b6f117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Direct model test:  howling outside my door as I write this. It’s a dark and stormy night, and I’m hunkered down in my cozy home, grateful for the roof over my head.\n",
            "\n",
            "It’s the kind of weather that makes me want to curl up with a good book, and I’m delighted to share with you a new title that I’ve recently discovered: “The Home for Unwanted Girls” by Joanna Goodman.\n",
            "\n",
            "Set in Quebec in the 1950s, the novel tells the story of Maggie, a young woman who falls in love with a man from the wrong side of the tracks. When she becomes pregnant, her parents send her away to a convent, where she is forced to give up her baby daughter.\n",
            "\n",
            "The story follows Maggie’s efforts to reunite with her child, as well as the experiences of her daughter, Elodie, as she grows up in a series of foster homes and institutions. The novel explores themes of family, identity, and social justice, and it’s a powerful reminder of the importance of standing up for what’s right.\n",
            "\n",
            "Goodman’s writing is evocative and engaging, and she brings the world\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Missing some input keys: {'example_category', 'example_text'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-4203598676.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Debug the full response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Full response structure:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             outputs = (\n\u001b[1;32m    157\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m_validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mmissing_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing some input keys: {missing_keys}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'example_category', 'example_text'}"
          ]
        }
      ],
      "source": [
        "xxx - prompt args not provided\n",
        "# Test if your model works at all\n",
        "test_prompt = \"The wind is\"\n",
        "try:\n",
        "    direct_response = mixtral_llm.invoke(test_prompt)\n",
        "    print(f\"Direct model test: {direct_response}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Model error: {e}\")\n",
        "\n",
        "text = \"\"\"\n",
        "        The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
        "\"\"\"\n",
        "\n",
        "categories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n",
        "\n",
        "template = \"\"\"\n",
        "            Classify the {text} into one of the {categories}.\n",
        "\n",
        "            Category:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Debug the full response\n",
        "response = llm_chain.invoke(input={\"text\": text, \"categories\": categories})\n",
        "print(\"Full response structure:\")\n",
        "print(response)\n",
        "print(\"\\nKeys in response:\")\n",
        "print(response.keys())\n",
        "print(\"\\nAnswer:\")\n",
        "print(response[\"category\"])\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "output_key = \"category\"\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)\n",
        "response = llm_chain.invoke(input = {\"text\":text ,\"categories\": categories})\n",
        "print(response[\"category\"])\n",
        "#print(response[\"category\"])"
      ],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip LLMChain and use the model directly\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "text = \"\"\"\n",
        "The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
        "\"\"\"\n",
        "\n",
        "categories = \"Entertainment, Food and Dining, Technology, Literature, Music\"\n",
        "\n",
        "template = \"\"\"\n",
        "Classify the following text into one of these categories: {categories}\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Choose only ONE category from the list above.\n",
        "\n",
        "Category:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "# Format the prompt\n",
        "formatted_prompt = prompt.format(text=text, categories=categories)\n",
        "print(\"Formatted prompt:\")\n",
        "print(formatted_prompt)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Get response directly\n",
        "try:\n",
        "    response = mixtral_llm.invoke(formatted_prompt)\n",
        "    print(f\"Category: {response}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzHPwV2P1anO",
        "outputId": "599c6f05-edbc-4d64-8bb8-ebbfeefd4038"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted prompt:\n",
            "\n",
            "Classify the following text into one of these categories: Entertainment, Food and Dining, Technology, Literature, Music\n",
            "\n",
            "Text: \n",
            "The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
            "\n",
            "\n",
            "Choose only ONE category from the list above.\n",
            "\n",
            "Category:\n",
            "\n",
            "==================================================\n",
            "\n",
            "Category:  Entertainment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7541ecc2-0393-4e0c-acc5-e1cc4c19965a"
      },
      "source": [
        "### Code generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76904576-eda2-4a13-8144-99295ece96eb"
      },
      "source": [
        "Here is an example of an SQL code generation agent. This agent is designed to generate SQL queries based on given descriptions. It interprets the requirements from your input and translates them into executable SQL code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b34db82d-1fb9-420d-8d14-9fc602236252",
        "outputId": "e2811ea5-b196-428d-a957-e5118ca2f57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            SELECT \n",
            "                customers.name, \n",
            "                customers.email \n",
            "            FROM \n",
            "                customers \n",
            "            INNER JOIN \n",
            "                purchases \n",
            "            ON \n",
            "                customers.id = purchases.customer_id \n",
            "            WHERE \n",
            "                purchases.purchase_date >= (CURRENT_DATE - INTERVAL '30 days');\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xxx - bad promptng\n",
        "description = \"\"\"\n",
        "        Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days.\n",
        "        The table 'purchases' contains a column 'purchase_date'\n",
        "\"\"\"\n",
        "\n",
        "template = \"\"\"\n",
        "            Generate an SQL query based on the {description}\n",
        "\n",
        "            SQL Query:\n",
        "\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "output_key = \"query\"\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)\n",
        "response = llm_chain.invoke(input = {\"description\":description})\n",
        "print(response[\"query\"])"
      ],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"\"\"\n",
        "Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days.\n",
        "The table 'purchases' contains a column 'purchase_date'\n",
        "\"\"\"\n",
        "\n",
        "# More detailed template for better SQL generation\n",
        "template = \"\"\"\n",
        "You are an expert SQL developer. Generate a SQL query based on the following description:\n",
        "\n",
        "{description}\n",
        "\n",
        "Database Schema Context:\n",
        "- customers table: contains customer information including names and email addresses\n",
        "- purchases table: contains purchase records with purchase_date column\n",
        "- Assume there's a relationship between customers and purchases tables\n",
        "\n",
        "Requirements:\n",
        "- Write a complete, executable SQL query\n",
        "- Use proper JOIN syntax\n",
        "- Use appropriate date functions for \"last 30 days\"\n",
        "- Return only the requested columns\n",
        "- Ensure the query is optimized\n",
        "\n",
        "SQL Query:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "formatted_prompt = prompt.format(description=description)\n",
        "\n",
        "response = mixtral_llm.invoke(formatted_prompt)\n",
        "print(\"SQL Query:\")\n",
        "print(response.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MibkVenw2YqF",
        "outputId": "756800c8-18bb-4568-a7ec-d833af0374fd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SQL Query:\n",
            "SELECT c.name, c.email\n",
            "FROM customers c\n",
            "JOIN purchases p ON c.id = p.customer_id\n",
            "WHERE p.purchase_date >= CURDATE() - INTERVAL 30 DAY;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a more complex example of abvve\n",
        "def generate_sql_query(description, llm):\n",
        "    \"\"\"\n",
        "    Generate SQL query from natural language description\n",
        "    \"\"\"\n",
        "    template = \"\"\"\n",
        "Generate an SQL query based on the following description:\n",
        "\n",
        "{description}\n",
        "\n",
        "Requirements:\n",
        "- Provide only the SQL query\n",
        "- Do not include explanations or markdown\n",
        "- Use proper SQL syntax\n",
        "\n",
        "SQL Query:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate.from_template(template)\n",
        "    formatted_prompt = prompt.format(description=description)\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(formatted_prompt)\n",
        "        # Clean up the response\n",
        "        sql_query = response.strip()\n",
        "\n",
        "        # Remove common prefixes that models sometimes add\n",
        "        prefixes_to_remove = [\"SQL Query:\", \"Query:\", \"```sql\", \"```\"]\n",
        "        for prefix in prefixes_to_remove:\n",
        "            if sql_query.startswith(prefix):\n",
        "                sql_query = sql_query[len(prefix):].strip()\n",
        "\n",
        "        # Remove trailing ```\n",
        "        if sql_query.endswith(\"```\"):\n",
        "            sql_query = sql_query[:-3].strip()\n",
        "\n",
        "        return sql_query\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generating SQL: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test it\n",
        "description = \"\"\"\n",
        "Retrieve the names and email addresses of all customers from the 'customers' table who have made a purchase in the last 30 days.\n",
        "The table 'purchases' contains a column 'purchase_date'\n",
        "\"\"\"\n",
        "\n",
        "sql_query = generate_sql_query(description, mixtral_llm)\n",
        "if sql_query:\n",
        "    print(\"Generated SQL Query:\")\n",
        "    print(sql_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBHydq0z3MeW",
        "outputId": "b84e75ec-df85-42f2-a0b5-a0bf9519e394"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL Query:\n",
            "SELECT customers.name, customers.email\n",
            "FROM customers\n",
            "JOIN purchases ON customers.id = purchases.customer_id\n",
            "WHERE purchases.purchase_date >= NOW() - INTERVAL 30 DAY;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c2e68de-c7ee-40ff-81c7-c50b97d86f37"
      },
      "source": [
        "### Role playing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c5570bf-c479-43ab-b647-e0aabbcccbe7"
      },
      "source": [
        "You can also configure the LLM to assume specific roles as defined by us, enabling it to follow predetermined rules and behave like a task-oriented chatbot.\n",
        "\n",
        "For example, the code below configures the LLM to act as a game master. In this role, the LLM answers questions about games while maintaining an engaging and immersive tone, enhancing the user experience.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90aa0c93-6a81-4893-8e9b-b611695f8d37"
      },
      "source": [
        "Run the following code to create the prompt template and create a LLMChian to wrap the prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08d13644-a659-4107-8f2c-c893c12c0c7e"
      },
      "outputs": [],
      "source": [
        "role = \"\"\"\n",
        "        game master\n",
        "\"\"\"\n",
        "\n",
        "tone = \"engaging and immersive\"\n",
        "\n",
        "template = \"\"\"\n",
        "            You are an expert {role}. I have this question {question}. I would like our conversation to be {tone}.\n",
        "\n",
        "            Answer:\n",
        "\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "output_key = \"answer\"\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)"
      ],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d89bf66-2dda-4069-b500-b0cc46e5735c"
      },
      "source": [
        "The following code will create a game master chatbot that takes your questions as input and provides responses from the model.\n",
        "\n",
        "Run the code below to launch the bot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b51fc6f4-3553-4877-adcd-11c456d47c71"
      },
      "source": [
        "You can test the bot by asking the question, \"Who are you?\" The bot will respond with \"I am a game master,\" indicating it has assumed the role that you predefined.\n",
        "\n",
        "The function is written within a while loop, allowing continuous interaction. To exit the loop and terminate the conversation, type \"quit,\" \"exit,\" or \"bye\" into the input box.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29004440-7e03-4cac-9ec1-d5b0f3c745c5",
        "outputId": "1abe1e39-569a-49c5-aae8-bb14c16ada3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is two plus two?\n",
            "Answer:              In the realm of arithmetic, the question of two plus two is a classic inquiry. It's a riddle as old as time itself, a puzzle that has perplexed scholars, mystified sages, and intrigued mathematicians for centuries. But fear not, for I, a seasoned game master, shall guide you through this numerical labyrinth, leading you to the illustrious answer that lies within.\n",
            "\n",
            "            To begin our journey, let us first imagine a world devoid of numbers, a realm where the very concept of addition has yet to be conceived. It is within this primordial void that our tale begins.\n",
            "\n",
            "            In the beginning, there was naught but the number one, a solitary entity adrift in the vast expanse of the numerical cosmos. This number, though singular, yearned for companionship, for to be alone is a fate worse than any other. And so, in its infinite wisdom, the number one begat another, and thus, the number two was born.\n",
            "\n",
            "            Two, now a creature of its own, reveled in its newfound existence. It danced and twirled, giddy with the joy\n",
            "Question: quit\n",
            "Answer: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    query = input(\"Question: \")\n",
        "\n",
        "    if query.lower() in [\"quit\",\"exit\",\"bye\"]:\n",
        "        print(\"Answer: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = llm_chain.invoke(input = {\"role\": role, \"question\": query, \"tone\": tone})\n",
        "\n",
        "    print(\"Answer: \", response[\"answer\"])"
      ],
      "execution_count": 29
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfca6371-af83-4212-bce8-e1036dee56a9"
      },
      "source": [
        "Great! You finish the lab. Now let's take some exercises.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca7f7bbe-97f4-4047-828a-72d91ba8a746"
      },
      "source": [
        "# Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ab38c4d-4d0d-4e0b-bc22-41a37775fbc6"
      },
      "source": [
        "### Exercise 1: Change parameters for the LLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7494526d-6e33-4fc3-b065-e7aa07f8f308"
      },
      "source": [
        "Experiment with changing the parameters of the LLM to observe how different settings impact the responses. Adjusting parameters such as `max_new_tokens`, `temperature`, or `top_p` can significantly alter the behavior of the model. Try different configurations to see how each variation influences the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3f53468-5926-409c-a6e6-fa35dcaa2af7",
        "outputId": "d230b57e-027f-4b75-8c57-3b24cfa95ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " howling outside, and the rain is coming down in sheets. It’s the perfect weather for a cozy night in with a good book. But what to read? If you’re looking for something to take your mind off the storm, here are five great books to curl up with on a rainy night.\n",
            "\n",
            "## 1. The Secret History by Donna Tartt\n",
            "\n",
            "This gripping novel tells the story of a group of classics students at an elite New England college who become embroiled in a murder conspiracy. With its richly drawn characters and atmospheric setting, The Secret History is\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "    \"max_new_tokens\": 128,\n",
        "    \"min_new_tokens\": 100,\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.1,\n",
        "    \"top_k\": 1\n",
        "}\n",
        "\n",
        "prompt = \"The wind is\"\n",
        "\n",
        "response = llm_model(prompt, params)\n",
        "print(response)"
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1449fedc-5347-4fa1-a49c-7dcad6c04cb3"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "params = {\n",
        "    \"max_new_tokens\": 128,\n",
        "    \"min_new_tokens\": 100,\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 0.1,\n",
        "    \"top_k\": 1\n",
        "}\n",
        "\n",
        "prompt = \"The wind is\"\n",
        "\n",
        "response = llm_model(prompt, params)\n",
        "print(response)\n",
        "\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a925a26-3bd1-4b10-8a99-94a19ae1bab8"
      },
      "source": [
        "### Exercise 2: Observe how LLM thinks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e323fbe-cdfe-4cec-97f8-2ebbdf5aedc3"
      },
      "source": [
        "You can set `verbose=True` in the `LLMChain()` to observe the thought process of the LLM, gaining insights into how it formulates its responses. Can you make it any agent you created before to observe it?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0c5cbd6-c92d-420f-babc-0eb56a9b9706",
        "outputId": "25cd85ee-f4ce-4b34-9818-b89019b269c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "            Answer the Which planets in the solar system are rocky and solid? based on the \n",
            "        The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets. \n",
            "        The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid. \n",
            "        The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
            ".\n",
            "            Respond \"Unsure about answer\" if not sure about the answer.\n",
            "\n",
            "            Answer:\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid.\n"
          ]
        }
      ],
      "source": [
        "content = \"\"\"\n",
        "        The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets.\n",
        "        The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid.\n",
        "        The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
        "\"\"\"\n",
        "\n",
        "question = \"Which planets in the solar system are rocky and solid?\"\n",
        "\n",
        "template = \"\"\"\n",
        "            Answer the {question} based on the {content}.\n",
        "            Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "            Answer:\n",
        "\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "output_key = \"answer\"\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key, verbose=True)\n",
        "response = llm_chain.invoke(input = {\"question\":question ,\"content\": content})\n",
        "print(response[\"answer\"])"
      ],
      "execution_count": 31
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b530c12-a07e-4da3-ad01-f8eb8364caec"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "content = \"\"\"\n",
        "        The solar system consists of the Sun, eight planets, their moons, dwarf planets, and smaller objects like asteroids and comets.\n",
        "        The inner planets—Mercury, Venus, Earth, and Mars—are rocky and solid.\n",
        "        The outer planets—Jupiter, Saturn, Uranus, and Neptune—are much larger and gaseous.\n",
        "\"\"\"\n",
        "\n",
        "question = \"Which planets in the solar system are rocky and solid?\"\n",
        "\n",
        "template = \"\"\"\n",
        "            Answer the {question} based on the {content}.\n",
        "            Respond \"Unsure about answer\" if not sure about the answer.\n",
        "            \n",
        "            Answer:\n",
        "            \n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "output_key = \"answer\"\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key, verbose=True)\n",
        "response = llm_chain.invoke(input = {\"question\":question ,\"content\": content})\n",
        "print(response[\"answer\"])\n",
        "\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11b1114d-10f9-4024-9883-79548b2b0459"
      },
      "source": [
        "### Exercise 3: Revise the text classification agent to one-shot learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10b66667-17a7-4977-8b0b-d2e9ba792b87"
      },
      "source": [
        "You were using zero-shot learning when you created the text classification agent. Can you revise it to use one-shot learning?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a82b6ae8-8d20-4fbd-8ffe-f65733ca8b8f",
        "outputId": "365950ef-51b3-495a-e9c2-0c77fd99a3a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Entertainment\n"
          ]
        }
      ],
      "source": [
        "example_text = \"\"\"\n",
        "               Last week's book fair was a delightful gathering of authors and readers, featuring discussions and book signings.\n",
        "               \"\"\"\n",
        "\n",
        "example_category = \"Literature\"\n",
        "\n",
        "text = \"\"\"\n",
        "       The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
        "       \"\"\"\n",
        "\n",
        "categories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n",
        "\n",
        "template = \"\"\"\n",
        "           Example:\n",
        "           Text: {example_text}\n",
        "           Category: {example_category}\n",
        "\n",
        "           Now, classify the following text into one of the specified categories: {categories}\n",
        "\n",
        "           Text: {text}\n",
        "\n",
        "           Category:\n",
        "\n",
        "           \"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "output_key = \"category\"\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)\n",
        "response = llm_chain.invoke(input = {\"example_text\": example_text, \"example_category\":example_category ,\"categories\": categories, \"text\":text})\n",
        "print(response[\"category\"])"
      ],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57b60a40-a67a-49a8-9c98-014d7d3314b8"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "example_text = \"\"\"\n",
        "               Last week's book fair was a delightful gathering of authors and readers, featuring discussions and book signings.\n",
        "               \"\"\"\n",
        "\n",
        "example_category = \"Literature\"\n",
        "\n",
        "text = \"\"\"\n",
        "       The concert last night was an exhilarating experience with outstanding performances by all artists.\n",
        "       \"\"\"\n",
        "\n",
        "categories = \"Entertainment, Food and Dining, Technology, Literature, Music.\"\n",
        "\n",
        "template = \"\"\"\n",
        "           Example:\n",
        "           Text: {example_text}\n",
        "           Category: {example_category}\n",
        "\n",
        "           Now, classify the following text into one of the specified categories: {categories}\n",
        "           \n",
        "           Text: {text}\n",
        "           \n",
        "           Category:\n",
        "           \n",
        "           \"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "output_key = \"category\"\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=mixtral_llm, output_key=output_key)\n",
        "response = llm_chain.invoke(input = {\"example_text\": example_text, \"example_category\":example_category ,\"categories\": categories, \"text\":text})\n",
        "print(response[\"category\"])\n",
        "\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4c1c5ee-1c61-48bf-8440-ff619aceecb3"
      },
      "source": [
        "## Authors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cbeeee1-5242-47b2-8bc4-018cc0b99153"
      },
      "source": [
        "[Kang Wang](https://author.skills.network/instructors/kang_wang)\n",
        "\n",
        "Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n",
        "\n",
        "And\n",
        "\n",
        "[Cal Page](https://www.linkedin.com/in/cal-page-1084311/)\n",
        "\n",
        "Cal Page, who fixed multiple bugs along with updating to the latest libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e5d91fb-d2fd-46e5-995e-5c6d3867a58f"
      },
      "source": [
        "© Copyright IBM Corporation. All rights reserved.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#\n",
        "# This file is autogenerated by pip-compile with Python 3.12\n",
        "# by the following command:\n",
        "#\n",
        "#    pip-compile\n",
        "#\n",
        "# It should be extracted and saved as requirements.txt in this\n",
        "# juptyr notebook.\n",
        "#\n",
        "annotated-types==0.7.0\n",
        "    # via pydantic\n",
        "anyio==4.9.0\n",
        "    # via httpx\n",
        "certifi==2025.6.15\n",
        "    # via\n",
        "    #   httpcore\n",
        "    #   httpx\n",
        "    #   ibm-watsonx-ai\n",
        "    #   requests\n",
        "charset-normalizer==3.4.2\n",
        "    # via requests\n",
        "greenlet==3.2.3\n",
        "    # via sqlalchemy\n",
        "h11==0.16.0\n",
        "    # via httpcore\n",
        "httpcore==1.0.9\n",
        "    # via httpx\n",
        "httpx==0.28.1\n",
        "    # via\n",
        "    #   ibm-watsonx-ai\n",
        "    #   langsmith\n",
        "ibm-cos-sdk==2.14.2\n",
        "    # via ibm-watsonx-ai\n",
        "ibm-cos-sdk-core==2.14.2\n",
        "    # via\n",
        "    #   ibm-cos-sdk\n",
        "    #   ibm-cos-sdk-s3transfer\n",
        "ibm-cos-sdk-s3transfer==2.14.2\n",
        "    # via ibm-cos-sdk\n",
        "ibm-watsonx-ai==1.3.26\n",
        "    # via\n",
        "    #   -r requirements.in\n",
        "    #   langchain-ibm\n",
        "idna==3.10\n",
        "    # via\n",
        "    #   anyio\n",
        "    #   httpx\n",
        "    #   requests\n",
        "jmespath==1.0.1\n",
        "    # via\n",
        "    #   ibm-cos-sdk\n",
        "    #   ibm-cos-sdk-core\n",
        "jsonpatch==1.33\n",
        "    # via langchain-core\n",
        "jsonpointer==3.0.0\n",
        "    # via jsonpatch\n",
        "langchain==0.3.25\n",
        "    # via -r requirements.in\n",
        "langchain-core==0.3.65\n",
        "    # via\n",
        "    #   langchain\n",
        "    #   langchain-ibm\n",
        "    #   langchain-text-splitters\n",
        "langchain-ibm==0.3.12\n",
        "    # via -r requirements.in\n",
        "langchain-text-splitters==0.3.8\n",
        "    # via langchain\n",
        "langsmith==0.3.45\n",
        "    # via\n",
        "    #   langchain\n",
        "    #   langchain-core\n",
        "lomond==0.3.3\n",
        "    # via ibm-watsonx-ai\n",
        "numpy==2.3.0\n",
        "    # via pandas\n",
        "orjson==3.10.18\n",
        "    # via langsmith\n",
        "packaging==24.2\n",
        "    # via\n",
        "    #   ibm-watsonx-ai\n",
        "    #   langchain-core\n",
        "    #   langsmith\n",
        "pandas==2.2.3\n",
        "    # via ibm-watsonx-ai\n",
        "pydantic==2.11.7\n",
        "    # via\n",
        "    #   langchain\n",
        "    #   langchain-core\n",
        "    #   langsmith\n",
        "pydantic-core==2.33.2\n",
        "    # via pydantic\n",
        "python-dateutil==2.9.0.post0\n",
        "    # via\n",
        "    #   ibm-cos-sdk-core\n",
        "    #   pandas\n",
        "pytz==2025.2\n",
        "    # via pandas\n",
        "pyyaml==6.0.2\n",
        "    # via\n",
        "    #   langchain\n",
        "    #   langchain-core\n",
        "requests==2.32.4\n",
        "    # via\n",
        "    #   ibm-cos-sdk-core\n",
        "    #   ibm-watsonx-ai\n",
        "    #   langchain\n",
        "    #   langsmith\n",
        "    #   requests-toolbelt\n",
        "requests-toolbelt==1.0.0\n",
        "    # via langsmith\n",
        "six==1.17.0\n",
        "    # via\n",
        "    #   lomond\n",
        "    #   python-dateutil\n",
        "sniffio==1.3.1\n",
        "    # via anyio\n",
        "sqlalchemy==2.0.41\n",
        "    # via langchain\n",
        "tabulate==0.9.0\n",
        "    # via ibm-watsonx-ai\n",
        "tenacity==9.1.2\n",
        "    # via langchain-core\n",
        "typing-extensions==4.14.0\n",
        "    # via\n",
        "    #   anyio\n",
        "    #   langchain-core\n",
        "    #   pydantic\n",
        "    #   pydantic-core\n",
        "    #   sqlalchemy\n",
        "    #   typing-inspection\n",
        "typing-inspection==0.4.1\n",
        "    # via pydantic\n",
        "tzdata==2025.2\n",
        "    # via pandas\n",
        "urllib3==2.5.0\n",
        "    # via\n",
        "    #   ibm-cos-sdk-core\n",
        "    #   ibm-watsonx-ai\n",
        "    #   requests\n",
        "zstandard==0.23.0\n",
        "    # via langsmith\n"
      ],
      "metadata": {
        "id": "37XRipSW4l-k"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "prev_pub_hash": "3cd9d8693a1fb2dfa1525107bea3b904b7669a1f05c2ade1610dc7eea60ed3d5",
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}