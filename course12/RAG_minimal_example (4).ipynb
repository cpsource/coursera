{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Retriever-Augmented Generation (RAG) is a powerful architecture that **combines retrieval-based and generation-based approaches** to build more accurate and scalable NLP systems—especially for tasks like **open-domain question answering**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 What Is Retriever-Augmented Generation (RAG)?\n",
        "\n",
        "RAG enhances Large Language Models (LLMs) by **injecting external knowledge** dynamically at inference time. Instead of relying solely on the model’s internal parameters (which can be limited or outdated), RAG **retrieves relevant documents** from a large corpus and uses them to **inform the generation** of a final answer.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 General Flow of a RAG Pipeline\n",
        "\n",
        "Here’s the step-by-step flow:\n",
        "\n",
        "```\n",
        "  ┌───────────────────────┐\n",
        "  │     User Query        │\n",
        "  └─────────┬─────────────┘\n",
        "            ↓\n",
        "  ┌───────────────────────┐\n",
        "  │  Question Encoder (e.g. DPR) ─────┐\n",
        "  └───────────────────────┘           │\n",
        "            ↓                         ▼\n",
        "  ┌───────────────────────┐    ┌───────────────────────┐\n",
        "  │ FAISS or other index   │<───┤ Context Encoder (DPR) │\n",
        "  │ (Document Embeddings)  │    └───────────────────────┘\n",
        "  └─────────┬─────────────┘\n",
        "            │\n",
        "     Top-k Relevant Passages\n",
        "            ↓\n",
        "  ┌────────────────────────────┐\n",
        "  │ Concatenate query + context│\n",
        "  └─────────┬──────────────────┘\n",
        "            ↓\n",
        "  ┌────────────────────────────┐\n",
        "  │   Generator Model (e.g. GPT2)│\n",
        "  └─────────┬──────────────────┘\n",
        "            ↓\n",
        "      Final Generated Answer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Key Components of a RAG System\n",
        "\n",
        "### 1. **Retriever**\n",
        "\n",
        "* **Goal**: Find relevant documents/passages from a corpus.\n",
        "* **Common choice**: Dense Passage Retriever (DPR)\n",
        "* **Parts**:\n",
        "\n",
        "  * `DPRQuestionEncoder`: encodes the user query into a dense vector\n",
        "  * `DPRContextEncoder`: pre-encodes documents in the corpus\n",
        "* **Similarity metric**: FAISS Index with L2 or inner product (dot-product) distance\n",
        "\n",
        "### 2. **Generator**\n",
        "\n",
        "* **Goal**: Generate natural language output based on the query + retrieved context\n",
        "* **Common choice**: GPT-2, BART, T5, etc.\n",
        "* **Input**: Query + top-k retrieved passages (concatenated)\n",
        "* **Output**: Final response or answer\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 What Gets Trained?\n",
        "\n",
        "There are **two major training phases**, depending on the use case:\n",
        "\n",
        "### ▶️ **Pretraining (optional)**\n",
        "\n",
        "* You may start with pretrained models:\n",
        "\n",
        "  * DPR (retriever): already trained on QA tasks like Natural Questions.\n",
        "  * Generator: pretrained language model (e.g., GPT-2 or BART).\n",
        "\n",
        "### 🏋️‍♀️ **Fine-tuning**\n",
        "\n",
        "You can fine-tune:\n",
        "\n",
        "1. **Retriever**:\n",
        "\n",
        "   * Use **contrastive learning** (positive vs negative passages).\n",
        "   * Objective: bring questions closer to correct context embeddings.\n",
        "\n",
        "2. **Generator**:\n",
        "\n",
        "   * Fine-tune to condition on query + retrieved documents to generate better outputs.\n",
        "   * Loss: language modeling loss (e.g., cross-entropy).\n",
        "\n",
        "### 👥 End-to-End (optional but advanced):\n",
        "\n",
        "* Train both retriever and generator together.\n",
        "* This is **more complex** and **less stable**, so often not done unless needed.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Why Use RAG?\n",
        "\n",
        "### ✅ Advantages:\n",
        "\n",
        "* Reduces hallucination by grounding output in actual retrieved facts.\n",
        "* Extensible to large corpora without retraining the LLM.\n",
        "* Memory-efficient: doesn't force the model to memorize all world knowledge.\n",
        "\n",
        "### ❌ Limitations:\n",
        "\n",
        "* Retrieval quality heavily affects generation quality.\n",
        "* Concatenating long contexts can exceed model token limits (e.g., GPT2's 1024).\n",
        "* Retrieval + generation latency is higher than generation-only.\n",
        "\n",
        "---\n",
        "\n",
        "## Example in Plain Terms\n",
        "\n",
        "> Q: “What is our company’s mobile phone policy?”\n",
        "\n",
        "* **Step 1**: Question is encoded into a vector.\n",
        "* **Step 2**: That vector is used to **search a corpus** of HR documents using FAISS.\n",
        "* **Step 3**: Top 5 matching paragraphs are retrieved.\n",
        "* **Step 4**: These paragraphs are **fed into GPT-2** along with the original question.\n",
        "* **Step 5**: GPT-2 **generates an answer**, grounded in the retrieved data.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary Table\n",
        "\n",
        "| Component        | Model Type | Purpose                    | Trainable?    |\n",
        "| ---------------- | ---------- | -------------------------- | ------------- |\n",
        "| Question Encoder | DPR        | Encode user query          | Yes           |\n",
        "| Context Encoder  | DPR        | Encode documents for FAISS | Yes           |\n",
        "| Retriever Index  | FAISS      | Fast similarity search     | No (prebuilt) |\n",
        "| Generator        | GPT2/BART  | Generate final answer      | Yes           |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like a diagram or a PyTorch code version of a minimal RAG implementation?\n"
      ],
      "metadata": {
        "id": "pmx2MddQmoVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers faiss-cpu torch"
      ],
      "metadata": {
        "id": "jI_nMxu1mQBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "from transformers import (\n",
        "    DPRContextEncoder, DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n",
        "    AutoTokenizer, AutoModelForCausalLM\n",
        ")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 1: Prepare a small corpus of documents\n",
        "# -----------------------------------------------------------------------------------\n",
        "documents = [\n",
        "    \"The company offers 15 days of paid vacation per year.\",\n",
        "    \"Employees should submit reimbursement forms within 30 days.\",\n",
        "    \"Mobile phones must be secured with a company-approved password.\",\n",
        "    \"Remote work is allowed up to 3 days per week.\",\n",
        "    \"Drinking alcohol during work hours is strictly prohibited.\",\n",
        "    \"The company policy prohibits sex at work.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "FTQ4k9FO8g7o"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 2: Load DPR Context Encoder and Tokenizer\n",
        "# These will convert documents into dense embeddings\n",
        "# -----------------------------------------------------------------------------------\n",
        "context_encoder =   DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\").to(device)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "context_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "#context_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "# Tokenize and encode each document\n",
        "context_embeddings = []\n",
        "for doc in documents:\n",
        "    inputs = context_tokenizer(doc, return_tensors='pt', max_length=256, truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        embedding = context_encoder(**inputs).pooler_output  # (1, 768)\n",
        "    context_embeddings.append(embedding.cpu().numpy())\n",
        "\n",
        "# Stack into numpy array for FAISS\n",
        "context_embeddings_np = np.vstack(context_embeddings).astype('float32')  # Shape: (5, 768)\n",
        "\n",
        "# Show each document and the first 5 values of its embedding vector\n",
        "for i, (doc, emb) in enumerate(zip(documents, context_embeddings_np)):\n",
        "    print(f\"\\n📄 Document {i+1}:\")\n",
        "    print(doc)\n",
        "    print(f\"\\n🔢 First 5 values of embedding vector (shape: {emb.shape}):\")\n",
        "    print(emb[:5])  # Only show the first 5 values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZyXjI_n-jon",
        "outputId": "91ce9b9d-0596-4642-a65f-bddef7b3a79a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Document 1:\n",
            "The company offers 15 days of paid vacation per year.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[ 0.42181766  0.16109067  0.3723451  -0.09172494  0.28965074]\n",
            "\n",
            "📄 Document 2:\n",
            "Employees should submit reimbursement forms within 30 days.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[ 0.3361708   0.27786404  0.5407016  -0.33155122  0.07597482]\n",
            "\n",
            "📄 Document 3:\n",
            "Mobile phones must be secured with a company-approved password.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[0.24592635 0.49385566 0.31940734 0.09724129 0.7529858 ]\n",
            "\n",
            "📄 Document 4:\n",
            "Remote work is allowed up to 3 days per week.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[0.18697365 0.00790876 0.4691345  0.08455902 0.34109434]\n",
            "\n",
            "📄 Document 5:\n",
            "Drinking alcohol during work hours is strictly prohibited.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[ 0.58111304  0.6653758   0.17271869 -0.36189055 -0.12203238]\n",
            "\n",
            "📄 Document 6:\n",
            "The company policy prohibits sex at work.\n",
            "\n",
            "🔢 First 5 values of embedding vector (shape: (768,)):\n",
            "[ 0.3238992   0.19877852 -0.06360116 -0.3587018   0.24410388]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 3: Create FAISS index\n",
        "# This allows us to retrieve similar documents by vector similarity\n",
        "# -----------------------------------------------------------------------------------\n",
        "embedding_dim = context_embeddings_np.shape[1]\n",
        "index = faiss.IndexFlatL2(embedding_dim)  # L2 = Euclidean distance\n",
        "index.add(context_embeddings_np)  # Add document embeddings to index\n"
      ],
      "metadata": {
        "id": "NSeJ1OwW-1Az"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 4: Load DPR Question Encoder\n",
        "# This will embed the user query in the same vector space\n",
        "# -----------------------------------------------------------------------------------\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\").to(device)\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydiJueDj-7hL",
        "outputId": "94b50a66-f667-4c6d-f469-a7f995548a66"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 5: Define a query and retrieve top-k relevant documents\n",
        "# -----------------------------------------------------------------------------------\n",
        "query = \"At work, Is it ok if Julie has sex in the utility closit?\"\n",
        "\n",
        "# Tokenize and encode the question\n",
        "inputs = question_tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    query_embedding = question_encoder(**inputs).pooler_output.cpu().numpy()\n",
        "\n",
        "# Search FAISS for top 2 closest documents\n",
        "D, I = index.search(query_embedding, k=2)\n",
        "\n",
        "print(\"Top matching documents:\")\n",
        "for idx in I[0]:\n",
        "    print(\"-\", documents[idx])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkwV1lEo_B87",
        "outputId": "e1f729b6-2361-4ed3-a886-91cf33f342ab"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matching documents:\n",
            "- The company policy prohibits sex at work.\n",
            "- Remote work is allowed up to 3 days per week.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[User Query] ──▶ DPR Question Encoder ──▶ vector\n",
        "                                     │\n",
        "                                     ▼\n",
        "                           Search FAISS Index\n",
        "                                     │\n",
        "                                     ▼\n",
        "              Top-k Relevant Docs (raw text) ──▶ GPT-2 ──▶ Final Answer\n"
      ],
      "metadata": {
        "id": "wSBgzuMY-pj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 6: Load GPT-2 for generation\n",
        "# -----------------------------------------------------------------------------------\n",
        "gpt_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
        "\n",
        "gpt_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "gpt_model.eval()\n",
        "\n",
        "# Set special token to avoid warnings\n",
        "gpt_model.generation_config.pad_token_id = gpt_tokenizer.eos_token_id\n"
      ],
      "metadata": {
        "id": "7SPUjvBk_EqS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raise \"Skipping\")\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 7a: Generate answer WITHOUT context\n",
        "# -----------------------------------------------------------------------------------\n",
        "def generate_without_context(query):\n",
        "    inputs = gpt_tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "    output = gpt_model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
        "    return gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nAnswer without context:\")\n",
        "print(generate_without_context(query))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRD6T46L_HCu",
        "outputId": "844e402e-bb32-43b8-ef74-6e4d3e11c49c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer without context:\n",
            "What is the mobile phone policy?\n",
            "\n",
            "The mobile phone policy is a policy that allows you to use your mobile phone for any purpose. It is a policy that allows you to use your mobile phone for any purpose. It is a policy that allows you to use your mobile phone for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 7a: Generate answer WITHOUT context\n",
        "# -----------------------------------------------------------------------------------\n",
        "def generate_without_context(query):\n",
        "    inputs = gpt_tokenizer(query, return_tensors=\"pt\", padding=True).to(device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    output = gpt_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=50\n",
        "    )\n",
        "    return gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"\\nAnswer without context:\")\n",
        "print(generate_without_context(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10nQ-aDFCIrG",
        "outputId": "bdd9324f-a422-4515-f8e9-28267afe2183"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer without context:\n",
            "At work, Is it ok if Julie has sex in the utility closit?\n",
            "\n",
            "I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great — you're now entering the **generation phase** of the RAG pipeline. Let’s walk through:\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **Purpose of Step 7a:**\n",
        "\n",
        "You're **testing GPT-2 on its own**, without feeding it any extra knowledge (i.e., *not* using FAISS-retrieved documents).\n",
        "\n",
        "> 🎯 This is a baseline. You're asking:\n",
        "> “What does GPT-2 already know about the question just from pretraining?”\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 What This Code Does (Line by Line):\n",
        "\n",
        "### 1. **Define a function: `generate_without_context(query)`**\n",
        "\n",
        "```python\n",
        "def generate_without_context(query):\n",
        "```\n",
        "\n",
        "This function takes a **query string** (like `\"What is the mobile phone policy?\"`) and returns a GPT-2 generated response.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Tokenize the input**\n",
        "\n",
        "```python\n",
        "inputs = gpt_tokenizer(query, return_tensors=\"pt\").to(device)\n",
        "```\n",
        "\n",
        "* Converts the string into token IDs using the **GPT-2 tokenizer**\n",
        "* Wraps in a PyTorch tensor\n",
        "* Moves it to the correct device (`cpu` or `cuda`)\n",
        "\n",
        "Example: `\"what is lunch?\"` → `[15496, 318, 17944, 30]`\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Generate a response**\n",
        "\n",
        "```python\n",
        "output = gpt_model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
        "```\n",
        "\n",
        "* Uses GPT-2 to **generate up to 50 new tokens** starting from the prompt\n",
        "* No extra knowledge — just what GPT-2 already learned during pretraining\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Decode the output tokens**\n",
        "\n",
        "```python\n",
        "return gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "```\n",
        "\n",
        "* Converts token IDs back into a human-readable string\n",
        "* Removes special tokens like \\`\\` if any\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Call the function and print**\n",
        "\n",
        "```python\n",
        "print(\"\\nAnswer without context:\")\n",
        "print(generate_without_context(query))\n",
        "```\n",
        "\n",
        "* Runs your function with the original query\n",
        "* Shows what GPT-2 says on its own\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Example Output\n",
        "\n",
        "Query:\n",
        "\n",
        "```python\n",
        "\"What is the mobile phone policy?\"\n",
        "```\n",
        "\n",
        "GPT-2 might say:\n",
        "\n",
        "```text\n",
        "\"The mobile phone policy may vary by organization. Employees are usually expected to keep phones off during meetings...\"\n",
        "```\n",
        "\n",
        "> But **it could also be vague, wrong, or hallucinated** — because it doesn't know your specific documents yet.\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 Why This Step Matters\n",
        "\n",
        "This is your **control group** in the experiment.\n",
        "\n",
        "Later, you'll compare it to:\n",
        "\n",
        "* ✅ `generate_with_context(retrieved_docs + query)`\n",
        "\n",
        "And see how **RAG improves accuracy** by injecting relevant info from FAISS.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary Table\n",
        "\n",
        "| Step                       | Purpose                                           |\n",
        "| -------------------------- | ------------------------------------------------- |\n",
        "| `generate_without_context` | Run GPT-2 by itself, no external help             |\n",
        "| Tokenizer                  | Converts text to token IDs for GPT-2              |\n",
        "| `generate(...)`            | GPT-2 makes predictions based on pretraining only |\n",
        "| Output                     | Baseline answer — can be vague, biased, or wrong  |\n",
        "\n",
        "---\n",
        "\n",
        "Let me know when you're ready to explain `generate_with_context()` — that's where RAG shines.\n"
      ],
      "metadata": {
        "id": "cqUfIdqHAbyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raise(\"skipping\")\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 7b: Generate answer WITH top-k context\n",
        "# -----------------------------------------------------------------------------------\n",
        "def generate_with_context(query, retrieved_docs):\n",
        "    full_input = query + \" \" + \" \".join(retrieved_docs)\n",
        "    inputs = gpt_tokenizer(full_input, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "    output = gpt_model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
        "    return gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Use the top 2 FAISS matches\n",
        "retrieved = [documents[i] for i in I[0]]\n",
        "\n",
        "print(\"\\nAnswer with retrieved context:\")\n",
        "print(generate_with_context(query, retrieved))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbibYxEv_JSA",
        "outputId": "cbb542d2-8f4f-495f-e240-b6e66a3f6014"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer with retrieved context:\n",
            "What is the mobile phone policy? Mobile phones must be secured with a company-approved password. The company offers 15 days of paid vacation per year.\n",
            "\n",
            "What is the mobile phone policy? Mobile phones must be secured with a company-approved password. The company offers 15 days of paid vacation per year. What is the mobile phone policy? Mobile phones must be secured with a company-approved password\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text(text, width=80, indent=4):\n",
        "    indent_str = ' ' * indent\n",
        "    return '\\n'.join(textwrap.wrap(text, width=width, subsequent_indent=indent_str))\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# STEP 7b: Generate answer WITH top-k context (pretty print)\n",
        "# -----------------------------------------------------------------------------------\n",
        "def generate_with_context(query, retrieved_docs):\n",
        "    full_input = \"Context: \" + \" \".join(retrieved_docs) + \"\\nQuestion: \" + query + \"\\nAnswer:\"\n",
        "    print(f\"full input = {full_input}\")\n",
        "#    inputs = gpt_tokenizer(full_input, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
        "#    output = gpt_model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n",
        "#    full_input = query + \" \" + \" \".join(retrieved_docs)\n",
        "    inputs = gpt_tokenizer(full_input, return_tensors=\"pt\", truncation=True, max_length=1024, padding=True).to(device)\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    print(f\"input_ids = {input_ids}\")\n",
        "    print(f\"attention_mask = {attention_mask}\")\n",
        "\n",
        "#    output = gpt_model.generate(\n",
        "#        input_ids=input_ids,\n",
        "#        attention_mask=attention_mask,\n",
        "#        max_new_tokens=50\n",
        "#    )\n",
        "\n",
        "    output = gpt_model.generate(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      max_new_tokens=100,\n",
        "      repetition_penalty=1.2,\n",
        "      do_sample=True,\n",
        "      temperature=0.8\n",
        "    )\n",
        "\n",
        "    print(f\"output = {output}\")\n",
        "    generated_text = gpt_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"generated_text = {generated_text}\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "    print(\"Question:\")\n",
        "    print(wrap_text(query))\n",
        "    print(\"\\nCombined Input:\")\n",
        "    print(wrap_text(full_input))\n",
        "    print(\"\\nOutput:\")\n",
        "    print(wrap_text(generated_text))\n",
        "    print(\"--------------------------------------------------\\n\")\n",
        "\n",
        "# Use the top 2 FAISS matches\n",
        "retrieved = [documents[i] for i in I[0]]\n",
        "print(f\"retrieved = {retrieved}\")\n",
        "\n",
        "#print(\"\\nAnswer with retrieved context:\")\n",
        "generate_with_context(query, retrieved)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiRf_L3EZ__Y",
        "outputId": "e4f065ed-6a1b-4e42-f1a9-c5e6ac107cf6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "retrieved = ['The company policy prohibits sex at work.', 'Remote work is allowed up to 3 days per week.']\n",
            "full input = Context: The company policy prohibits sex at work. Remote work is allowed up to 3 days per week.\n",
            "Question: At work, Is it ok if Julie has sex in the utility closit?\n",
            "Answer:\n",
            "input_ids = tensor([[21947,    25,   383,  1664,  2450, 24059,  1714,   379,   670,    13,\n",
            "         21520,   670,   318,  3142,   510,   284,   513,  1528,   583,  1285,\n",
            "            13,   198, 24361,    25,  1629,   670,    11,  1148,   340, 12876,\n",
            "           611, 21946,   468,  1714,   287,   262, 10361,  3542,   270,    30,\n",
            "           198, 33706,    25]])\n",
            "attention_mask = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "output = tensor([[21947,    25,   383,  1664,  2450, 24059,  1714,   379,   670,    13,\n",
            "         21520,   670,   318,  3142,   510,   284,   513,  1528,   583,  1285,\n",
            "            13,   198, 24361,    25,  1629,   670,    11,  1148,   340, 12876,\n",
            "           611, 21946,   468,  1714,   287,   262, 10361,  3542,   270,    30,\n",
            "           198, 33706,    25,  3363,     0,  1400,  1917,   355,   890,   355,\n",
            "           345,   466,   407,  7262,   393, 18605,   395,   607,  1231,  1719,\n",
            "           257,  2106,   286,   884,  6529,   878,  8218,   656,   777,  8592,\n",
            "           357,  5832,   460,   772,   779,   428,  1672,   737,  4091,   674,\n",
            "         18749,  2174,   329,   517, 31321,   319,   703, 39685,  3206,  2316,\n",
            "          1022,  1450,   290,  1466,   389, 12118,   739,  2219,  1099, 11704,\n",
            "           351, 11390,   706,   852,  9657,   416,   281,  6538,   338, 15383,\n",
            "         12684,   986,   628,   464,  1708,  1808,   815,   467,   572,  1165,\n",
            "          2952,   475,   356,   477,   423,   617,  1321,   326,  1244,  1037,\n",
            "           514,   503,   994, 47226, 19879,  3574,   532,   509,   380,   368,\n",
            "          1236,    12,    33]])\n",
            "generated_text = Context: The company policy prohibits sex at work. Remote work is allowed up to 3 days per week.\n",
            "Question: At work, Is it ok if Julie has sex in the utility closit?\n",
            "Answer: Yes! No problem as long as you do not rape or molest her without having a history of such acts before entering into these contracts (you can even use this example). See our FAQ below for more clarification on how consensual sexual relations between men and women are handled under common law agreements with employers after being hired by an employee's workplace organisation...\n",
            "\n",
            "The following question should go off too quickly but we all have some information that might help us out here :-) Quote From - Kriemann-B\n",
            "--------------------------------------------------\n",
            "Question:\n",
            "At work, Is it ok if Julie has sex in the utility closit?\n",
            "\n",
            "Combined Input:\n",
            "Context: The company policy prohibits sex at work. Remote work is allowed up to\n",
            "    3 days per week. Question: At work, Is it ok if Julie has sex in the utility\n",
            "    closit? Answer:\n",
            "\n",
            "Output:\n",
            "Context: The company policy prohibits sex at work. Remote work is allowed up to\n",
            "    3 days per week. Question: At work, Is it ok if Julie has sex in the utility\n",
            "    closit? Answer: Yes! No problem as long as you do not rape or molest her\n",
            "    without having a history of such acts before entering into these contracts\n",
            "    (you can even use this example). See our FAQ below for more clarification on\n",
            "    how consensual sexual relations between men and women are handled under\n",
            "    common law agreements with employers after being hired by an employee's\n",
            "    workplace organisation...  The following question should go off too quickly\n",
            "    but we all have some information that might help us out here :-) Quote From\n",
            "    - Kriemann-B\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That kind of output — `\"I don't know. I don't know. ...\"` — is a classic **low-confidence degenerative loop** in language models like GPT-2. Here's what it means and how to fix it:\n",
        "\n",
        "---\n",
        "\n",
        "## 🤖 Why It Happens\n",
        "\n",
        "1. **Lack of Strong Context**\n",
        "\n",
        "   * If the retrieved context from FAISS isn't relevant or is too weak, the model may revert to generic phrases like “I don’t know.”\n",
        "\n",
        "2. **No Fine-Tuning**\n",
        "\n",
        "   * GPT-2 isn't specifically fine-tuned on Q\\&A tasks with knowledge grounding. It’s just predicting the next likely token — and if it’s uncertain, it often loops simple outputs.\n",
        "\n",
        "3. **Temperature Too Low**\n",
        "\n",
        "   * If you're using `temperature=1.0` or lower without other constraints, the model may play it safe and stick to repetitive defaults.\n",
        "\n",
        "4. **No Output Penalty**\n",
        "\n",
        "   * If you don’t apply a `repetition_penalty`, the model is prone to repeating the same token patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Fixes to Try\n",
        "\n",
        "### 🔧 Update `generate()` with these parameters:\n",
        "\n",
        "```python\n",
        "output = gpt_model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=100,\n",
        "    repetition_penalty=1.5,   # discourages \"I don't know\" loops\n",
        "    temperature=0.8,          # adds variability\n",
        "    top_p=0.95,               # nucleus sampling\n",
        "    do_sample=True            # enables sampling instead of greedy decoding\n",
        ")\n",
        "```\n",
        "\n",
        "These help:\n",
        "\n",
        "* Encourage variation\n",
        "* Avoid repetitive loops\n",
        "* Explore better outputs\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 Optional Upgrade: Use a better model\n",
        "\n",
        "GPT-2 is small and not trained for retrieval-based generation.\n",
        "\n",
        "* **Try**: `facebook/bart-large` or `facebook/bart-large-cnn`\n",
        "* **Or RAG models**: `facebook/rag-token-base`\n",
        "\n",
        "Those are trained **specifically for question-answering** over retrieved documents.\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to help you switch the generator model to one of those? Or adjust your current `generate_with_context()` to include these new settings?\n"
      ],
      "metadata": {
        "id": "6SLCWSLekulL"
      }
    }
  ]
}