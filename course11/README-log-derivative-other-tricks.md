Great! Here's a breakdown of common **non-differentiable functions or operations** you might encounter in neural networks, and the **best workaround** strategies depending on what you're trying to do.

---

## üîß 1. **`round()`, `step()`, `sign()` ‚Äî Discrete Activations**

These are **not differentiable** and block gradient flow.

### ‚úÖ Workaround: **Straight-Through Estimator (STE)**

**Forward pass:** use the hard function
**Backward pass:** use a smooth approximation or identity gradient

### üîÅ Example in PyTorch:

```python
class RoundSTE(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        return torch.round(input)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output  # pretend it's identity (‚àÇoutput/‚àÇinput = 1)

round_ste = RoundSTE.apply

x = torch.tensor([0.3, 0.7, 1.5], requires_grad=True)
y = round_ste(x)
y.sum().backward()
print(x.grad)  # This will be [1, 1, 1]
```

Use this for training quantized or binarized networks.

---

## üîß 2. **`argmax()` ‚Äî Non-differentiable choice**

Often used to pick a class, but blocks gradients.

### ‚úÖ Workaround: Use `softmax` in training

```python
probs = torch.softmax(logits, dim=-1)  # differentiable
```

Then sample or choose deterministically at test time:

```python
action = torch.argmax(probs)  # OK in inference
```

If you must sample during training:

### ‚úÖ Workaround: Use **Gumbel-Softmax** (differentiable approximation to categorical sampling)

```python
def gumbel_softmax(logits, tau=1.0):
    noise = torch.rand_like(logits).clamp(1e-6, 1-1e-6)
    gumbel = -torch.log(-torch.log(noise))
    return torch.softmax((logits + gumbel) / tau, dim=-1)

probs = gumbel_softmax(logits, tau=0.5)  # differentiable "argmax"
```

---

## üîß 3. **Sampling Discrete Values (e.g. Reinforcement Learning)**

When you need to sample (e.g., actions, tokens), backprop **won‚Äôt work directly**.

### ‚úÖ Workaround: **Log-derivative trick** (as discussed)

```python
dist = torch.distributions.Categorical(probs)
action = dist.sample()
logprob = dist.log_prob(action)
loss = -logprob * reward
loss.backward()
```

---

## üîß 4. **Binary Neural Networks (e.g., weights are ¬±1)**

### ‚úÖ Use:

* BinaryConnect or BinaryNet
* STE + custom quantization functions
* Surrogate gradients (identity, tanh, etc.)

---

## üßæ Summary Table

| Non-differentiable Op | Workaround                       | Notes                          |
| --------------------- | -------------------------------- | ------------------------------ |
| `round(x)`            | Straight-through estimator       | Replace gradient with 1        |
| `argmax(logits)`      | Softmax or Gumbel-Softmax        | Differentiable during training |
| `sign(x)`             | tanh or STE                      | For binarized nets             |
| `sample()`            | Log-derivative trick (REINFORCE) | For policy gradients           |

---

Let me know your specific use case ‚Äî e.g., quantized networks, RL, token generation ‚Äî and I can tailor the workaround for that.


