from transformers import pipeline
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# Load the model and tokenizer

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

# Preprocess the input text

# Sample text
text = "Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim."

# Tokenize the input text
inputs = tokenizer(text, return_tensors="pt")

print(inputs)

# Perform inference

# Perform inference
with torch.no_grad():
    outputs = model(**inputs)

#model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])

# Get the logits

logits = outputs.logits
print(logits.shape)

# Convert logits to probabilities
probs = torch.softmax(logits, dim=-1)

# Get the predicted class
predicted_class = torch.argmax(probs, dim=-1)

# Map the predicted class to the label
labels = ["NEGATIVE", "POSITIVE"]
predicted_label = labels[predicted_class.item()]

print(f"Predicted label: {predicted_label}")

# Text generation with GPT-2

# Load the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
# Add this line:
tokenizer.pad_token = tokenizer.eos_token

# Load the tokenizer and model

model = GPT2LMHeadModel.from_pretrained("gpt2")

# Preprocess the input text

# Prompt
prompt = "Once upon a time"

# Tokenize the input text
inputs = tokenizer(prompt, return_tensors="pt")
inputs

# Perform inference
#  Generate text using the model
#
#  inputs: Input token IDs from the tokenizer
#
#  attention_mask: Mask indicating which tokens to attend to
#
#  pad_token_id:Padding token ID set to the end-of-sequence token ID
#
#  max_length: Maximum length of the generated sequences
#
#  num_return_sequence: Number of sequences to generate

# Generate text
output_ids = model.generate(
    inputs.input_ids,
    attention_mask=inputs.attention_mask,
    pad_token_id=tokenizer.eos_token_id,
    max_length=50,
    num_return_sequences=1
)

print(output_ids)

# or
# with torch.no_grad():
#    outputs = model(**inputs)
#
# print(outputs)

# Post-process the output

# Decode the generated text
generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

print(generated_text)

#
# Hugging Face pipeline() function
#

print("# Example 1: Text classification using pipeline()")
# Example 1: Text classification using pipeline()

# Load a general text classification model
classifier = pipeline("text-classification", model="distilbert-base-uncased-finetuned-sst-2-english")

# Classify a sample text
result = classifier("Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.")
print(result)

print("# Example 2: Language detection using pipeline()")
# Example 2: Language detection using pipeline()

from transformers import pipeline
import os
# Enable progress bars for downloads
os.environ['TRANSFORMERS_VERBOSITY'] = 'info'

print("Loading language detection model (this may take a few minutes on first run)...")
classifier = pipeline("text-classification", model="papluca/xlm-roberta-base-language-detection")
result = classifier("Bonjour, comment Ã§a va?")
print(result)

print("# Example 3: Text generation using pipeline()")
# Example 3: Text generation using pipeline()

# Initialize the text generation pipeline with GPT-2
generator = pipeline("text-generation", model="gpt2")

# Generate text based on a given prompt
prompt = "Once upon a time"
result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)

# Print the generated text
print(result[0]['generated_text'])

print("# Example 4: Text generation using T5 with pipeline()")
# Example 4: Text generation using T5 with pipeline()

# Initialize the text generation pipeline with T5
generator = pipeline("text2text-generation", model="t5-small")
# Generate text based on a given prompt
prompt = "translate English to French: How are you?"
result = generator(prompt, max_length=50, num_return_sequences=1)

# Print the generated text
print(result[0]['generated_text'])

print("# Exercise: Fill-mask task using BERT with pipeline()")
# Exercise: Fill-mask task using BERT with pipeline()

# Initialize the fill-mask pipeline with BERT
fill_mask = pipeline("fill-mask", model="bert-base-uncased")

# Generate text by filling in the masked token
prompt = "The capital of France is [MASK]."
result = fill_mask(prompt)

# Print the generated text
print(result)
