from tqdm import tqdm
import numpy as np
import pandas as pd
from itertools import accumulate
import matplotlib.pyplot as plt
import requests
import os
from collections import Counter
import re

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import numpy as np
from IPython.display import Markdown as md
from tqdm import tqdm

from torch.utils.data.dataset import random_split
from sklearn.manifold import TSNE
import plotly.graph_objs as go
from sklearn.model_selection import train_test_split

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

def plot(COST, ACC):
    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(COST, color=color)
    ax1.set_xlabel('epoch', color=color)
    ax1.set_ylabel('total loss', color=color)
    ax1.tick_params(axis='y', color=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color=color)
    ax2.plot(ACC, color=color)
    ax2.tick_params(axis='y', color=color)
    fig.tight_layout()

    plt.show()

# Custom tokenizer function (replacing torchtext tokenizer)
def basic_english_tokenize(text):
    """Simple tokenizer that mimics basic_english from torchtext"""
    # Convert to lowercase and split on whitespace and punctuation
    text = text.lower()
    # Replace punctuation with spaces and split
    text = re.sub(r'[^\w\s]', ' ', text)
    tokens = text.split()
    return tokens

# Custom vocabulary class
class Vocabulary:
    def __init__(self, specials=None):
        self.specials = specials or []
        self.token_to_idx = {}
        self.idx_to_token = {}
        self.default_index = 0
        
    def build_from_iterator(self, iterator, min_freq=1):
        # Count tokens
        counter = Counter()
        for tokens in iterator:
            counter.update(tokens)
        
        # Add special tokens first
        for special in self.specials:
            self.token_to_idx[special] = len(self.token_to_idx)
            self.idx_to_token[len(self.idx_to_token)] = special
            
        # Add regular tokens
        for token, freq in counter.items():
            if freq >= min_freq and token not in self.token_to_idx:
                self.token_to_idx[token] = len(self.token_to_idx)
                self.idx_to_token[len(self.idx_to_token)] = token
    
    def __len__(self):
        return len(self.token_to_idx)
    
    def __call__(self, tokens):
        if isinstance(tokens, str):
            return self.token_to_idx.get(tokens, self.default_index)
        return [self.token_to_idx.get(token, self.default_index) for token in tokens]
    
    def set_default_index(self, index):
        self.default_index = index
    
    def get_stoi(self):
        return self.token_to_idx

# Custom dataset class for AG News
class AGNewsDataset(Dataset):
    def __init__(self, split='train'):
        self.data = []
        self.labels = []
        
        # Download and load AG News dataset
        self._load_ag_news(split)
    
    def _load_ag_news(self, split):
        """Load AG News dataset from CSV files"""
        urls = {
            'train': 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv',
            'test': 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'
        }
        
        if split not in urls:
            raise ValueError(f"Split {split} not supported. Use 'train' or 'test'")
        
        # Try to load from local file first, then download if needed
        filename = f'ag_news_{split}.csv'
        if not os.path.exists(filename):
            print(f"Downloading AG News {split} dataset...")
            response = requests.get(urls[split])
            with open(filename, 'wb') as f:
                f.write(response.content)
        
        # Load data from CSV
        df = pd.read_csv(filename, header=None, names=['label', 'title', 'description'])
        
        # Combine title and description
        for _, row in df.iterrows():
            self.labels.append(int(row['label']))
            text = str(row['title']) + ' ' + str(row['description'])
            self.data.append(text)
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.labels[idx], self.data[idx]

# Create datasets
print("Loading AG News dataset...")
train_dataset = AGNewsDataset('train')
test_dataset = AGNewsDataset('test')

# Get sample data
sample_label, sample_text = train_dataset[0]
print(f"Sample: {sample_label}, {sample_text[:100]}...")

ag_news_label = {1: "World", 2: "Sports", 3: "Business", 4: "Sci/Tec"}
print(f"Label meaning: {ag_news_label[sample_label]}")

# Get number of classes
num_class = len(ag_news_label)
print(f"Number of classes: {num_class}")

# Build vocabulary
print("Building vocabulary...")
tokenizer = basic_english_tokenize

def yield_tokens(dataset):
    for _, text in dataset:
        yield tokenizer(text)

vocab = Vocabulary(specials=["<unk>"])
vocab.build_from_iterator(yield_tokens(train_dataset))
vocab.set_default_index(vocab(["<unk>"])[0])

print(f"Vocabulary size: {len(vocab)}")
print(f"Sample tokens: {list(vocab.get_stoi().keys())[:10]}")

# Test vocabulary
print(f"Vocab test: {vocab(['age', 'hello'])}")

# Split training data into train/validation
num_train = int(len(train_dataset) * 0.95)
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

def text_pipeline(x):
    return vocab(tokenizer(x))

def label_pipeline(x):
    return int(x) - 1

def collate_batch(batch):
    label_list, text_list, offsets = [], [], [0]
    for _label, _text in batch:
        label_list.append(label_pipeline(_label))
        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
        text_list.append(processed_text)
        offsets.append(processed_text.size(0))
    label_list = torch.tensor(label_list, dtype=torch.int64)
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    text_list = torch.cat(text_list)
    return label_list.to(device), text_list.to(device), offsets.to(device)

BATCH_SIZE = 64

train_dataloader = DataLoader(
    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
valid_dataloader = DataLoader(
    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)

# Test dataloader
label, text, offsets = next(iter(valid_dataloader))
print(f"Batch shapes - Label: {label.shape}, Text: {text.shape}, Offsets: {offsets.shape}")

# Text classification model
class TextClassificationModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_class):
        super(TextClassificationModel, self).__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)
        self.fc = nn.Linear(embed_dim, num_class)
        self.init_weights()

    def init_weights(self):
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.fc.weight.data.uniform_(-initrange, initrange)
        self.fc.bias.data.zero_()

    def forward(self, text, offsets):
        embedded = self.embedding(text, offsets)
        return self.fc(embedded)

emsize = 64
vocab_size = len(vocab)
print(f"Vocab size: {vocab_size}")
print(f"Number of classes: {num_class}")

model = TextClassificationModel(vocab_size, emsize, num_class).to(device)
print(model)

predicted_label = model(text, offsets)
print(f"Prediction shape: {predicted_label.shape}")

def predict(text, text_pipeline):
    with torch.no_grad():
        text_tensor = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device)
        if len(text_tensor) == 0:  # Handle empty text
            text_tensor = torch.tensor([0], dtype=torch.int64).to(device)
        output = model(text_tensor, torch.tensor([0]).to(device))
        return ag_news_label[output.argmax(1).item() + 1]

# Test prediction
print(f"Test prediction: {predict('I like sports', text_pipeline)}")

def evaluate(dataloader):
    model.eval()
    total_acc, total_count = 0, 0

    with torch.no_grad():
        for idx, (label, text, offsets) in enumerate(dataloader):
            predicted_label = model(text, offsets)
            total_acc += (predicted_label.argmax(1) == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count

print(f"Initial test accuracy: {evaluate(test_dataloader):.4f}")

# Training setup
LR = 0.1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)

EPOCHS = 10
cum_loss_list = []
acc_epoch = []
acc_old = 0

print("Starting training...")
for epoch in tqdm(range(1, EPOCHS + 1)):
    model.train()
    cum_loss = 0
    for idx, (label, text, offsets) in enumerate(train_dataloader):
        optimizer.zero_grad()
        predicted_label = model(text, offsets)
        loss = criterion(predicted_label, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
        optimizer.step()
        cum_loss += loss.item()

    cum_loss_list.append(cum_loss)
    accu_val = evaluate(valid_dataloader)
    acc_epoch.append(accu_val)
    
    print(f"Epoch {epoch}: Loss = {cum_loss:.4f}, Validation Accuracy = {accu_val:.4f}")

    if accu_val > acc_old:
        acc_old = accu_val
        torch.save(model.state_dict(), 'my_model.pth')

plot(cum_loss_list, acc_epoch)

print(f"Final test accuracy: {evaluate(test_dataloader):.4f}")

# Visualization
try:
    # Get the first batch from the validation data
    batch = next(iter(valid_dataloader))
    label, text, offsets = batch
    text = text.to(device)
    offsets = offsets.to(device)

    # Get the embeddings
    embedded = model.embedding(text, offsets)
    embeddings_numpy = embedded.detach().cpu().numpy()

    # Perform t-SNE
    X_embedded_3d = TSNE(n_components=3, random_state=42).fit_transform(embeddings_numpy)

    # Create 3D scatter plot
    trace = go.Scatter3d(
        x=X_embedded_3d[:, 0],
        y=X_embedded_3d[:, 1],
        z=X_embedded_3d[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=label.cpu().numpy(),
            colorscale='Viridis',
            opacity=0.8
        )
    )

    layout = go.Layout(
        title="3D t-SNE Visualization of Embeddings",
        scene=dict(
            xaxis_title='Dimension 1',
            yaxis_title='Dimension 2',
            zaxis_title='Dimension 3'
        )
    )

    fig = go.Figure(data=[trace], layout=layout)
    fig.show()
except Exception as e:
    print(f"Visualization error: {e}")

# Test with sample article
article = """Canada navigated a stiff test against the Republic of Ireland on a rain soaked evening in Perth, coming from behind to claim a vital 2-1 victory at the Women's World Cup.
Katie McCabe opened the scoring with an incredible Olimpico goal – scoring straight from a corner kick – as her corner flew straight over the despairing Canada goalkeeper Kailen Sheridan at Perth Rectangular Stadium in Australia.
Just when Ireland thought it had safely navigated itself to half time with a lead, Megan Connolly failed to get a clean connection on a clearance with the resulting contact squirming into her own net to level the score.
Minutes into the second half, Adriana Leon completed the turnaround for the Olympic champion, slotting home from the edge of the area to seal the three points."""

result = predict(article, text_pipeline)
print(f"\nArticle classification: {result}")

# Load best model and test on new articles
if os.path.exists('my_model.pth'):
    model.load_state_dict(torch.load('my_model.pth'))
    model.eval()

new_articles = [
    "International talks have made significant headway with the signing of a climate accord that commits countries to reduce emissions by 40% over the next two decades. World leaders expressed optimism at the conclusion of the summit.",
    "In a stunning upset, the underdog team won the national title, beating the favorites in a match that featured an incredible comeback and a last-minute goal that sealed their victory in front of a record crowd.",
    "Market analysts are optimistic as the tech startup's stock prices soared after the announcement of their latest product, which promises to revolutionize how we interact with smart devices.",
    "A recent study published in a leading scientific journal suggests that a new drug has shown promise in the treatment of Alzheimer's disease, outperforming current leading medications in early clinical trials.",
    "Diplomatic relations have taken a positive turn with the recent peace talks that aim to end decades of conflict. The ceasefire agreement has been welcomed by the international community.",
    "Economic indicators show a sharp rebound in manufacturing, with the automobile industry leading the charge. Analysts predict this surge will result in significant job creation over the next year.",
    "Researchers at the university's astrophysics department have discovered a potentially habitable exoplanet. The planet, which lies in a nearby star system, has conditions that could support liquid water and, possibly, life.",
    "The sports world is in shock as a legendary player announces their retirement. Over an illustrious 20-year career, the athlete has amassed numerous records and is regarded as one of the greatest to ever play the game.",
    "A multinational corporation has announced a major investment in renewable energy. The initiative includes the construction of new wind farms and solar panels that will power hundreds of thousands of homes.",
    "Climate scientists warn that the melting of the polar ice caps has been accelerating at an alarming rate, raising sea levels and threatening coastal cities worldwide with increased flooding risks."
]

print("\nClassifying new articles:")
for i, article in enumerate(new_articles, start=1):
    prediction = predict(article, text_pipeline)
    print(f"Article {i} is classified as: {prediction}")
