from tqdm import tqdm
import numpy as np
import pandas as pd
from itertools import accumulate
import matplotlib.pyplot as plt
import requests
import os
from collections import Counter
import re

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import numpy as np
from IPython.display import Markdown as md
from tqdm import tqdm

from torch.utils.data.dataset import random_split
from sklearn.manifold import TSNE
import plotly.graph_objs as go
from sklearn.model_selection import train_test_split

# You can also use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    """
    Function: Dummy function to suppress warning messages
    Purpose: Replaces the default warning function to silence all warnings
    
    Input example: warn("This is a warning message", category=UserWarning)
    Output example: (no output - function does nothing)
    """
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

def plot(COST, ACC):
    """
    Function: Creates a dual-axis plot showing training loss and validation accuracy over epochs
    Purpose: Visualizes model training progress with loss and accuracy trends
    
    Input example: 
        COST = [150.2, 120.1, 95.3, 80.2, 70.1]  # List of cumulative losses per epoch
        ACC = [0.65, 0.72, 0.78, 0.82, 0.85]     # List of accuracy values per epoch
    
    Output example: 
        Displays a matplotlib plot with:
        - Red line showing decreasing loss over epochs (left y-axis)
        - Blue line showing increasing accuracy over epochs (right y-axis)
        - X-axis represents epoch numbers
    """
    fig, ax1 = plt.subplots()
    color = 'tab:red'
    ax1.plot(COST, color=color)
    ax1.set_xlabel('epoch', color=color)
    ax1.set_ylabel('total loss', color=color)
    ax1.tick_params(axis='y', color=color)

    ax2 = ax1.twinx()
    color = 'tab:blue'
    ax2.set_ylabel('accuracy', color=color)
    ax2.plot(ACC, color=color)
    ax2.tick_params(axis='y', color=color)
    fig.tight_layout()

    plt.show()

# Custom tokenizer function (replacing torchtext tokenizer)
def basic_english_tokenize(text):
    """
    Function: Simple tokenizer that breaks text into individual words
    Purpose: Converts raw text into a list of lowercase tokens, removing punctuation
    
    Input example: "Hello, World! How are you?"
    Output example: ['hello', 'world', 'how', 'are', 'you']
    
    Process:
    1. Converts text to lowercase
    2. Replaces punctuation with spaces using regex
    3. Splits on whitespace to create token list
    """
    # Convert to lowercase and split on whitespace and punctuation
    text = text.lower()
    # Replace punctuation with spaces and split
    text = re.sub(r'[^\w\s]', ' ', text)
    tokens = text.split()
    return tokens

# Custom vocabulary class
class Vocabulary:
    def __init__(self, specials=None):
        """
        Function: Initialize an empty vocabulary object
        Purpose: Sets up data structures to store token-to-index mappings
        
        Input example: Vocabulary(specials=["<unk>", "<pad>"])
        Output example: Empty vocabulary with special tokens ready to be added
        
        Creates:
        - token_to_idx: dict mapping tokens to indices
        - idx_to_token: dict mapping indices to tokens  
        - default_index: index to use for unknown tokens
        """
        self.specials = specials or []
        self.token_to_idx = {}
        self.idx_to_token = {}
        self.default_index = 0
        
    def build_from_iterator(self, iterator, min_freq=1):
        """
        Function: Builds vocabulary from an iterator of tokenized texts
        Purpose: Creates token-to-index mappings by counting token frequencies
        
        Input example: 
            iterator = [['hello', 'world'], ['hello', 'python'], ['world', 'programming']]
            min_freq = 2
        
        Output example:
            token_to_idx = {'<unk>': 0, 'hello': 1, 'world': 2}
            (tokens appearing less than min_freq times are excluded)
        
        Process:
        1. Count frequency of all tokens
        2. Add special tokens first
        3. Add regular tokens that meet minimum frequency threshold
        """
        # Count tokens
        counter = Counter()
        for tokens in iterator:
            counter.update(tokens)
        
        # Add special tokens first
        for special in self.specials:
            self.token_to_idx[special] = len(self.token_to_idx)
            self.idx_to_token[len(self.idx_to_token)] = special
            
        # Add regular tokens
        for token, freq in counter.items():
            if freq >= min_freq and token not in self.token_to_idx:
                self.token_to_idx[token] = len(self.token_to_idx)
                self.idx_to_token[len(self.idx_to_token)] = token
    
    def __len__(self):
        """
        Function: Returns the total number of tokens in vocabulary
        Purpose: Allows len(vocab) to work and provides vocab size for model initialization
        
        Input example: (called automatically when using len(vocab_object))
        Output example: 15000 (if vocabulary contains 15,000 unique tokens)
        """
        return len(self.token_to_idx)
    
    def __call__(self, tokens):
        """
        Function: Converts tokens to their corresponding indices
        Purpose: Transforms text tokens into numerical indices for model input
        
        Input example 1: "hello" (single token)
        Output example 1: 5 (index of "hello" in vocabulary)
        
        Input example 2: ["hello", "world", "unknown_word"]
        Output example 2: [5, 10, 0] (indices, with 0 for unknown token)
        
        Note: Unknown tokens get mapped to default_index (usually 0 for <unk>)
        """
        if isinstance(tokens, str):
            return self.token_to_idx.get(tokens, self.default_index)
        return [self.token_to_idx.get(token, self.default_index) for token in tokens]
    
    def set_default_index(self, index):
        """
        Function: Sets the index to use for unknown/out-of-vocabulary tokens
        Purpose: Configures how the vocabulary handles unseen words
        
        Input example: vocab.set_default_index(0)  # Use index 0 for unknown tokens
        Output example: self.default_index = 0
        
        Typically used to set default to <unk> token index after vocabulary is built
        """
        self.default_index = index
    
    def get_stoi(self):
        """
        Function: Returns the string-to-index mapping dictionary
        Purpose: Provides access to the internal token-to-index mapping for inspection
        
        Input example: vocab.get_stoi()
        Output example: {'<unk>': 0, 'the': 1, 'hello': 2, 'world': 3, ...}
        
        Useful for debugging vocabulary contents or compatibility with other libraries
        """
        return self.token_to_idx

# Custom dataset class for AG News
class AGNewsDataset(Dataset):
    def __init__(self, split='train'):
        """
        Function: Initialize the AG News dataset
        Purpose: Sets up empty data structures and loads the specified dataset split
        
        Input example: AGNewsDataset(split='train')
        Output example: Dataset object with loaded training data
        
        Creates:
        - self.data: list of text strings  
        - self.labels: list of integer labels (1-4)
        """
        self.data = []
        self.labels = []
        
        # Download and load AG News dataset
        self._load_ag_news(split)
    
    def _load_ag_news(self, split):
        """
        Function: Downloads and loads AG News dataset from CSV files
        Purpose: Fetches data from online source and populates self.data and self.labels
        
        Input example: _load_ag_news('train')
        Output example: 
            - Downloads ag_news_train.csv if not exists
            - Populates self.data with text strings
            - Populates self.labels with integer labels
        
        Data format after loading:
        - self.data[0] = "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - ..."
        - self.labels[0] = 3 (Business category)
        """
        urls = {
            'train': 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv',
            'test': 'https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv'
        }
        
        if split not in urls:
            raise ValueError(f"Split {split} not supported. Use 'train' or 'test'")
        
        # Try to load from local file first, then download if needed
        filename = f'ag_news_{split}.csv'
        if not os.path.exists(filename):
            print(f"Downloading AG News {split} dataset...")
            response = requests.get(urls[split])
            with open(filename, 'wb') as f:
                f.write(response.content)
        
        # Load data from CSV
        df = pd.read_csv(filename, header=None, names=['label', 'title', 'description'])
        
        # Combine title and description
        for _, row in df.iterrows():
            self.labels.append(int(row['label']))
            text = str(row['title']) + ' ' + str(row['description'])
            self.data.append(text)
    
    def __len__(self):
        """
        Function: Returns the number of samples in the dataset
        Purpose: Allows len(dataset) to work and enables PyTorch DataLoader functionality
        
        Input example: len(train_dataset)
        Output example: 120000 (number of training samples in AG News)
        """
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        Function: Retrieves a single sample from the dataset by index
        Purpose: Enables dataset[index] syntax and PyTorch DataLoader iteration
        
        Input example: dataset[0]
        Output example: (3, "Wall St. Bears Claw Back Into the Black (Reuters) Reuters - ...")
                       (label, text_string)
        
        Returns tuple of (label, text) where:
        - label: integer from 1-4 representing news category
        - text: string containing the full article text
        """
        return self.labels[idx], self.data[idx]

# Create datasets
print("Loading AG News dataset...")
train_dataset = AGNewsDataset('train')
test_dataset = AGNewsDataset('test')

# Get sample data
sample_label, sample_text = train_dataset[0]
print(f"Sample: {sample_label}, {sample_text[:100]}...")

ag_news_label = {1: "World", 2: "Sports", 3: "Business", 4: "Sci/Tec"}
print(f"Label meaning: {ag_news_label[sample_label]}")

# Get number of classes
num_class = len(ag_news_label)
print(f"Number of classes: {num_class}")

# Build vocabulary
print("Building vocabulary...")
tokenizer = basic_english_tokenize

def yield_tokens(dataset):
    """
    Function: Generator that yields tokenized text from dataset samples
    Purpose: Provides an iterator over tokens for vocabulary building
    
    Input example: yield_tokens(train_dataset) where dataset contains news articles
    Output example: Generator yielding:
        ['wall', 'st', 'bears', 'claw', 'back', 'into', 'the', 'black']
        ['reuters', 'short', 'sellers', 'wall', 'street', 'renewed']
        ...
    
    Used by vocabulary builder to count token frequencies across all texts
    """
    for _, text in dataset:
        yield tokenizer(text)

vocab = Vocabulary(specials=["<unk>"])
vocab.build_from_iterator(yield_tokens(train_dataset))
vocab.set_default_index(vocab(["<unk>"])[0])

print(f"Vocabulary size: {len(vocab)}")
print(f"Sample tokens: {list(vocab.get_stoi().keys())[:10]}")

# Test vocabulary
print(f"Vocab test: {vocab(['age', 'hello'])}")

# Split training data into train/validation
num_train = int(len(train_dataset) * 0.95)
split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

def text_pipeline(x):
    return vocab(tokenizer(x))

def label_pipeline(x):
    return int(x) - 1

def collate_batch(batch):
    """
    Function: Combines multiple samples into a single batch for training
    Purpose: Processes variable-length texts into format suitable for EmbeddingBag layer
    
    Input example: 
        batch = [(1, "Hello world"), (2, "Good morning everyone")]
    
    Output example:
        labels: tensor([0, 1])  # Converted to zero-indexed
        texts: tensor([245, 1089, 892, 156, 634])  # All tokens concatenated
        offsets: tensor([0, 2])  # Starting positions of each text
    
    The EmbeddingBag layer uses offsets to know where each text begins in the concatenated tensor
    """
    label_list, text_list, offsets = [], [], [0]
    for _label, _text in batch:
        label_list.append(label_pipeline(_label))
        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
        text_list.append(processed_text)
        offsets.append(processed_text.size(0))
    label_list = torch.tensor(label_list, dtype=torch.int64)
    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
    text_list = torch.cat(text_list)
    return label_list.to(device), text_list.to(device), offsets.to(device)

BATCH_SIZE = 64

train_dataloader = DataLoader(
    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
valid_dataloader = DataLoader(
    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)
test_dataloader = DataLoader(
    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch
)

# Test dataloader
label, text, offsets = next(iter(valid_dataloader))
print(f"Batch shapes - Label: {label.shape}, Text: {text.shape}, Offsets: {offsets.shape}")

# Text classification model
class TextClassificationModel(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_class):
        """
        Function: Initialize the text classification neural network
        Purpose: Sets up embedding layer and fully connected layer for classification
        
        Input example: TextClassificationModel(vocab_size=15000, embed_dim=64, num_class=4)
        Output example: Model with embedding and linear layers ready for training
        
        Architecture:
        - EmbeddingBag: Maps token indices to dense vectors and averages them
        - Linear layer: Maps averaged embeddings to class probabilities
        """
        super(TextClassificationModel, self).__init__()
        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)
        self.fc = nn.Linear(embed_dim, num_class)
        self.init_weights()

    def init_weights(self):
        """
        Function: Initialize model weights with small random values
        Purpose: Sets up proper weight initialization for stable training
        
        Input example: (called automatically during model creation)
        Output example: 
            - Embedding weights: random values in [-0.5, 0.5]
            - Linear weights: random values in [-0.5, 0.5] 
            - Linear bias: all zeros
        
        Good weight initialization helps the model converge faster during training
        """
        initrange = 0.5
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.fc.weight.data.uniform_(-initrange, initrange)
        self.fc.bias.data.zero_()

    def forward(self, text, offsets):
        """
        Function: Forward pass through the neural network
        Purpose: Transforms token indices into class prediction logits
        
        Input example:
            text: tensor([245, 1089, 892, 156, 634])  # Concatenated tokens from batch
            offsets: tensor([0, 2])  # Starting positions of each text in batch
        
        Output example:
            tensor([[-0.2, 1.1, 0.3, -0.8],    # Logits for text 1 (4 classes)
                    [0.5, -0.1, 0.9, 0.2]])      # Logits for text 2 (4 classes)
        
        Process:
        1. EmbeddingBag: Convert tokens to embeddings and average by text
        2. Linear layer: Map averaged embeddings to class scores
        """
        embedded = self.embedding(text, offsets)
        return self.fc(embedded)

emsize = 64
vocab_size = len(vocab)
print(f"Vocab size: {vocab_size}")
print(f"Number of classes: {num_class}")

model = TextClassificationModel(vocab_size, emsize, num_class).to(device)
print(model)

predicted_label = model(text, offsets)
print(f"Prediction shape: {predicted_label.shape}")

def predict(text, text_pipeline):
    """
    Function: Makes a prediction on a single text string
    Purpose: Classifies input text into one of the four news categories
    
    Input example: 
        text = "The stock market rose today after strong earnings reports"
        text_pipeline = function that converts text to token indices
    
    Output example: "Business" (predicted category)
    
    Process:
    1. Convert text to token indices using text_pipeline
    2. Run through model to get logits
    3. Take highest scoring class (argmax)
    4. Convert back to human-readable label
    """
    with torch.no_grad():
        text_tensor = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device)
        if len(text_tensor) == 0:  # Handle empty text
            text_tensor = torch.tensor([0], dtype=torch.int64).to(device)
        output = model(text_tensor, torch.tensor([0]).to(device))
        return ag_news_label[output.argmax(1).item() + 1]

# Test prediction
print(f"Test prediction: {predict('I like sports', text_pipeline)}")

def evaluate(dataloader):
    """
    Function: Evaluates model accuracy on a dataset
    Purpose: Measures how well the model performs on validation/test data
    
    Input example: evaluate(test_dataloader)
    Output example: 0.8425 (84.25% accuracy)
    
    Process:
    1. Sets model to evaluation mode (disables dropout, etc.)
    2. Runs inference on all batches without computing gradients
    3. Counts correct predictions vs total predictions
    4. Returns accuracy as a decimal (0.0 to 1.0)
    """
    model.eval()
    total_acc, total_count = 0, 0

    with torch.no_grad():
        for idx, (label, text, offsets) in enumerate(dataloader):
            predicted_label = model(text, offsets)
            total_acc += (predicted_label.argmax(1) == label).sum().item()
            total_count += label.size(0)
    return total_acc / total_count

print(f"Initial test accuracy: {evaluate(test_dataloader):.4f}")

# Training setup
LR = 0.1
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=LR)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)

EPOCHS = 10
cum_loss_list = []
acc_epoch = []
acc_old = 0

print("Starting training...")
for epoch in tqdm(range(1, EPOCHS + 1)):
    model.train()
    cum_loss = 0
    for idx, (label, text, offsets) in enumerate(train_dataloader):
        optimizer.zero_grad()
        predicted_label = model(text, offsets)
        loss = criterion(predicted_label, label)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)
        optimizer.step()
        cum_loss += loss.item()

    cum_loss_list.append(cum_loss)
    accu_val = evaluate(valid_dataloader)
    acc_epoch.append(accu_val)
    
    print(f"Epoch {epoch}: Loss = {cum_loss:.4f}, Validation Accuracy = {accu_val:.4f}")

    if accu_val > acc_old:
        acc_old = accu_val
        torch.save(model.state_dict(), 'my_model.pth')

plot(cum_loss_list, acc_epoch)

print(f"Final test accuracy: {evaluate(test_dataloader):.4f}")

# Visualization
try:
    # Get the first batch from the validation data
    batch = next(iter(valid_dataloader))
    label, text, offsets = batch
    text = text.to(device)
    offsets = offsets.to(device)

    # Get the embeddings
    embedded = model.embedding(text, offsets)
    embeddings_numpy = embedded.detach().cpu().numpy()

    # Perform t-SNE
    X_embedded_3d = TSNE(n_components=3, random_state=42).fit_transform(embeddings_numpy)

    # Create 3D scatter plot
    trace = go.Scatter3d(
        x=X_embedded_3d[:, 0],
        y=X_embedded_3d[:, 1],
        z=X_embedded_3d[:, 2],
        mode='markers',
        marker=dict(
            size=5,
            color=label.cpu().numpy(),
            colorscale='Viridis',
            opacity=0.8
        )
    )

    layout = go.Layout(
        title="3D t-SNE Visualization of Embeddings",
        scene=dict(
            xaxis_title='Dimension 1',
            yaxis_title='Dimension 2',
            zaxis_title='Dimension 3'
        )
    )

    #fig = go.Figure(data=[trace], layout=layout)
    #fig.show()
except Exception as e:
    print(f"Visualization error: {e}")

# Test with sample article
article = """Canada navigated a stiff test against the Republic of Ireland on a rain soaked evening in Perth, coming from behind to claim a vital 2-1 victory at the Women's World Cup.
Katie McCabe opened the scoring with an incredible Olimpico goal – scoring straight from a corner kick – as her corner flew straight over the despairing Canada goalkeeper Kailen Sheridan at Perth Rectangular Stadium in Australia.
Just when Ireland thought it had safely navigated itself to half time with a lead, Megan Connolly failed to get a clean connection on a clearance with the resulting contact squirming into her own net to level the score.
Minutes into the second half, Adriana Leon completed the turnaround for the Olympic champion, slotting home from the edge of the area to seal the three points."""

result = predict(article, text_pipeline)
print(f"\nArticle classification: {result}")

# Load best model and test on new articles
if os.path.exists('my_model.pth'):
    model.load_state_dict(torch.load('my_model.pth'))
    model.eval()

new_articles = [
    "International talks have made significant headway with the signing of a climate accord that commits countries to reduce emissions by 40% over the next two decades. World leaders expressed optimism at the conclusion of the summit.",
    "In a stunning upset, the underdog team won the national title, beating the favorites in a match that featured an incredible comeback and a last-minute goal that sealed their victory in front of a record crowd.",
    "Market analysts are optimistic as the tech startup's stock prices soared after the announcement of their latest product, which promises to revolutionize how we interact with smart devices.",
    "A recent study published in a leading scientific journal suggests that a new drug has shown promise in the treatment of Alzheimer's disease, outperforming current leading medications in early clinical trials.",
    "Diplomatic relations have taken a positive turn with the recent peace talks that aim to end decades of conflict. The ceasefire agreement has been welcomed by the international community.",
    "Economic indicators show a sharp rebound in manufacturing, with the automobile industry leading the charge. Analysts predict this surge will result in significant job creation over the next year.",
    "Researchers at the university's astrophysics department have discovered a potentially habitable exoplanet. The planet, which lies in a nearby star system, has conditions that could support liquid water and, possibly, life.",
    "The sports world is in shock as a legendary player announces their retirement. Over an illustrious 20-year career, the athlete has amassed numerous records and is regarded as one of the greatest to ever play the game.",
    "A multinational corporation has announced a major investment in renewable energy. The initiative includes the construction of new wind farms and solar panels that will power hundreds of thousands of homes.",
    "Climate scientists warn that the melting of the polar ice caps has been accelerating at an alarming rate, raising sea levels and threatening coastal cities worldwide with increased flooding risks."
]

print("\nClassifying new articles:")
for i, article in enumerate(new_articles, start=1):
    prediction = predict(article, text_pipeline)
    print(f"Article {i} is classified as: {prediction}")

